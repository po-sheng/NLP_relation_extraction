{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import _pickle as pickle\n",
    "sys.path.append(\"/work/corpus_reader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/work/corpus_reader/corpus_reader/miwa_reader\")\n",
    "from miwa_reader import MiwaReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = MiwaReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reader.read(\"/work/miwa2016/corpus/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = \"testing_data/train_pathsv3\"\n",
    "relation_file = \"testing_data/train_relationsv3.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the lowest common ancestor of two entity token\n",
    "def find_lowest_ancest(token_idx, check, sen, word, dep, pos_tag, pos):\n",
    "    '''\n",
    "    '''\n",
    "    if check[token_idx] == 0:\n",
    "        check[token_idx] = 1\n",
    "        word[-1].append(sen.tokens[token_idx].text)\n",
    "        dep[-1].append(sen.tokens[token_idx].dep_type)\n",
    "        pos_tag[-1].append(sen.tokens[token_idx].pos)\n",
    "        pos[-1].append(token_idx + 1)\n",
    "        if sen.tokens[token_idx].dep_type != \"ROOT\":\n",
    "            for tok_idx in range(len(sen.tokens)):\n",
    "                if sen.tokens[tok_idx].id == sen.tokens[token_idx].dep_head:\n",
    "                    token_idx = find_lowest_ancest(tok_idx, check, sen, word, dep, pos_tag, pos)\n",
    "                    break\n",
    "    return token_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable\n",
    "words_seq = []\n",
    "deps_seq = []\n",
    "pos_tags_seq = []\n",
    "word_path1 = []\n",
    "word_path2 = []\n",
    "dep_path1 = []\n",
    "dep_path2 = []\n",
    "pos_tags_path1 = []\n",
    "pos_tags_path2 = []\n",
    "pos_path1 = []\n",
    "pos_path2 = []\n",
    "childs_path1 = []\n",
    "childs_path2 = []\n",
    "relation = []\n",
    "temp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain our input data for miwa dataset, there will be C(#entity mention, 2) of input data for every sentence in each file\n",
    "for file_name in docs:\n",
    "    for sentence_index in range(len(docs[file_name].sentences)):\n",
    "        # skip those sentences sequences larger than 90 or less than 1\n",
    "        if len(docs[file_name].sentences[sentence_index].tokens) >= 90 or len(docs[file_name].sentences[sentence_index].tokens) <= 1:\n",
    "            continue\n",
    "        for entity1 in range(len(docs[file_name].sentences[sentence_index].entity_mentions)):\n",
    "            for entity2 in range(entity1 + 1, len(docs[file_name].sentences[sentence_index].entity_mentions)):\n",
    "                \n",
    "                # get words_seq, deps_seq, pos_tags_seq of each sentence \n",
    "                words_seq.append([])\n",
    "                deps_seq.append([])\n",
    "                pos_tags_seq.append([])\n",
    "                \n",
    "                for token_index in range(len(docs[file_name].sentences[sentence_index].tokens)):\n",
    "                    words_seq[-1].append(docs[file_name].sentences[sentence_index].tokens[token_index].text)\n",
    "                    deps_seq[-1].append(docs[file_name].sentences[sentence_index].tokens[token_index].dep_type)\n",
    "                    pos_tags_seq[-1].append(docs[file_name].sentences[sentence_index].tokens[token_index].pos)\n",
    "                    \n",
    "                # get word_path, dep_path, pos_tag_path, pos_path of each sentence \n",
    "                word_path1.append([])\n",
    "                word_path2.append([])\n",
    "                dep_path1.append([])\n",
    "                dep_path2.append([])\n",
    "                pos_tags_path1.append([])\n",
    "                pos_tags_path2.append([])\n",
    "                pos_path1.append([])\n",
    "                pos_path2.append([])\n",
    "                check_list = [0 for i in range(len(docs[file_name].sentences[sentence_index].tokens))]\n",
    "                \n",
    "                # generate ralation file\n",
    "                break_sig = 0\n",
    "                for rlt_idx in range(len(docs[file_name].sentences[sentence_index].relation_mentions)):\n",
    "                    if (docs[file_name].sentences[sentence_index].relation_mentions[rlt_idx].arg1.string == docs[file_name].sentences[sentence_index].entity_mentions[entity1].string and  docs[file_name].sentences[sentence_index].relation_mentions[rlt_idx].arg2.string == docs[file_name].sentences[sentence_index].entity_mentions[entity2].string ) or (docs[file_name].sentences[sentence_index].relation_mentions[rlt_idx].arg1.string == docs[file_name].sentences[sentence_index].entity_mentions[entity2].string and docs[file_name].sentences[sentence_index].relation_mentions[rlt_idx].arg2.string == docs[file_name].sentences[sentence_index].entity_mentions[entity1].string):\n",
    "                        relation.append(docs[file_name].sentences[sentence_index].relation_mentions[rlt_idx].type)\n",
    "                        break_sig = 1\n",
    "                        break\n",
    "                if break_sig == 0:\n",
    "                    relation.append(\"NONE\")\n",
    "                \n",
    "                # find the lowest common ancestor\n",
    "                for tok_idx in range(len(docs[file_name].sentences[sentence_index].tokens)):\n",
    "                    if str(docs[file_name].sentences[sentence_index].entity_mentions[entity1].char_e) == docs[file_name].sentences[sentence_index].tokens[tok_idx].char_e:\n",
    "                        top = find_lowest_ancest(tok_idx, check_list, docs[file_name].sentences[sentence_index], word_path1, dep_path1, pos_tags_path1, pos_path1)\n",
    "                        break\n",
    "\n",
    "                for tok_idx in range(len(docs[file_name].sentences[sentence_index].tokens)):                \n",
    "                    if str(docs[file_name].sentences[sentence_index].entity_mentions[entity2].char_e) == docs[file_name].sentences[sentence_index].tokens[tok_idx].char_e:\n",
    "                        top = find_lowest_ancest(tok_idx, check_list, docs[file_name].sentences[sentence_index], word_path2, dep_path2, pos_tags_path2, pos_path2)\n",
    "                        while top != pos_path1[-1][-1] - 1:\n",
    "                            word_path1[-1].pop()\n",
    "                            dep_path1[-1].pop()\n",
    "                            pos_tags_path1[-1].pop()\n",
    "                            pos_path1[-1].pop()\n",
    "                                                   \n",
    "                check_list.clear()\n",
    "                \n",
    "                # restrict our LCA path in dependency layer less than 20\n",
    "                if len(pos_path1[-1]) > 20 or len(pos_path2[-1]) > 20:\n",
    "                    words_seq.pop()\n",
    "                    deps_seq.pop()\n",
    "                    pos_tags_seq.pop()\n",
    "                    word_path1.pop()\n",
    "                    word_path2.pop()\n",
    "                    dep_path1.pop()\n",
    "                    dep_path2.pop()\n",
    "                    pos_tags_path1.pop()\n",
    "                    pos_tags_path2.pop()\n",
    "                    pos_path1.pop()\n",
    "                    pos_path2.pop()\n",
    "                    relation.pop()\n",
    "                    continue\n",
    "                \n",
    "                # get childs_path of each sentence \n",
    "                childs_path1.append([])\n",
    "                childs_path2.append([])\n",
    "\n",
    "                for tok_targ in pos_path1[-1]:\n",
    "                    childs_path1[-1].append([])\n",
    "                    for tok_idx in range(len(docs[file_name].sentences[sentence_index].tokens)):\n",
    "                        if docs[file_name].sentences[sentence_index].tokens[tok_idx].dep_head == docs[file_name].sentences[sentence_index].tokens[tok_targ - 1].id:\n",
    "                            childs_path1[-1][-1].append(tok_idx + 1)\n",
    "                \n",
    "                for tok_targ in pos_path2[-1]:\n",
    "                    childs_path2[-1].append([])\n",
    "                    for tok_idx in range(len(docs[file_name].sentences[sentence_index].tokens)):\n",
    "                        if docs[file_name].sentences[sentence_index].tokens[tok_idx].dep_head == docs[file_name].sentences[sentence_index].tokens[tok_targ - 1].id:\n",
    "                            childs_path2[-1][-1].append(tok_idx + 1)\n",
    "                \n",
    "                # restrict our children number of each entity less than 20\n",
    "                for i in childs_path1[-1]:\n",
    "                    if len(i) > 20:\n",
    "                        words_seq.pop()\n",
    "                        deps_seq.pop()\n",
    "                        pos_tags_seq.pop()\n",
    "                        word_path1.pop()\n",
    "                        word_path2.pop()\n",
    "                        dep_path1.pop()\n",
    "                        dep_path2.pop()\n",
    "                        pos_tags_path1.pop()\n",
    "                        pos_tags_path2.pop()\n",
    "                        pos_path1.pop()\n",
    "                        pos_path2.pop()\n",
    "                        childs_path1.pop()\n",
    "                        childs_path2.pop()\n",
    "                        relation.pop()\n",
    "                        continue\n",
    "                for i in childs_path2[-1]:\n",
    "                    if len(i) > 20:\n",
    "                        words_seq.pop()\n",
    "                        deps_seq.pop()\n",
    "                        pos_tags_seq.pop()\n",
    "                        word_path1.pop()\n",
    "                        word_path2.pop()\n",
    "                        dep_path1.pop()\n",
    "                        dep_path2.pop()\n",
    "                        pos_tags_path1.pop()\n",
    "                        pos_tags_path2.pop()\n",
    "                        pos_path1.pop()\n",
    "                        pos_path2.pop()\n",
    "                        childs_path1.pop()\n",
    "                        childs_path2.pop()\n",
    "                        relation.pop()\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76390\n"
     ]
    }
   ],
   "source": [
    "# make data the multiple of our batch_size\n",
    "while len(words_seq) % 10 != 0:\n",
    "    words_seq.pop()\n",
    "    deps_seq.pop()\n",
    "    pos_tags_seq.pop()\n",
    "    word_path1.pop()\n",
    "    word_path2.pop()\n",
    "    dep_path1.pop()\n",
    "    dep_path2.pop()\n",
    "    pos_tags_path1.pop()\n",
    "    pos_tags_path2.pop()\n",
    "    pos_path1.pop()\n",
    "    pos_path2.pop()\n",
    "    childs_path1.pop()\n",
    "    childs_path2.pop()\n",
    "    relation.pop()\n",
    "print(len(words_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our file \n",
    "# if it is training then \"train_pathsv3\", testing then \"test_pathsv3\"\n",
    "\n",
    "with open(path_file, \"wb\") as f:\n",
    "    pickle.dump((words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1, pos_path2, childs_path1, childs_path2), f)\n",
    "    f.close()\n",
    "    \n",
    "with open(relation_file, \"w\") as f:\n",
    "    for i in range(len(relation)):\n",
    "        f.write(\"{} {}\\n\".format(i + 1, relation[i]))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
