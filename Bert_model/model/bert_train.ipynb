{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/multi_doc_analyzer\")\n",
    "\n",
    "import torch as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda\n",
    "from multi_doc_analyzer.structure.structure import *\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.fields import TextField, LabelField, ArrayField\n",
    "\n",
    "from multi_doc_analyzer.corpus_reader.ace2005_reader.ace05_reader import ACE05Reader\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.iterators import BucketIterator, DataIterator\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "import random\n",
    "\n",
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    lr=3e-4,                # learning rate\n",
    "    epochs=50,\n",
    "    hidden_sz=128,\n",
    "    arg_sz=3,\n",
    "    max_seq_len=100,\n",
    "    model_folder=\"/work/model_checkpoint/bert_model_checkpoint/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPU = T.cuda.is_available()\n",
    "USE_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fba080313f0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed for both CPU and CUDA\n",
    "T.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_type2idx = {'X':0, 'O': 1, 'PER': 2, 'ORG': 3, 'LOC': 4, 'GPE': 5, 'FAC': 6, 'VEH': 7, 'WEA': 8}\n",
    "\n",
    "r_label2idx = {'PHYS-lr': 1, 'PART-WHOLE-lr': 2, 'PER-SOC-lr': 3, 'ORG-AFF-lr': 4, 'ART-lr': 5, 'GEN-AFF-lr': 6,\n",
    "               'PHYS-rl': 7, 'PART-WHOLE-rl': 8, 'PER-SOC-rl': 9, 'ORG-AFF-rl': 10, 'ART-rl': 11, 'GEN-AFF-rl': 12,\n",
    "               'NONE': 0}\n",
    "\n",
    "\n",
    "class RelationDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads Structure object formatted datasets files, and creates AllenNLP instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: Tokenizer=None, token_indexers: Dict[str, TokenIndexer]=None, \n",
    "                 MAX_WORDPIECES: int=config.max_seq_len, \n",
    "                 is_training = False, ace05_reader: ACE05Reader=None):\n",
    "        # make sure results may be reproduced when sampling...\n",
    "        super().__init__(lazy=False)\n",
    "        random.seed(0)\n",
    "        self.is_training = is_training\n",
    "        self.ace05_reader = ace05_reader\n",
    "        \n",
    "        # NOTE AllenNLP automatically adds [CLS] and [SEP] word peices in the begining and end of the context,\n",
    "        # therefore we need to subtract 2\n",
    "        self.MAX_WORDPIECES = MAX_WORDPIECES - 2\n",
    "        \n",
    "        self.tokenizer = tokenizer or WordTokenizer()\n",
    "        \n",
    "        # BERT specific init\n",
    "        self._token_indexers = token_indexers\n",
    "\n",
    "    def text_to_instance(self, sentence: Sentence) -> Instance:\n",
    "#         sentence_tokens = [Token(x) for x in self.tokenizer(sentence.text)]\n",
    "#         sentence_tokens = [Token(i) for t in sentence.text for i in self.tokenizer(t)]\n",
    "#         sentence_tokens = [Token(text=t) for t in sentence.text]\n",
    "        sentence_tokens = []\n",
    "        td = self.tokenizer(sentence.text)\n",
    "#         print(td)\n",
    "        for t in td:\n",
    "            sentence_tokens.append(Token(text=t))\n",
    "        \n",
    "#         for t in sentence.text:\n",
    "#             td = self.tokenizer(t)\n",
    "#             assert len(td) == 1 or len(td) == 0\n",
    "#             if td:\n",
    "#                 sentence_tokens.append(Token(text=td[0]))\n",
    "#             else:\n",
    "#                 sentence_tokens.append(Token(text='[MASK]'))\n",
    "\n",
    "        sentence_field = TextField(sentence_tokens, self._token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "#         char_list_field = ListField([t for t in sentence.text])\n",
    "#         fields['char_list'] = char_list_field\n",
    "\n",
    "        e_tuple_check_dicts = {} # {(train_arg_l.id, train_arg_r.id):true_label, ...}\n",
    "        if self.is_training: \n",
    "            for r in sentence.relation_mentions:\n",
    "                train_arg_l, train_arg_r, true_label = r.get_left_right_args()\n",
    "                e_tuple_check_dicts[(train_arg_l.id, train_arg_r.id)] = true_label\n",
    "\n",
    "        # construct pair entities\n",
    "        for arg_left_idx in range(len(sentence.entity_mentions)-1):\n",
    "            for arg_right_idx in range(arg_left_idx+1, len(sentence.entity_mentions)):\n",
    "                arg_left = sentence.entity_mentions[arg_left_idx]\n",
    "                arg_right = sentence.entity_mentions[arg_right_idx]\n",
    "                arg_vec = T.zeros(self.MAX_WORDPIECES + 2, dtype=T.long)\n",
    "                arg_vec[:len(sentence_tokens)+2] = 1\n",
    "\n",
    "                # +1 because the first token is [CLS]\n",
    "                arg_vec[arg_left.char_b+1:arg_left.char_e+1] = e_type2idx[arg_left.type]\n",
    "                arg_vec[arg_right.char_b+1:arg_right.char_e+1] = e_type2idx[arg_right.type]\n",
    "                fields[\"arg_idx\"] = ArrayField(arg_vec)\n",
    "\n",
    "\n",
    "#                 fields[\"arg_left\"] = SpanField(arg_left.char_b, arg_left.char_e, char_list_field)\n",
    "#                 fields[\"arg_right\"] = SpanField(arg_right.char_b, arg_right.char_e, char_list_field)\n",
    "                if self.is_training:\n",
    "                    if (arg_left.id, arg_right.id) in e_tuple_check_dicts.keys():\n",
    "                        fields[\"label\"] = LabelField(r_label2idx[e_tuple_check_dicts[(arg_left.id, arg_right.id)]], skip_indexing=True)\n",
    "                    else:\n",
    "                        fields[\"label\"] = LabelField(r_label2idx['NONE'], skip_indexing=True)\n",
    "                yield Instance(fields)\n",
    "    \n",
    "    def _read(self, file_path: str)->Iterator:\n",
    "        doc_dicts = self.ace05_reader.read(file_path)\n",
    "        for doc in doc_dicts.values():\n",
    "            for s in doc.sentences: \n",
    "                if len(s.text) <= config.max_seq_len:\n",
    "                    for instance in self.text_to_instance(s):\n",
    "                        yield instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                out_sz: int=len(r_label2idx)):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self._entity_embeddings = T.nn.Embedding(num_embeddings=len(e_type2idx), embedding_dim=config.arg_sz, padding_idx=0)\n",
    "        self.gru = T.nn.GRU(word_embeddings.get_output_dim()+config.arg_sz, config.hidden_sz, batch_first=True)\n",
    "        self.projection = nn.Linear(config.hidden_sz, out_sz)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, tokens: Dict[str, T.tensor], arg_idx: T.tensor, label: T.tensor = None) -> Dict[str, T.tensor]:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        \n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        pad_len = embeddings.shape[-2]\n",
    "        \n",
    "        arg_idx = arg_idx[:,:pad_len]\n",
    "        arg_idx = arg_idx.type(T.long)\n",
    "        \n",
    "        arg_emb = self._entity_embeddings(arg_idx)\n",
    "\n",
    "        concat = T.cat((embeddings, arg_emb), -1)\n",
    "        ot, ht = self.gru(concat, None) # revise this \"None\"\n",
    "        ot = ot[:,-1,:]    \n",
    "        class_logits = self.projection(ot)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit # the sigmoid function\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> np.ndarray:\n",
    "        out_dict = self.model(**batch)\n",
    "        return expit(tonp(out_dict[\"class_logits\"]))\n",
    "    \n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = tqdm(pred_generator, total=self.iterator.get_num_batches(ds))\n",
    "        preds = []\n",
    "        with T.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                preds.append(self._extract_data(batch))\n",
    "        return np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 8/60 [00:00<00:00, 74.01it/s]\u001b[A\n",
      " 25%|██▌       | 15/60 [00:00<00:00, 71.60it/s]\u001b[A\n",
      " 38%|███▊      | 23/60 [00:00<00:00, 71.71it/s]\u001b[A\n",
      " 52%|█████▏    | 31/60 [00:00<00:00, 73.16it/s]\u001b[A\n",
      " 65%|██████▌   | 39/60 [00:00<00:00, 73.89it/s]\u001b[A\n",
      " 77%|███████▋  | 46/60 [00:00<00:00, 72.44it/s]\u001b[A\n",
      " 90%|█████████ | 54/60 [00:00<00:00, 71.65it/s]\u001b[A\n",
      "100%|██████████| 60/60 [00:00<00:00, 70.94it/s]\u001b[A\n",
      "  0%|          | 0/226 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 23/226 [00:00<00:00, 224.75it/s]\u001b[A\n",
      " 22%|██▏       | 50/226 [00:00<00:00, 235.36it/s]\u001b[A\n",
      " 34%|███▎      | 76/226 [00:00<00:00, 241.76it/s]\u001b[A\n",
      " 44%|████▍     | 99/226 [00:00<00:00, 236.58it/s]\u001b[A\n",
      " 58%|█████▊    | 130/226 [00:00<00:00, 136.61it/s]\u001b[A\n",
      " 69%|██████▊   | 155/226 [00:00<00:00, 157.63it/s]\u001b[A\n",
      " 80%|███████▉  | 180/226 [00:01<00:00, 176.85it/s]\u001b[A\n",
      " 90%|████████▉ | 203/226 [00:01<00:00, 189.36it/s]\u001b[A\n",
      " 99%|█████████▉| 224/226 [00:01<00:00, 172.77it/s]\u001b[A\n",
      "100%|██████████| 226/226 [00:01<00:00, 171.44it/s]\u001b[A\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 5/39 [00:00<00:00, 47.35it/s]\u001b[A\n",
      " 26%|██▌       | 10/39 [00:00<00:00, 44.19it/s]\u001b[A\n",
      " 36%|███▌      | 14/39 [00:00<00:00, 41.97it/s]\u001b[A\n",
      " 49%|████▊     | 19/39 [00:00<00:00, 42.03it/s]\u001b[A\n",
      " 62%|██████▏   | 24/39 [00:00<00:00, 42.52it/s]\u001b[A\n",
      " 74%|███████▍  | 29/39 [00:00<00:00, 42.88it/s]\u001b[A\n",
      " 87%|████████▋ | 34/39 [00:00<00:00, 43.25it/s]\u001b[A\n",
      "100%|██████████| 39/39 [00:00<00:00, 44.08it/s]\u001b[A\n",
      "  0%|          | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|█▎        | 14/106 [00:00<00:00, 124.43it/s]\u001b[A\n",
      " 28%|██▊       | 30/106 [00:00<00:00, 130.35it/s]\u001b[A\n",
      " 39%|███▊      | 41/106 [00:00<00:00, 123.42it/s]\u001b[A\n",
      " 52%|█████▏    | 55/106 [00:00<00:00, 125.22it/s]\u001b[A\n",
      " 67%|██████▋   | 71/106 [00:00<00:00, 133.00it/s]\u001b[A\n",
      " 83%|████████▎ | 88/106 [00:00<00:00, 140.65it/s]\u001b[A\n",
      " 95%|█████████▌| 101/106 [00:00<00:00, 125.33it/s]\u001b[A\n",
      "100%|██████████| 106/106 [00:00<00:00, 127.87it/s]\u001b[A\n",
      "  0%|          | 0/49 [00:00<?, ?it/s]\u001b[A\n",
      " 18%|█▊        | 9/49 [00:00<00:00, 88.28it/s]\u001b[A\n",
      " 33%|███▎      | 16/49 [00:00<00:00, 81.18it/s]\u001b[A\n",
      " 47%|████▋     | 23/49 [00:00<00:00, 77.32it/s]\u001b[A\n",
      " 63%|██████▎   | 31/49 [00:00<00:00, 74.83it/s]\u001b[A\n",
      " 80%|███████▉  | 39/49 [00:00<00:00, 73.88it/s]\u001b[A\n",
      "100%|██████████| 49/49 [00:00<00:00, 78.60it/s]\u001b[A\n",
      "  0%|          | 0/119 [00:00<?, ?it/s]\u001b[A\n",
      " 21%|██        | 25/119 [00:00<00:00, 231.19it/s]\u001b[A\n",
      " 44%|████▎     | 52/119 [00:00<00:00, 240.55it/s]\u001b[A\n",
      " 63%|██████▎   | 75/119 [00:01<00:00, 69.04it/s] \u001b[A\n",
      " 76%|███████▋  | 91/119 [00:01<00:00, 82.99it/s]\u001b[A\n",
      " 93%|█████████▎| 111/119 [00:01<00:00, 100.22it/s]\u001b[A\n",
      "21751it [00:44, 488.98it/s]0:01<00:00, 87.43it/s] \u001b[A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    ace_path = \"/work/LDC2006T06/\"\n",
    "    models_folder = \"/work/relation_extraction/bert_pretrain_model/\"\n",
    "\n",
    "    ace05_reader = ACE05Reader(lang='en')\n",
    "    \n",
    "    token_indexer = PretrainedBertIndexer(\n",
    "        pretrained_model=\"bert-base-cased\",\n",
    "        max_pieces=config.max_seq_len,\n",
    "#         truncate_long_sequences=False,\n",
    "        do_lowercase=False\n",
    "    )\n",
    "\n",
    "\t# AllenNLP DatasetReader\n",
    "    reader = RelationDatasetReader(\n",
    "        is_training=True, \n",
    "        ace05_reader=ace05_reader, \n",
    "        tokenizer=lambda s: token_indexer.wordpiece_tokenizer(s),\n",
    "        token_indexers={\"tokens\": token_indexer}\n",
    "    )\n",
    "\n",
    "    train_ds = reader.read(ace_path)\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    iterator = BucketIterator(batch_size=config.batch_size, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    bert_embedder = PretrainedBertEmbedder(\n",
    "        pretrained_model=\"bert-base-cased\",\n",
    "        top_layer_only=True, # conserve memory   \n",
    "    )\n",
    "    word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                                # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                               allow_unmatched_keys = True)\n",
    "    model = BERT(word_embeddings)\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # training\n",
    "    from allennlp.training.trainer import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds,\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=config.epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.5380 ||: 100%|██████████| 340/340 [00:20<00:00, 16.47it/s]\n",
      "loss: 0.3851 ||: 100%|██████████| 340/340 [00:17<00:00, 19.65it/s]\n",
      "loss: 0.2650 ||: 100%|██████████| 340/340 [00:18<00:00, 18.37it/s]\n",
      "loss: 0.1750 ||: 100%|██████████| 340/340 [00:18<00:00, 18.66it/s]\n",
      "loss: 0.1218 ||: 100%|██████████| 340/340 [00:18<00:00, 18.81it/s]\n",
      "loss: 0.0916 ||: 100%|██████████| 340/340 [00:18<00:00, 18.81it/s]\n",
      "loss: 0.0691 ||: 100%|██████████| 340/340 [00:19<00:00, 17.13it/s]\n",
      "loss: 0.0520 ||: 100%|██████████| 340/340 [00:18<00:00, 21.26it/s]\n",
      "loss: 0.0412 ||: 100%|██████████| 340/340 [00:18<00:00, 18.69it/s]\n",
      "loss: 0.0332 ||: 100%|██████████| 340/340 [00:18<00:00, 18.40it/s]\n",
      "loss: 0.0276 ||: 100%|██████████| 340/340 [00:18<00:00, 18.50it/s]\n",
      "loss: 0.0204 ||: 100%|██████████| 340/340 [00:18<00:00, 18.21it/s]\n",
      "loss: 0.0211 ||: 100%|██████████| 340/340 [00:17<00:00, 18.27it/s]\n",
      "loss: 0.0161 ||: 100%|██████████| 340/340 [00:18<00:00, 20.37it/s]\n",
      "loss: 0.0135 ||: 100%|██████████| 340/340 [00:17<00:00, 18.51it/s]\n",
      "loss: 0.0121 ||: 100%|██████████| 340/340 [00:18<00:00, 18.55it/s]\n",
      "loss: 0.0115 ||: 100%|██████████| 340/340 [00:18<00:00, 17.66it/s]\n",
      "loss: 0.0073 ||: 100%|██████████| 340/340 [00:18<00:00, 19.47it/s]\n",
      "loss: 0.0061 ||: 100%|██████████| 340/340 [00:18<00:00, 18.35it/s]\n",
      "loss: 0.0083 ||: 100%|██████████| 340/340 [00:18<00:00, 18.33it/s]\n",
      "loss: 0.0046 ||: 100%|██████████| 340/340 [00:18<00:00, 18.37it/s]\n",
      "loss: 0.0101 ||: 100%|██████████| 340/340 [00:18<00:00, 20.92it/s]\n",
      "loss: 0.0062 ||: 100%|██████████| 340/340 [00:18<00:00, 18.30it/s]\n",
      "loss: 0.0041 ||: 100%|██████████| 340/340 [00:18<00:00, 18.64it/s]\n",
      "loss: 0.0056 ||: 100%|██████████| 340/340 [00:18<00:00, 18.51it/s]\n",
      "loss: 0.0030 ||: 100%|██████████| 340/340 [00:18<00:00, 18.79it/s]\n",
      "loss: 0.0037 ||: 100%|██████████| 340/340 [00:18<00:00, 17.97it/s]\n",
      "loss: 0.0052 ||: 100%|██████████| 340/340 [00:18<00:00, 18.42it/s]\n",
      "loss: 0.0049 ||: 100%|██████████| 340/340 [00:18<00:00, 18.40it/s]\n",
      "loss: 0.0039 ||: 100%|██████████| 340/340 [00:18<00:00, 18.25it/s]\n",
      "loss: 0.0059 ||: 100%|██████████| 340/340 [00:18<00:00, 19.86it/s]\n",
      "loss: 0.0025 ||: 100%|██████████| 340/340 [00:18<00:00, 18.60it/s]\n",
      "loss: 0.0052 ||: 100%|██████████| 340/340 [00:18<00:00, 18.12it/s]\n",
      "loss: 0.0034 ||: 100%|██████████| 340/340 [00:18<00:00, 17.98it/s]\n",
      "loss: 0.0020 ||: 100%|██████████| 340/340 [00:18<00:00, 18.10it/s]\n",
      "loss: 0.0035 ||: 100%|██████████| 340/340 [00:18<00:00, 18.92it/s]\n",
      "loss: 0.0039 ||: 100%|██████████| 340/340 [00:18<00:00, 18.23it/s]\n",
      "loss: 0.0029 ||: 100%|██████████| 340/340 [00:18<00:00, 18.82it/s]\n",
      "loss: 0.0031 ||: 100%|██████████| 340/340 [00:18<00:00, 17.00it/s]\n",
      "loss: 0.0016 ||: 100%|██████████| 340/340 [00:18<00:00, 19.41it/s]\n",
      "loss: 0.0028 ||: 100%|██████████| 340/340 [00:18<00:00, 17.91it/s]\n",
      "loss: 0.0029 ||: 100%|██████████| 340/340 [00:18<00:00, 19.13it/s]\n",
      "loss: 0.0010 ||: 100%|██████████| 340/340 [00:18<00:00, 17.90it/s]\n",
      "loss: 0.0048 ||: 100%|██████████| 340/340 [00:19<00:00, 17.88it/s]\n",
      "loss: 0.0036 ||: 100%|██████████| 340/340 [00:18<00:00, 16.79it/s]\n",
      "loss: 0.0011 ||: 100%|██████████| 340/340 [00:18<00:00, 18.12it/s]\n",
      "loss: 0.0013 ||: 100%|██████████| 340/340 [00:18<00:00, 17.87it/s]\n",
      "loss: 0.0011 ||: 100%|██████████| 340/340 [00:18<00:00, 18.27it/s]\n",
      "loss: 0.0010 ||: 100%|██████████| 340/340 [00:18<00:00, 18.06it/s]\n",
      "loss: 0.0037 ||: 100%|██████████| 340/340 [00:18<00:00, 18.48it/s]\n"
     ]
    }
   ],
   "source": [
    "    metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # save \n",
    "    with open(config.model_folder+'model.th', 'wb') as f:\n",
    "        T.save(model.state_dict(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
