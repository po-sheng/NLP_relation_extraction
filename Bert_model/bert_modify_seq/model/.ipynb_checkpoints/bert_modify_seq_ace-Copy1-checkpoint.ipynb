{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/multi_doc_analyzer\")\n",
    "sys.path.append(\"/work/relation_extraction/Bert_model/baseline/data/\")\n",
    "\n",
    "import torch as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda\n",
    "from allennlp.nn import util as nn_util\n",
    "from multi_doc_analyzer.structure.structure import *\n",
    "from multi_doc_analyzer.tokenization.tokenizer import MDATokenizer\n",
    "from tqdm import tqdm\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.fields import TextField, LabelField, ArrayField\n",
    "\n",
    "from ace05_set_reader import ACE05Reader\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.iterators import BucketIterator, DataIterator, BasicIterator\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "import random\n",
    "\n",
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as prs\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/work/LDC2006T06/dataset/dev/\"\n",
    "test_path = \"/work/LDC2006T06/dataset/test/\"\n",
    "model_folder = \"/work/model_checkpoint/bert_model_checkpoint/bert_modify_seq/\"\n",
    "output_path = \"/work/relation_extraction/Bert_model/bert_modify_seq/analysis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    lr=3e-4,                # learning rate\n",
    "    epochs=30,\n",
    "    mlp_hidden_sz=300,\n",
    "    lstm_hidden_sz=768,\n",
    "    arg_sz=10,              # position embedding size\n",
    "    max_seq_len=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPU = T.cuda.is_available()\n",
    "USE_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efc380ee830>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed for both CPU and CUDA\n",
    "T.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_type2idx = {'X':0, 'O': 1, 'PER': 2, 'ORG': 3, 'LOC': 4, 'GPE': 5, 'FAC': 6, 'VEH': 7, 'WEA': 8}\n",
    "\n",
    "r_label2idx = {'PHYS-lr': 1, 'PART-WHOLE-lr': 2, 'PER-SOC-lr': 3, 'ORG-AFF-lr': 4, 'ART-lr': 5, 'GEN-AFF-lr': 6,\n",
    "               'PHYS-rl': 7, 'PART-WHOLE-rl': 8, 'PER-SOC-rl': 9, 'ORG-AFF-rl': 10, 'ART-rl': 11, 'GEN-AFF-rl': 12,\n",
    "               'NONE': 0}\n",
    "\n",
    "# r_label2idx = {'PHYS': 1, 'PART-WHOLE': 2, 'PER-SOC': 3, 'ORG-AFF': 4, 'ART': 5, 'GEN-AFF': 6, 'NONE': 0}\n",
    "\n",
    "r_idx2label = {v: k for k, v in r_label2idx.items()}\n",
    "\n",
    "class RelationDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads Structure object formatted datasets files, and creates AllenNLP instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer]=None, \n",
    "                 MAX_WORDPIECES: int=config.max_seq_len, \n",
    "                 is_training = False, ace05_reader: ACE05Reader=None):\n",
    "        # make sure results may be reproduced when sampling...\n",
    "        super().__init__(lazy=False)\n",
    "        random.seed(0)\n",
    "        self.is_training = is_training\n",
    "        self.ace05_reader = ace05_reader\n",
    "        \n",
    "        # NOTE AllenNLP automatically adds [CLS] and [SEP] word peices in the begining and end of the context,\n",
    "        # therefore we need to subtract 2\n",
    "        self.MAX_WORDPIECES = MAX_WORDPIECES - 2\n",
    "        \n",
    "        # BERT specific init\n",
    "        self._token_indexers = token_indexers\n",
    "\n",
    "    def text_to_instance(self, sentence: Sentence) -> Instance:\n",
    "\n",
    "        e_tuple_check_dicts = {} # {(train_arg_l.id, train_arg_r.id):true_label, ...}\n",
    "        if self.is_training: \n",
    "            for r in sentence.relation_mentions:\n",
    "                train_arg_l, train_arg_r, true_label = r.get_left_right_args()\n",
    "                e_tuple_check_dicts[(train_arg_l.id, train_arg_r.id)] = true_label\n",
    "\n",
    "        # construct pair entities\n",
    "        for arg1_idx in range(len(sentence.entity_mentions)-1):\n",
    "            for arg2_idx in range(arg1_idx+1, len(sentence.entity_mentions)):\n",
    "                field = {}\n",
    "                sentence_tokens = []\n",
    "    \n",
    "                arg1 = sentence.entity_mentions[arg1_idx]\n",
    "                arg2 = sentence.entity_mentions[arg2_idx]\n",
    "                \n",
    "                # dicide which is on the left and redefine\n",
    "                if arg1.char_b > arg2.char_b:\n",
    "                    entity_l = arg2\n",
    "                    entity_r = arg1\n",
    "                else:\n",
    "                    entity_l = arg1\n",
    "                    entity_r = arg2                    \n",
    "                \n",
    "                # in order to save our two entities which may be splited\n",
    "                ent = [[] for i in range(2)]             # index 0 : 0 for ent_l, 1 for ent_r\n",
    "                \n",
    "                # create our manual form of seq\n",
    "                for i,t in enumerate(sentence.tokens):\n",
    "                    if i >= entity_l.token_b and i < entity_l.token_e:\n",
    "                        ent[0].append(t.text)\n",
    "                        sentence_tokens.append(Token(text=\"[unused\" + str(e_type2idx[entity_l.type]) + \"]\"))\n",
    "                    elif i >= entity_r.token_b and i < entity_r.token_e:\n",
    "                        ent[1].append(t.text)\n",
    "                        sentence_tokens.append(Token(text=\"[unused\" + str(e_type2idx[entity_r.type]) + \"]\"))\n",
    "                    else:\n",
    "                        sentence_tokens.append(Token(text=t.text))\n",
    "                sentence_tokens.append(Token(text=\"[SEP]\"))\n",
    "                for i in range(len(ent[0])):\n",
    "                    sentence_tokens.append(Token(text=ent[0][i]))\n",
    "                sentence_tokens.append(Token(text=\"[SEP]\"))\n",
    "                for i in range(len(ent[1])):\n",
    "                    sentence_tokens.append(Token(text=ent[1][i]))\n",
    "                \n",
    "                sentence_field = TextField(sentence_tokens, self._token_indexers)\n",
    "                fields = {\"tokens\": sentence_field}\n",
    "\n",
    "#                 arg_vec = T.tensor([[0, 0] for i in range(len(sentence.tokens) + 2)], dtype=T.long)   # long type to feed into embedding layer\n",
    "                arg_vec = T.tensor([[99, 99] for i in range(config.max_seq_len)], dtype=T.long)   # long type to feed into embedding layer\n",
    "                \n",
    "                # +1 because the first token is [CLS]\n",
    "                pos = lambda t, b, e: 0 if t >= b and t < e else ( (b-t) if t < b else (t-e+1) ) \n",
    "                for i in range(len(sentence.tokens) + 2):\n",
    "                    arg_vec[i][0] = pos(i-1, entity_l.token_b, entity_l.token_e)    # arg_l position, i-1 for [CLS]\n",
    "                    arg_vec[i][1] = pos(i-1, entity_r.token_b, entity_r.token_e)    # arg_r position, i-1 for [CLS]\n",
    "                fields[\"arg_idx\"] = ArrayField(arg_vec)\n",
    "                \n",
    "                # information for each sentence length\n",
    "                fields[\"sen_len\"] = LabelField(len(sentence.tokens) + 2, skip_indexing=True)\n",
    "                \n",
    "                # relation\n",
    "                if self.is_training:\n",
    "                    if (entity_l.id, entity_r.id) in e_tuple_check_dicts.keys():\n",
    "                        fields[\"label\"] = LabelField(r_label2idx[e_tuple_check_dicts[(entity_l.id, entity_r.id)]], skip_indexing=True)\n",
    "                    else:\n",
    "                        fields[\"label\"] = LabelField(r_label2idx['NONE'], skip_indexing=True)\n",
    "                yield Instance(fields)\n",
    "    \n",
    "    def _read(self, file_path: str)->Iterator: \n",
    "        doc_dicts = self.ace05_reader.read(file_path)\n",
    "        tokenizer = MDATokenizer('bert-en')\n",
    "        for doc in doc_dicts.values():\n",
    "            tokenizer.annotate_document(doc)\n",
    "            for s in doc.sentences: \n",
    "                if len(s.tokens) <= config.max_seq_len - 2:\n",
    "                    for instance in self.text_to_instance(s):\n",
    "                        yield instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ace05_reader = ACE05Reader(lang='en')\n",
    "\n",
    "# token_indexer = PretrainedBertIndexer(\n",
    "#     pretrained_model=\"bert-base-uncased\",\n",
    "# #         max_pieces=config.max_seq_len,\n",
    "# #         do_lowercase=False               # for cased condition\n",
    "# )\n",
    "\n",
    "# # AllenNLP DatasetReader\n",
    "# reader = RelationDatasetReader(\n",
    "#     is_training=True, \n",
    "#     ace05_reader=ace05_reader, \n",
    "#     tokenizer=lambda s: token_indexer.wordpiece_tokenizer(s),\n",
    "#     token_indexers={\"tokens\": token_indexer}\n",
    "# )\n",
    "\n",
    "# train_ds = reader.read(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                out_sz: int=len(r_label2idx)):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self._position_embeddings = T.nn.Embedding(num_embeddings=(config.max_seq_len), embedding_dim=config.arg_sz, padding_idx=0)\n",
    "        \n",
    "        # bert output is of dimension 768\n",
    "        self.lstm = T.nn.LSTM(input_size=768 + 2*config.arg_sz, hidden_size=768, batch_first=True, bidirectional=True)\n",
    "        self.projection1 = nn.Linear(config.lstm_hidden_sz * 2, config.mlp_hidden_sz)\n",
    "        self.projection2 = nn.Linear(config.mlp_hidden_sz, out_sz)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, tokens: Dict[str, T.tensor], arg_idx: T.tensor, sen_len: T.tensor, label: T.tensor = None) -> Dict[str, T.tensor]:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "#         print(len(tokens[\"tokens\"][0]))\n",
    "#         print(arg_idx.shape)\n",
    "#         print(sen_len)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "#         print(embeddings.shape)\n",
    "        arg_idx = arg_idx.type(T.long)\n",
    "        \n",
    "#         cut_len = 0\n",
    "#         for i in range(len(arg_idx)):\n",
    "#             for j in range(len(arg_idx[i])-1, -1, -1):\n",
    "#                 if arg_idx[i][j][0] != config.max_seq_len-1 or arg_idx[i][j][1] != config.max_seq_len-1:\n",
    "#                     pad_len = j+1\n",
    "# #                     print(pad_len, arg_idx[i][j][0], arg_idx[i][j][1])\n",
    "#                     break\n",
    "#             if pad_len > cut_len:\n",
    "#                 cut_len = pad_len\n",
    "\n",
    "#         print(cut_len)\n",
    "#         cut_len = max(len(arg_idx[i]) for i in range(len(arg_idx))) \n",
    "#         cut_len = arg_idx.shape[-2]\n",
    "        cut_len = max(i for i in sen_len)\n",
    "#         print(cut_len)\n",
    "        embeddings = embeddings[:,:cut_len,:]\n",
    "        arg_idx = arg_idx[:,:cut_len,:]\n",
    "#         print(arg_idx[0])\n",
    "#         print(embeddings.shape)\n",
    "        arg_emb = self._position_embeddings(arg_idx)\n",
    "#         print(arg_emb[0])\n",
    "        arg_cat = T.cat((arg_emb[:,:,0,:], arg_emb[:,:,1,:]), -1)\n",
    "#         print(arg_emb)\n",
    "#         print(arg_cat)\n",
    "        concat = T.cat((embeddings, arg_cat), -1)\n",
    "#         print(concat)\n",
    "        ot, hs = self.lstm(concat)\n",
    "        \n",
    "        mlp_hs = self.projection1(ot[:, -1, :])\n",
    "#         print(mlp_hs)\n",
    "        class_logits = self.projection2(mlp_hs)\n",
    "#         print(label)\n",
    "        output = {\"class_logits\": class_logits}\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit # the sigmoid function\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> np.ndarray:\n",
    "        out_dict = self.model(**batch)\n",
    "        return expit(tonp(out_dict[\"class_logits\"]))\n",
    "    \n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = tqdm(pred_generator, total=self.iterator.get_num_batches(ds))\n",
    "        preds = []\n",
    "        with T.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                preds.append(self._extract_data(batch))\n",
    "        return np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comfusion_matrix(label_classes, predict_classes, out_folder, file_name):\n",
    "    label_types = list(r_idx2label.values())\n",
    "\n",
    "    cm = confusion_matrix(label_classes, predict_classes, label_types)\n",
    "    print(cm)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    for (i, j), z in np.ndenumerate(cm):\n",
    "        ax.text(j, i, '{:0.0f}'.format(z), ha='center', va='center', color='white')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + label_types)\n",
    "    ax.set_yticklabels([''] + label_types)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    plt.savefig(out_folder + 'confusion_matrix_' + file_name + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # remove the data which is none or predict none\n",
    "    \n",
    "    pre, recall, f1, sup = prs(label_classes, predict_classes, average='macro')\n",
    "    \n",
    "    print(\"Accuracy:\", sum(cm[i][i] for i in range(len(cm))) / len(label_classes))\n",
    "    print(\"Precision:\", pre)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_analyze(ds, true, pred, opt):\n",
    "    \n",
    "    # classify different kinds of error\n",
    "    detail = [[[] for j in range(len(r_label2idx))] for i in range(len(r_label2idx))]\n",
    "    for i in range(len(ds)):\n",
    "         if true[i] != pred[i]:\n",
    "            detail[r_label2idx[true[i]]][r_label2idx[pred[i]]].append(i)\n",
    "    \n",
    "    # print into a csv file\n",
    "    with open(output_path + \"error_detail_\" + opt + \".csv\", \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Sentence\", \"Two_Entity\", \"Predict\", \"Label\", \"idx\"])\n",
    "        for j in range(len(detail)):\n",
    "            for k in range(len(detail)):\n",
    "                with_element = 0\n",
    "                if k == j:\n",
    "                    continue\n",
    "                for i in detail[j][k]:\n",
    "                    with_element = 1\n",
    "                    ent1 = []\n",
    "                    ent2 = []\n",
    "                    for g in range(len(vars(ds[i].fields['arg_idx'])['array'])):\n",
    "                        if int(vars(ds[i].fields['arg_idx'])['array'][g][0]) == config.max_seq_len or int(vars(ds[i].fields['arg_idx'])['array'][g][1]) == config.max_seq_len:\n",
    "                            if int(vars(ds[i].fields['arg_idx'])['array'][g][0]) == config.max_seq_len:\n",
    "                                ent1.append(vars(ds[i].fields['tokens'])['tokens'][g-1])\n",
    "                            else:\n",
    "                                ent2.append(vars(ds[i].fields['tokens'])['tokens'][g-1])\n",
    "    \n",
    "                    tostr = lambda a: [str(a[i]) for i in range(len(a))] \n",
    "                    writer.writerow([\" \".join(tostr(vars(ds[i].fields['tokens'])['tokens'])), [ent1, ent2], pred[i], true[i], i])\n",
    "                if with_element == 1:\n",
    "                    writer.writerow(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 10/80 [00:00<00:00, 97.99it/s]\u001b[A\n",
      " 34%|███▍      | 27/80 [00:00<00:00, 112.08it/s]\u001b[A\n",
      " 60%|██████    | 48/80 [00:00<00:00, 129.71it/s]\u001b[A\n",
      " 76%|███████▋  | 61/80 [00:00<00:00, 125.70it/s]\u001b[A\n",
      " 92%|█████████▎| 74/80 [00:00<00:00, 126.37it/s]\u001b[A\n",
      "15624it [00:28, 555.06it/s] 0<00:00, 140.55it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15624\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, [unused3], may, not, want, jordan, ,, but, the, [unused2], of, the, expansion, team, says, jordan, could, run, operations, there, ,, if, he, wants, to, ., [SEP], wizards, [SEP], head], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[28, 36],\n",
      "        [27, 35],\n",
      "        [26, 34],\n",
      "        [25, 33],\n",
      "        [24, 32],\n",
      "        [23, 31],\n",
      "        [22, 30],\n",
      "        [21, 29],\n",
      "        [20, 28],\n",
      "        [19, 27],\n",
      "        [18, 26],\n",
      "        [17, 25],\n",
      "        [16, 24],\n",
      "        [15, 23],\n",
      "        [14, 22],\n",
      "        [13, 21],\n",
      "        [12, 20],\n",
      "        [11, 19],\n",
      "        [10, 18],\n",
      "        [ 9, 17],\n",
      "        [ 8, 16],\n",
      "        [ 7, 15],\n",
      "        [ 6, 14],\n",
      "        [ 5, 13],\n",
      "        [ 4, 12],\n",
      "        [ 3, 11],\n",
      "        [ 2, 10],\n",
      "        [ 1,  9],\n",
      "        [ 0,  8],\n",
      "        [ 1,  7],\n",
      "        [ 2,  6],\n",
      "        [ 3,  5],\n",
      "        [ 4,  4],\n",
      "        [ 5,  3],\n",
      "        [ 6,  2],\n",
      "        [ 7,  1],\n",
      "        [ 8,  0],\n",
      "        [ 9,  1],\n",
      "        [10,  2],\n",
      "        [11,  3],\n",
      "        [12,  4],\n",
      "        [13,  5],\n",
      "        [14,  6],\n",
      "        [15,  7],\n",
      "        [16,  8],\n",
      "        [17,  9],\n",
      "        [18, 10],\n",
      "        [19, 11],\n",
      "        [20, 12],\n",
      "        [21, 13],\n",
      "        [22, 14],\n",
      "        [23, 15],\n",
      "        [24, 16],\n",
      "        [25, 17],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, jordan, ,, but, the, [unused2], of, the, expansion, [unused3], says, jordan, could, run, operations, there, ,, if, he, wants, to, ., [SEP], head, [SEP], team], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[36, 40],\n",
      "        [35, 39],\n",
      "        [34, 38],\n",
      "        [33, 37],\n",
      "        [32, 36],\n",
      "        [31, 35],\n",
      "        [30, 34],\n",
      "        [29, 33],\n",
      "        [28, 32],\n",
      "        [27, 31],\n",
      "        [26, 30],\n",
      "        [25, 29],\n",
      "        [24, 28],\n",
      "        [23, 27],\n",
      "        [22, 26],\n",
      "        [21, 25],\n",
      "        [20, 24],\n",
      "        [19, 23],\n",
      "        [18, 22],\n",
      "        [17, 21],\n",
      "        [16, 20],\n",
      "        [15, 19],\n",
      "        [14, 18],\n",
      "        [13, 17],\n",
      "        [12, 16],\n",
      "        [11, 15],\n",
      "        [10, 14],\n",
      "        [ 9, 13],\n",
      "        [ 8, 12],\n",
      "        [ 7, 11],\n",
      "        [ 6, 10],\n",
      "        [ 5,  9],\n",
      "        [ 4,  8],\n",
      "        [ 3,  7],\n",
      "        [ 2,  6],\n",
      "        [ 1,  5],\n",
      "        [ 0,  4],\n",
      "        [ 1,  3],\n",
      "        [ 2,  2],\n",
      "        [ 3,  1],\n",
      "        [ 4,  0],\n",
      "        [ 5,  1],\n",
      "        [ 6,  2],\n",
      "        [ 7,  3],\n",
      "        [ 8,  4],\n",
      "        [ 9,  5],\n",
      "        [10,  6],\n",
      "        [11,  7],\n",
      "        [12,  8],\n",
      "        [13,  9],\n",
      "        [14, 10],\n",
      "        [15, 11],\n",
      "        [16, 12],\n",
      "        [17, 13],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 4, '_label_namespace': 'labels', '_label_id': 4}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, jordan, ,, but, the, [unused2], of, the, expansion, team, says, jordan, could, run, operations, [unused3], ,, if, he, wants, to, ., [SEP], head, [SEP], there], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[36, 46],\n",
      "        [35, 45],\n",
      "        [34, 44],\n",
      "        [33, 43],\n",
      "        [32, 42],\n",
      "        [31, 41],\n",
      "        [30, 40],\n",
      "        [29, 39],\n",
      "        [28, 38],\n",
      "        [27, 37],\n",
      "        [26, 36],\n",
      "        [25, 35],\n",
      "        [24, 34],\n",
      "        [23, 33],\n",
      "        [22, 32],\n",
      "        [21, 31],\n",
      "        [20, 30],\n",
      "        [19, 29],\n",
      "        [18, 28],\n",
      "        [17, 27],\n",
      "        [16, 26],\n",
      "        [15, 25],\n",
      "        [14, 24],\n",
      "        [13, 23],\n",
      "        [12, 22],\n",
      "        [11, 21],\n",
      "        [10, 20],\n",
      "        [ 9, 19],\n",
      "        [ 8, 18],\n",
      "        [ 7, 17],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 14],\n",
      "        [ 3, 13],\n",
      "        [ 2, 12],\n",
      "        [ 1, 11],\n",
      "        [ 0, 10],\n",
      "        [ 1,  9],\n",
      "        [ 2,  8],\n",
      "        [ 3,  7],\n",
      "        [ 4,  6],\n",
      "        [ 5,  5],\n",
      "        [ 6,  4],\n",
      "        [ 7,  3],\n",
      "        [ 8,  2],\n",
      "        [ 9,  1],\n",
      "        [10,  0],\n",
      "        [11,  1],\n",
      "        [12,  2],\n",
      "        [13,  3],\n",
      "        [14,  4],\n",
      "        [15,  5],\n",
      "        [16,  6],\n",
      "        [17,  7],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, [unused2], ,, but, the, [unused2], of, the, expansion, team, says, jordan, could, run, operations, there, ,, if, he, wants, to, ., [SEP], jordan, [SEP], head], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[32, 36],\n",
      "        [31, 35],\n",
      "        [30, 34],\n",
      "        [29, 33],\n",
      "        [28, 32],\n",
      "        [27, 31],\n",
      "        [26, 30],\n",
      "        [25, 29],\n",
      "        [24, 28],\n",
      "        [23, 27],\n",
      "        [22, 26],\n",
      "        [21, 25],\n",
      "        [20, 24],\n",
      "        [19, 23],\n",
      "        [18, 22],\n",
      "        [17, 21],\n",
      "        [16, 20],\n",
      "        [15, 19],\n",
      "        [14, 18],\n",
      "        [13, 17],\n",
      "        [12, 16],\n",
      "        [11, 15],\n",
      "        [10, 14],\n",
      "        [ 9, 13],\n",
      "        [ 8, 12],\n",
      "        [ 7, 11],\n",
      "        [ 6, 10],\n",
      "        [ 5,  9],\n",
      "        [ 4,  8],\n",
      "        [ 3,  7],\n",
      "        [ 2,  6],\n",
      "        [ 1,  5],\n",
      "        [ 0,  4],\n",
      "        [ 1,  3],\n",
      "        [ 2,  2],\n",
      "        [ 3,  1],\n",
      "        [ 4,  0],\n",
      "        [ 5,  1],\n",
      "        [ 6,  2],\n",
      "        [ 7,  3],\n",
      "        [ 8,  4],\n",
      "        [ 9,  5],\n",
      "        [10,  6],\n",
      "        [11,  7],\n",
      "        [12,  8],\n",
      "        [13,  9],\n",
      "        [14, 10],\n",
      "        [15, 11],\n",
      "        [16, 12],\n",
      "        [17, 13],\n",
      "        [18, 14],\n",
      "        [19, 15],\n",
      "        [20, 16],\n",
      "        [21, 17],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, jordan, ,, but, the, [unused2], of, the, expansion, team, says, [unused2], could, run, operations, there, ,, if, he, wants, to, ., [SEP], head, [SEP], jordan], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[36, 42],\n",
      "        [35, 41],\n",
      "        [34, 40],\n",
      "        [33, 39],\n",
      "        [32, 38],\n",
      "        [31, 37],\n",
      "        [30, 36],\n",
      "        [29, 35],\n",
      "        [28, 34],\n",
      "        [27, 33],\n",
      "        [26, 32],\n",
      "        [25, 31],\n",
      "        [24, 30],\n",
      "        [23, 29],\n",
      "        [22, 28],\n",
      "        [21, 27],\n",
      "        [20, 26],\n",
      "        [19, 25],\n",
      "        [18, 24],\n",
      "        [17, 23],\n",
      "        [16, 22],\n",
      "        [15, 21],\n",
      "        [14, 20],\n",
      "        [13, 19],\n",
      "        [12, 18],\n",
      "        [11, 17],\n",
      "        [10, 16],\n",
      "        [ 9, 15],\n",
      "        [ 8, 14],\n",
      "        [ 7, 13],\n",
      "        [ 6, 12],\n",
      "        [ 5, 11],\n",
      "        [ 4, 10],\n",
      "        [ 3,  9],\n",
      "        [ 2,  8],\n",
      "        [ 1,  7],\n",
      "        [ 0,  6],\n",
      "        [ 1,  5],\n",
      "        [ 2,  4],\n",
      "        [ 3,  3],\n",
      "        [ 4,  2],\n",
      "        [ 5,  1],\n",
      "        [ 6,  0],\n",
      "        [ 7,  1],\n",
      "        [ 8,  2],\n",
      "        [ 9,  3],\n",
      "        [10,  4],\n",
      "        [11,  5],\n",
      "        [12,  6],\n",
      "        [13,  7],\n",
      "        [14,  8],\n",
      "        [15,  9],\n",
      "        [16, 10],\n",
      "        [17, 11],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, jordan, ,, but, the, [unused2], of, the, expansion, team, says, jordan, could, run, operations, there, ,, if, [unused2], wants, to, ., [SEP], head, [SEP], he], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[36, 49],\n",
      "        [35, 48],\n",
      "        [34, 47],\n",
      "        [33, 46],\n",
      "        [32, 45],\n",
      "        [31, 44],\n",
      "        [30, 43],\n",
      "        [29, 42],\n",
      "        [28, 41],\n",
      "        [27, 40],\n",
      "        [26, 39],\n",
      "        [25, 38],\n",
      "        [24, 37],\n",
      "        [23, 36],\n",
      "        [22, 35],\n",
      "        [21, 34],\n",
      "        [20, 33],\n",
      "        [19, 32],\n",
      "        [18, 31],\n",
      "        [17, 30],\n",
      "        [16, 29],\n",
      "        [15, 28],\n",
      "        [14, 27],\n",
      "        [13, 26],\n",
      "        [12, 25],\n",
      "        [11, 24],\n",
      "        [10, 23],\n",
      "        [ 9, 22],\n",
      "        [ 8, 21],\n",
      "        [ 7, 20],\n",
      "        [ 6, 19],\n",
      "        [ 5, 18],\n",
      "        [ 4, 17],\n",
      "        [ 3, 16],\n",
      "        [ 2, 15],\n",
      "        [ 1, 14],\n",
      "        [ 0, 13],\n",
      "        [ 1, 12],\n",
      "        [ 2, 11],\n",
      "        [ 3, 10],\n",
      "        [ 4,  9],\n",
      "        [ 5,  8],\n",
      "        [ 6,  7],\n",
      "        [ 7,  6],\n",
      "        [ 8,  5],\n",
      "        [ 9,  4],\n",
      "        [10,  3],\n",
      "        [11,  2],\n",
      "        [12,  1],\n",
      "        [13,  0],\n",
      "        [14,  1],\n",
      "        [15,  2],\n",
      "        [16,  3],\n",
      "        [17,  4],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, [unused3], may, not, want, jordan, ,, but, the, head, of, the, expansion, [unused3], says, jordan, could, run, operations, there, ,, if, he, wants, to, ., [SEP], wizards, [SEP], team], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[28, 40],\n",
      "        [27, 39],\n",
      "        [26, 38],\n",
      "        [25, 37],\n",
      "        [24, 36],\n",
      "        [23, 35],\n",
      "        [22, 34],\n",
      "        [21, 33],\n",
      "        [20, 32],\n",
      "        [19, 31],\n",
      "        [18, 30],\n",
      "        [17, 29],\n",
      "        [16, 28],\n",
      "        [15, 27],\n",
      "        [14, 26],\n",
      "        [13, 25],\n",
      "        [12, 24],\n",
      "        [11, 23],\n",
      "        [10, 22],\n",
      "        [ 9, 21],\n",
      "        [ 8, 20],\n",
      "        [ 7, 19],\n",
      "        [ 6, 18],\n",
      "        [ 5, 17],\n",
      "        [ 4, 16],\n",
      "        [ 3, 15],\n",
      "        [ 2, 14],\n",
      "        [ 1, 13],\n",
      "        [ 0, 12],\n",
      "        [ 1, 11],\n",
      "        [ 2, 10],\n",
      "        [ 3,  9],\n",
      "        [ 4,  8],\n",
      "        [ 5,  7],\n",
      "        [ 6,  6],\n",
      "        [ 7,  5],\n",
      "        [ 8,  4],\n",
      "        [ 9,  3],\n",
      "        [10,  2],\n",
      "        [11,  1],\n",
      "        [12,  0],\n",
      "        [13,  1],\n",
      "        [14,  2],\n",
      "        [15,  3],\n",
      "        [16,  4],\n",
      "        [17,  5],\n",
      "        [18,  6],\n",
      "        [19,  7],\n",
      "        [20,  8],\n",
      "        [21,  9],\n",
      "        [22, 10],\n",
      "        [23, 11],\n",
      "        [24, 12],\n",
      "        [25, 13],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, [unused3], may, not, want, jordan, ,, but, the, head, of, the, expansion, team, says, jordan, could, run, operations, [unused3], ,, if, he, wants, to, ., [SEP], wizards, [SEP], there], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[28, 46],\n",
      "        [27, 45],\n",
      "        [26, 44],\n",
      "        [25, 43],\n",
      "        [24, 42],\n",
      "        [23, 41],\n",
      "        [22, 40],\n",
      "        [21, 39],\n",
      "        [20, 38],\n",
      "        [19, 37],\n",
      "        [18, 36],\n",
      "        [17, 35],\n",
      "        [16, 34],\n",
      "        [15, 33],\n",
      "        [14, 32],\n",
      "        [13, 31],\n",
      "        [12, 30],\n",
      "        [11, 29],\n",
      "        [10, 28],\n",
      "        [ 9, 27],\n",
      "        [ 8, 26],\n",
      "        [ 7, 25],\n",
      "        [ 6, 24],\n",
      "        [ 5, 23],\n",
      "        [ 4, 22],\n",
      "        [ 3, 21],\n",
      "        [ 2, 20],\n",
      "        [ 1, 19],\n",
      "        [ 0, 18],\n",
      "        [ 1, 17],\n",
      "        [ 2, 16],\n",
      "        [ 3, 15],\n",
      "        [ 4, 14],\n",
      "        [ 5, 13],\n",
      "        [ 6, 12],\n",
      "        [ 7, 11],\n",
      "        [ 8, 10],\n",
      "        [ 9,  9],\n",
      "        [10,  8],\n",
      "        [11,  7],\n",
      "        [12,  6],\n",
      "        [13,  5],\n",
      "        [14,  4],\n",
      "        [15,  3],\n",
      "        [16,  2],\n",
      "        [17,  1],\n",
      "        [18,  0],\n",
      "        [19,  1],\n",
      "        [20,  2],\n",
      "        [21,  3],\n",
      "        [22,  4],\n",
      "        [23,  5],\n",
      "        [24,  6],\n",
      "        [25,  7],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, [unused3], may, not, want, [unused2], ,, but, the, head, of, the, expansion, team, says, jordan, could, run, operations, there, ,, if, he, wants, to, ., [SEP], wizards, [SEP], jordan], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[28, 32],\n",
      "        [27, 31],\n",
      "        [26, 30],\n",
      "        [25, 29],\n",
      "        [24, 28],\n",
      "        [23, 27],\n",
      "        [22, 26],\n",
      "        [21, 25],\n",
      "        [20, 24],\n",
      "        [19, 23],\n",
      "        [18, 22],\n",
      "        [17, 21],\n",
      "        [16, 20],\n",
      "        [15, 19],\n",
      "        [14, 18],\n",
      "        [13, 17],\n",
      "        [12, 16],\n",
      "        [11, 15],\n",
      "        [10, 14],\n",
      "        [ 9, 13],\n",
      "        [ 8, 12],\n",
      "        [ 7, 11],\n",
      "        [ 6, 10],\n",
      "        [ 5,  9],\n",
      "        [ 4,  8],\n",
      "        [ 3,  7],\n",
      "        [ 2,  6],\n",
      "        [ 1,  5],\n",
      "        [ 0,  4],\n",
      "        [ 1,  3],\n",
      "        [ 2,  2],\n",
      "        [ 3,  1],\n",
      "        [ 4,  0],\n",
      "        [ 5,  1],\n",
      "        [ 6,  2],\n",
      "        [ 7,  3],\n",
      "        [ 8,  4],\n",
      "        [ 9,  5],\n",
      "        [10,  6],\n",
      "        [11,  7],\n",
      "        [12,  8],\n",
      "        [13,  9],\n",
      "        [14, 10],\n",
      "        [15, 11],\n",
      "        [16, 12],\n",
      "        [17, 13],\n",
      "        [18, 14],\n",
      "        [19, 15],\n",
      "        [20, 16],\n",
      "        [21, 17],\n",
      "        [22, 18],\n",
      "        [23, 19],\n",
      "        [24, 20],\n",
      "        [25, 21],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, [unused3], may, not, want, jordan, ,, but, the, head, of, the, expansion, team, says, [unused2], could, run, operations, there, ,, if, he, wants, to, ., [SEP], wizards, [SEP], jordan], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[28, 42],\n",
      "        [27, 41],\n",
      "        [26, 40],\n",
      "        [25, 39],\n",
      "        [24, 38],\n",
      "        [23, 37],\n",
      "        [22, 36],\n",
      "        [21, 35],\n",
      "        [20, 34],\n",
      "        [19, 33],\n",
      "        [18, 32],\n",
      "        [17, 31],\n",
      "        [16, 30],\n",
      "        [15, 29],\n",
      "        [14, 28],\n",
      "        [13, 27],\n",
      "        [12, 26],\n",
      "        [11, 25],\n",
      "        [10, 24],\n",
      "        [ 9, 23],\n",
      "        [ 8, 22],\n",
      "        [ 7, 21],\n",
      "        [ 6, 20],\n",
      "        [ 5, 19],\n",
      "        [ 4, 18],\n",
      "        [ 3, 17],\n",
      "        [ 2, 16],\n",
      "        [ 1, 15],\n",
      "        [ 0, 14],\n",
      "        [ 1, 13],\n",
      "        [ 2, 12],\n",
      "        [ 3, 11],\n",
      "        [ 4, 10],\n",
      "        [ 5,  9],\n",
      "        [ 6,  8],\n",
      "        [ 7,  7],\n",
      "        [ 8,  6],\n",
      "        [ 9,  5],\n",
      "        [10,  4],\n",
      "        [11,  3],\n",
      "        [12,  2],\n",
      "        [13,  1],\n",
      "        [14,  0],\n",
      "        [15,  1],\n",
      "        [16,  2],\n",
      "        [17,  3],\n",
      "        [18,  4],\n",
      "        [19,  5],\n",
      "        [20,  6],\n",
      "        [21,  7],\n",
      "        [22,  8],\n",
      "        [23,  9],\n",
      "        [24, 10],\n",
      "        [25, 11],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, [unused3], may, not, want, jordan, ,, but, the, head, of, the, expansion, team, says, jordan, could, run, operations, there, ,, if, [unused2], wants, to, ., [SEP], wizards, [SEP], he], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[28, 49],\n",
      "        [27, 48],\n",
      "        [26, 47],\n",
      "        [25, 46],\n",
      "        [24, 45],\n",
      "        [23, 44],\n",
      "        [22, 43],\n",
      "        [21, 42],\n",
      "        [20, 41],\n",
      "        [19, 40],\n",
      "        [18, 39],\n",
      "        [17, 38],\n",
      "        [16, 37],\n",
      "        [15, 36],\n",
      "        [14, 35],\n",
      "        [13, 34],\n",
      "        [12, 33],\n",
      "        [11, 32],\n",
      "        [10, 31],\n",
      "        [ 9, 30],\n",
      "        [ 8, 29],\n",
      "        [ 7, 28],\n",
      "        [ 6, 27],\n",
      "        [ 5, 26],\n",
      "        [ 4, 25],\n",
      "        [ 3, 24],\n",
      "        [ 2, 23],\n",
      "        [ 1, 22],\n",
      "        [ 0, 21],\n",
      "        [ 1, 20],\n",
      "        [ 2, 19],\n",
      "        [ 3, 18],\n",
      "        [ 4, 17],\n",
      "        [ 5, 16],\n",
      "        [ 6, 15],\n",
      "        [ 7, 14],\n",
      "        [ 8, 13],\n",
      "        [ 9, 12],\n",
      "        [10, 11],\n",
      "        [11, 10],\n",
      "        [12,  9],\n",
      "        [13,  8],\n",
      "        [14,  7],\n",
      "        [15,  6],\n",
      "        [16,  5],\n",
      "        [17,  4],\n",
      "        [18,  3],\n",
      "        [19,  2],\n",
      "        [20,  1],\n",
      "        [21,  0],\n",
      "        [22,  1],\n",
      "        [23,  2],\n",
      "        [24,  3],\n",
      "        [25,  4],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, jordan, ,, but, the, head, of, the, expansion, [unused3], says, jordan, could, run, operations, [unused3], ,, if, he, wants, to, ., [SEP], team, [SEP], there], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[40, 46],\n",
      "        [39, 45],\n",
      "        [38, 44],\n",
      "        [37, 43],\n",
      "        [36, 42],\n",
      "        [35, 41],\n",
      "        [34, 40],\n",
      "        [33, 39],\n",
      "        [32, 38],\n",
      "        [31, 37],\n",
      "        [30, 36],\n",
      "        [29, 35],\n",
      "        [28, 34],\n",
      "        [27, 33],\n",
      "        [26, 32],\n",
      "        [25, 31],\n",
      "        [24, 30],\n",
      "        [23, 29],\n",
      "        [22, 28],\n",
      "        [21, 27],\n",
      "        [20, 26],\n",
      "        [19, 25],\n",
      "        [18, 24],\n",
      "        [17, 23],\n",
      "        [16, 22],\n",
      "        [15, 21],\n",
      "        [14, 20],\n",
      "        [13, 19],\n",
      "        [12, 18],\n",
      "        [11, 17],\n",
      "        [10, 16],\n",
      "        [ 9, 15],\n",
      "        [ 8, 14],\n",
      "        [ 7, 13],\n",
      "        [ 6, 12],\n",
      "        [ 5, 11],\n",
      "        [ 4, 10],\n",
      "        [ 3,  9],\n",
      "        [ 2,  8],\n",
      "        [ 1,  7],\n",
      "        [ 0,  6],\n",
      "        [ 1,  5],\n",
      "        [ 2,  4],\n",
      "        [ 3,  3],\n",
      "        [ 4,  2],\n",
      "        [ 5,  1],\n",
      "        [ 6,  0],\n",
      "        [ 7,  1],\n",
      "        [ 8,  2],\n",
      "        [ 9,  3],\n",
      "        [10,  4],\n",
      "        [11,  5],\n",
      "        [12,  6],\n",
      "        [13,  7],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, [unused2], ,, but, the, head, of, the, expansion, [unused3], says, jordan, could, run, operations, there, ,, if, he, wants, to, ., [SEP], jordan, [SEP], team], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[32, 40],\n",
      "        [31, 39],\n",
      "        [30, 38],\n",
      "        [29, 37],\n",
      "        [28, 36],\n",
      "        [27, 35],\n",
      "        [26, 34],\n",
      "        [25, 33],\n",
      "        [24, 32],\n",
      "        [23, 31],\n",
      "        [22, 30],\n",
      "        [21, 29],\n",
      "        [20, 28],\n",
      "        [19, 27],\n",
      "        [18, 26],\n",
      "        [17, 25],\n",
      "        [16, 24],\n",
      "        [15, 23],\n",
      "        [14, 22],\n",
      "        [13, 21],\n",
      "        [12, 20],\n",
      "        [11, 19],\n",
      "        [10, 18],\n",
      "        [ 9, 17],\n",
      "        [ 8, 16],\n",
      "        [ 7, 15],\n",
      "        [ 6, 14],\n",
      "        [ 5, 13],\n",
      "        [ 4, 12],\n",
      "        [ 3, 11],\n",
      "        [ 2, 10],\n",
      "        [ 1,  9],\n",
      "        [ 0,  8],\n",
      "        [ 1,  7],\n",
      "        [ 2,  6],\n",
      "        [ 3,  5],\n",
      "        [ 4,  4],\n",
      "        [ 5,  3],\n",
      "        [ 6,  2],\n",
      "        [ 7,  1],\n",
      "        [ 8,  0],\n",
      "        [ 9,  1],\n",
      "        [10,  2],\n",
      "        [11,  3],\n",
      "        [12,  4],\n",
      "        [13,  5],\n",
      "        [14,  6],\n",
      "        [15,  7],\n",
      "        [16,  8],\n",
      "        [17,  9],\n",
      "        [18, 10],\n",
      "        [19, 11],\n",
      "        [20, 12],\n",
      "        [21, 13],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, jordan, ,, but, the, head, of, the, expansion, [unused3], says, [unused2], could, run, operations, there, ,, if, he, wants, to, ., [SEP], team, [SEP], jordan], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[40, 42],\n",
      "        [39, 41],\n",
      "        [38, 40],\n",
      "        [37, 39],\n",
      "        [36, 38],\n",
      "        [35, 37],\n",
      "        [34, 36],\n",
      "        [33, 35],\n",
      "        [32, 34],\n",
      "        [31, 33],\n",
      "        [30, 32],\n",
      "        [29, 31],\n",
      "        [28, 30],\n",
      "        [27, 29],\n",
      "        [26, 28],\n",
      "        [25, 27],\n",
      "        [24, 26],\n",
      "        [23, 25],\n",
      "        [22, 24],\n",
      "        [21, 23],\n",
      "        [20, 22],\n",
      "        [19, 21],\n",
      "        [18, 20],\n",
      "        [17, 19],\n",
      "        [16, 18],\n",
      "        [15, 17],\n",
      "        [14, 16],\n",
      "        [13, 15],\n",
      "        [12, 14],\n",
      "        [11, 13],\n",
      "        [10, 12],\n",
      "        [ 9, 11],\n",
      "        [ 8, 10],\n",
      "        [ 7,  9],\n",
      "        [ 6,  8],\n",
      "        [ 5,  7],\n",
      "        [ 4,  6],\n",
      "        [ 3,  5],\n",
      "        [ 2,  4],\n",
      "        [ 1,  3],\n",
      "        [ 0,  2],\n",
      "        [ 1,  1],\n",
      "        [ 2,  0],\n",
      "        [ 3,  1],\n",
      "        [ 4,  2],\n",
      "        [ 5,  3],\n",
      "        [ 6,  4],\n",
      "        [ 7,  5],\n",
      "        [ 8,  6],\n",
      "        [ 9,  7],\n",
      "        [10,  8],\n",
      "        [11,  9],\n",
      "        [12, 10],\n",
      "        [13, 11],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, jordan, ,, but, the, head, of, the, expansion, [unused3], says, jordan, could, run, operations, there, ,, if, [unused2], wants, to, ., [SEP], team, [SEP], he], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[40, 49],\n",
      "        [39, 48],\n",
      "        [38, 47],\n",
      "        [37, 46],\n",
      "        [36, 45],\n",
      "        [35, 44],\n",
      "        [34, 43],\n",
      "        [33, 42],\n",
      "        [32, 41],\n",
      "        [31, 40],\n",
      "        [30, 39],\n",
      "        [29, 38],\n",
      "        [28, 37],\n",
      "        [27, 36],\n",
      "        [26, 35],\n",
      "        [25, 34],\n",
      "        [24, 33],\n",
      "        [23, 32],\n",
      "        [22, 31],\n",
      "        [21, 30],\n",
      "        [20, 29],\n",
      "        [19, 28],\n",
      "        [18, 27],\n",
      "        [17, 26],\n",
      "        [16, 25],\n",
      "        [15, 24],\n",
      "        [14, 23],\n",
      "        [13, 22],\n",
      "        [12, 21],\n",
      "        [11, 20],\n",
      "        [10, 19],\n",
      "        [ 9, 18],\n",
      "        [ 8, 17],\n",
      "        [ 7, 16],\n",
      "        [ 6, 15],\n",
      "        [ 5, 14],\n",
      "        [ 4, 13],\n",
      "        [ 3, 12],\n",
      "        [ 2, 11],\n",
      "        [ 1, 10],\n",
      "        [ 0,  9],\n",
      "        [ 1,  8],\n",
      "        [ 2,  7],\n",
      "        [ 3,  6],\n",
      "        [ 4,  5],\n",
      "        [ 5,  4],\n",
      "        [ 6,  3],\n",
      "        [ 7,  2],\n",
      "        [ 8,  1],\n",
      "        [ 9,  0],\n",
      "        [10,  1],\n",
      "        [11,  2],\n",
      "        [12,  3],\n",
      "        [13,  4],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, [unused2], ,, but, the, head, of, the, expansion, team, says, jordan, could, run, operations, [unused3], ,, if, he, wants, to, ., [SEP], jordan, [SEP], there], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[32, 46],\n",
      "        [31, 45],\n",
      "        [30, 44],\n",
      "        [29, 43],\n",
      "        [28, 42],\n",
      "        [27, 41],\n",
      "        [26, 40],\n",
      "        [25, 39],\n",
      "        [24, 38],\n",
      "        [23, 37],\n",
      "        [22, 36],\n",
      "        [21, 35],\n",
      "        [20, 34],\n",
      "        [19, 33],\n",
      "        [18, 32],\n",
      "        [17, 31],\n",
      "        [16, 30],\n",
      "        [15, 29],\n",
      "        [14, 28],\n",
      "        [13, 27],\n",
      "        [12, 26],\n",
      "        [11, 25],\n",
      "        [10, 24],\n",
      "        [ 9, 23],\n",
      "        [ 8, 22],\n",
      "        [ 7, 21],\n",
      "        [ 6, 20],\n",
      "        [ 5, 19],\n",
      "        [ 4, 18],\n",
      "        [ 3, 17],\n",
      "        [ 2, 16],\n",
      "        [ 1, 15],\n",
      "        [ 0, 14],\n",
      "        [ 1, 13],\n",
      "        [ 2, 12],\n",
      "        [ 3, 11],\n",
      "        [ 4, 10],\n",
      "        [ 5,  9],\n",
      "        [ 6,  8],\n",
      "        [ 7,  7],\n",
      "        [ 8,  6],\n",
      "        [ 9,  5],\n",
      "        [10,  4],\n",
      "        [11,  3],\n",
      "        [12,  2],\n",
      "        [13,  1],\n",
      "        [14,  0],\n",
      "        [15,  1],\n",
      "        [16,  2],\n",
      "        [17,  3],\n",
      "        [18,  4],\n",
      "        [19,  5],\n",
      "        [20,  6],\n",
      "        [21,  7],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 4, '_label_namespace': 'labels', '_label_id': 4}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, jordan, ,, but, the, head, of, the, expansion, team, says, [unused2], could, run, operations, [unused3], ,, if, he, wants, to, ., [SEP], jordan, [SEP], there], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[42, 46],\n",
      "        [41, 45],\n",
      "        [40, 44],\n",
      "        [39, 43],\n",
      "        [38, 42],\n",
      "        [37, 41],\n",
      "        [36, 40],\n",
      "        [35, 39],\n",
      "        [34, 38],\n",
      "        [33, 37],\n",
      "        [32, 36],\n",
      "        [31, 35],\n",
      "        [30, 34],\n",
      "        [29, 33],\n",
      "        [28, 32],\n",
      "        [27, 31],\n",
      "        [26, 30],\n",
      "        [25, 29],\n",
      "        [24, 28],\n",
      "        [23, 27],\n",
      "        [22, 26],\n",
      "        [21, 25],\n",
      "        [20, 24],\n",
      "        [19, 23],\n",
      "        [18, 22],\n",
      "        [17, 21],\n",
      "        [16, 20],\n",
      "        [15, 19],\n",
      "        [14, 18],\n",
      "        [13, 17],\n",
      "        [12, 16],\n",
      "        [11, 15],\n",
      "        [10, 14],\n",
      "        [ 9, 13],\n",
      "        [ 8, 12],\n",
      "        [ 7, 11],\n",
      "        [ 6, 10],\n",
      "        [ 5,  9],\n",
      "        [ 4,  8],\n",
      "        [ 3,  7],\n",
      "        [ 2,  6],\n",
      "        [ 1,  5],\n",
      "        [ 0,  4],\n",
      "        [ 1,  3],\n",
      "        [ 2,  2],\n",
      "        [ 3,  1],\n",
      "        [ 4,  0],\n",
      "        [ 5,  1],\n",
      "        [ 6,  2],\n",
      "        [ 7,  3],\n",
      "        [ 8,  4],\n",
      "        [ 9,  5],\n",
      "        [10,  6],\n",
      "        [11,  7],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, jordan, ,, but, the, head, of, the, expansion, team, says, jordan, could, run, operations, [unused3], ,, if, [unused2], wants, to, ., [SEP], there, [SEP], he], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[46, 49],\n",
      "        [45, 48],\n",
      "        [44, 47],\n",
      "        [43, 46],\n",
      "        [42, 45],\n",
      "        [41, 44],\n",
      "        [40, 43],\n",
      "        [39, 42],\n",
      "        [38, 41],\n",
      "        [37, 40],\n",
      "        [36, 39],\n",
      "        [35, 38],\n",
      "        [34, 37],\n",
      "        [33, 36],\n",
      "        [32, 35],\n",
      "        [31, 34],\n",
      "        [30, 33],\n",
      "        [29, 32],\n",
      "        [28, 31],\n",
      "        [27, 30],\n",
      "        [26, 29],\n",
      "        [25, 28],\n",
      "        [24, 27],\n",
      "        [23, 26],\n",
      "        [22, 25],\n",
      "        [21, 24],\n",
      "        [20, 23],\n",
      "        [19, 22],\n",
      "        [18, 21],\n",
      "        [17, 20],\n",
      "        [16, 19],\n",
      "        [15, 18],\n",
      "        [14, 17],\n",
      "        [13, 16],\n",
      "        [12, 15],\n",
      "        [11, 14],\n",
      "        [10, 13],\n",
      "        [ 9, 12],\n",
      "        [ 8, 11],\n",
      "        [ 7, 10],\n",
      "        [ 6,  9],\n",
      "        [ 5,  8],\n",
      "        [ 4,  7],\n",
      "        [ 3,  6],\n",
      "        [ 2,  5],\n",
      "        [ 1,  4],\n",
      "        [ 0,  3],\n",
      "        [ 1,  2],\n",
      "        [ 2,  1],\n",
      "        [ 3,  0],\n",
      "        [ 4,  1],\n",
      "        [ 5,  2],\n",
      "        [ 6,  3],\n",
      "        [ 7,  4],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, [unused2], ,, but, the, head, of, the, expansion, team, says, [unused2], could, run, operations, there, ,, if, he, wants, to, ., [SEP], jordan, [SEP], jordan], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[32, 42],\n",
      "        [31, 41],\n",
      "        [30, 40],\n",
      "        [29, 39],\n",
      "        [28, 38],\n",
      "        [27, 37],\n",
      "        [26, 36],\n",
      "        [25, 35],\n",
      "        [24, 34],\n",
      "        [23, 33],\n",
      "        [22, 32],\n",
      "        [21, 31],\n",
      "        [20, 30],\n",
      "        [19, 29],\n",
      "        [18, 28],\n",
      "        [17, 27],\n",
      "        [16, 26],\n",
      "        [15, 25],\n",
      "        [14, 24],\n",
      "        [13, 23],\n",
      "        [12, 22],\n",
      "        [11, 21],\n",
      "        [10, 20],\n",
      "        [ 9, 19],\n",
      "        [ 8, 18],\n",
      "        [ 7, 17],\n",
      "        [ 6, 16],\n",
      "        [ 5, 15],\n",
      "        [ 4, 14],\n",
      "        [ 3, 13],\n",
      "        [ 2, 12],\n",
      "        [ 1, 11],\n",
      "        [ 0, 10],\n",
      "        [ 1,  9],\n",
      "        [ 2,  8],\n",
      "        [ 3,  7],\n",
      "        [ 4,  6],\n",
      "        [ 5,  5],\n",
      "        [ 6,  4],\n",
      "        [ 7,  3],\n",
      "        [ 8,  2],\n",
      "        [ 9,  1],\n",
      "        [10,  0],\n",
      "        [11,  1],\n",
      "        [12,  2],\n",
      "        [13,  3],\n",
      "        [14,  4],\n",
      "        [15,  5],\n",
      "        [16,  6],\n",
      "        [17,  7],\n",
      "        [18,  8],\n",
      "        [19,  9],\n",
      "        [20, 10],\n",
      "        [21, 11],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n",
      "56\n",
      "{'tokens': [cnn, _, eng, _, 2003, ##0, ##50, ##9, _, 123, ##60, ##1, ., 13, news, story, 2003, -, 05, -, 09, 14, :, 02, :, 17, the, wizards, may, not, want, [unused2], ,, but, the, head, of, the, expansion, team, says, jordan, could, run, operations, there, ,, if, [unused2], wants, to, ., [SEP], jordan, [SEP], he], '_token_indexers': {'tokens': <allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer object at 0x7efb72e67828>}, '_indexed_tokens': None, '_indexer_name_to_indexed_token': None, '_token_index_to_indexer_name': None}\n",
      "100\n",
      "{'array': tensor([[32, 49],\n",
      "        [31, 48],\n",
      "        [30, 47],\n",
      "        [29, 46],\n",
      "        [28, 45],\n",
      "        [27, 44],\n",
      "        [26, 43],\n",
      "        [25, 42],\n",
      "        [24, 41],\n",
      "        [23, 40],\n",
      "        [22, 39],\n",
      "        [21, 38],\n",
      "        [20, 37],\n",
      "        [19, 36],\n",
      "        [18, 35],\n",
      "        [17, 34],\n",
      "        [16, 33],\n",
      "        [15, 32],\n",
      "        [14, 31],\n",
      "        [13, 30],\n",
      "        [12, 29],\n",
      "        [11, 28],\n",
      "        [10, 27],\n",
      "        [ 9, 26],\n",
      "        [ 8, 25],\n",
      "        [ 7, 24],\n",
      "        [ 6, 23],\n",
      "        [ 5, 22],\n",
      "        [ 4, 21],\n",
      "        [ 3, 20],\n",
      "        [ 2, 19],\n",
      "        [ 1, 18],\n",
      "        [ 0, 17],\n",
      "        [ 1, 16],\n",
      "        [ 2, 15],\n",
      "        [ 3, 14],\n",
      "        [ 4, 13],\n",
      "        [ 5, 12],\n",
      "        [ 6, 11],\n",
      "        [ 7, 10],\n",
      "        [ 8,  9],\n",
      "        [ 9,  8],\n",
      "        [10,  7],\n",
      "        [11,  6],\n",
      "        [12,  5],\n",
      "        [13,  4],\n",
      "        [14,  3],\n",
      "        [15,  2],\n",
      "        [16,  1],\n",
      "        [17,  0],\n",
      "        [18,  1],\n",
      "        [19,  2],\n",
      "        [20,  3],\n",
      "        [21,  4],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99],\n",
      "        [99, 99]]), 'padding_value': 0, 'dtype': <class 'numpy.float32'>}\n",
      "{'label': 0, '_label_namespace': 'labels', '_label_id': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ace05_reader = ACE05Reader(lang='en', nlp = StanfordCoreNLP('http://140.109.19.190', port=9000, lang='en'))\n",
    "    \n",
    "    token_indexer = PretrainedBertIndexer(\n",
    "        pretrained_model=\"bert-base-uncased\",\n",
    "#         do_lowercase=False               # for cased condition\n",
    "    )\n",
    " \n",
    "\t# AllenNLP DatasetReader\n",
    "    reader = RelationDatasetReader(\n",
    "        is_training=True, \n",
    "        ace05_reader=ace05_reader, \n",
    "        token_indexers={\"tokens\": token_indexer}\n",
    "    )\n",
    "\n",
    "    train_ds = reader.read(train_path)\n",
    "    print(len(train_ds))\n",
    "#     for e in range(20):\n",
    "#         print(len(vars(train_ds[e].fields['tokens'])['tokens']))\n",
    "#         print(vars(train_ds[e].fields['tokens']))\n",
    "#         print(len(vars(train_ds[0].fields['arg_idx'])['array']))\n",
    "#         print(vars(train_ds[e].fields['arg_idx']))\n",
    "#         print(vars(train_ds[e].fields['label']))\n",
    "    \n",
    "    # user-defined new label\n",
    "#     new_token = {\"bert-pretrained\": [\"[\" + i +\"]\" for i in e_type2idx.keys()]}\n",
    "#     print(type(new_token[\"tokens\"]))\n",
    "#     vocab = Vocabulary(tokens_to_add=new_token)\n",
    "    vocab = Vocabulary()\n",
    "#     print(vocab.get_index_to_token_vocabulary(namespace=\"token\"))\n",
    "#     print(type(vocab))\n",
    "    iterator = BucketIterator(batch_size=config.batch_size, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    bert_embedder = PretrainedBertEmbedder(\n",
    "        pretrained_model=\"bert-base-uncased\",\n",
    "        top_layer_only=True, # conserve memory   \n",
    "    )\n",
    "    \n",
    "#     bert_embedder.vocab={\"[per]\":500000 , \"[org]\": 60000000}\n",
    "    \n",
    "    word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                                # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                               allow_unmatched_keys = True)\n",
    "#     print(vocab.by_name(\"default\"))\n",
    "#     print(token_indexer.tokens_to_indices([Token(text=\"[unused100]\")], vocab, \"test\"))\n",
    "#     print(vocab.get_index_to_token_vocabulary(namespace=\"bert\"))\n",
    "    model = BERT(word_embeddings)\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # training\n",
    "    from allennlp.training.trainer import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds,\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=config.epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # train the model \n",
    "#     metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/model_checkpoint/bert_model_checkpoint/bert_modify_seq/model.th'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d915930f2e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"model.th\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/model_checkpoint/bert_model_checkpoint/bert_modify_seq/model.th'"
     ]
    }
   ],
   "source": [
    "    # load model\n",
    "    model.load_state_dict(T.load(model_folder + \"model.th\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # save \n",
    "    with open(model_folder+'model.th', 'wb') as f:\n",
    "        T.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # training data analysis\n",
    "    seq_iterator = BasicIterator(batch_size=config.batch_size)\n",
    "    seq_iterator.index_with(vocab)\n",
    "    \n",
    "    predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "    train_preds = predictor.predict(train_ds) \n",
    "    \n",
    "    label_types = [r_idx2label.get(i.fields['label'].label) for i in train_ds]\n",
    "    predict_types = [r_idx2label.get(i) for i in np.argmax(train_preds, axis=-1)]\n",
    "    err_analyze(train_ds, label_types, predict_types, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plot_comfusion_matrix(label_types, predict_types, output_path, \"train_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # testing data analysis\n",
    "    \n",
    "    # AllenNLP DatasetReader\n",
    "    reader = RelationDatasetReader(\n",
    "        is_training=True, \n",
    "        ace05_reader=ace05_reader, \n",
    "        token_indexers={\"tokens\": token_indexer}\n",
    "    )\n",
    "    \n",
    "    test_ds = reader.read(test_path)\n",
    "    print(len(test_ds))\n",
    "    seq_iterator = BasicIterator(batch_size=config.batch_size)\n",
    "    seq_iterator.index_with(vocab)\n",
    "    \n",
    "    predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "    test_preds = predictor.predict(test_ds) \n",
    "    \n",
    "    label_types = [r_idx2label.get(i.fields['label'].label) for i in test_ds]\n",
    "    predict_types = [r_idx2label.get(i) for i in np.argmax(test_preds, axis=-1)]  \n",
    "    \n",
    "    err_analyze(test_ds, label_types, predict_types, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plot_comfusion_matrix(label_types, predict_types, output_path, \"test_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
