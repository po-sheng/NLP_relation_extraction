{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/work/multi_doc_analyzer\")\n",
    "sys.path.append(\"/work/relation_extraction/Bert_model/baseline/data/\")\n",
    "\n",
    "import torch as T\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda\n",
    "from allennlp.nn import util as nn_util\n",
    "from multi_doc_analyzer.structure.structure import *\n",
    "from multi_doc_analyzer.tokenization.tokenizer import MDATokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.fields import TextField, LabelField, ArrayField\n",
    "\n",
    "from tacred_preprocess import DataLoader\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.iterators import BucketIterator, DataIterator, BasicIterator\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "import random\n",
    "\n",
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as prs \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/work/multi-relation-extraction/\")\n",
    "from evaluate import compute_macro_PRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/work/tacred/data/json/train.json\"\n",
    "test_path = \"/work/tacred/data/json/test.json\"\n",
    "model_folder = \"/work/model_checkpoint/bert_model_checkpoint/baseline/tacred\"\n",
    "output_path = \"/work/relation_extraction/Bert_model/baseline/analysis/tacred/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "config = Config(\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    lr=3e-4,                # learning rate\n",
    "    epochs=150,\n",
    "    hidden_sz=128,\n",
    "    arg_sz=20,\n",
    "    max_seq_len=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPU = T.cuda.is_available()\n",
    "USE_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb6600ed910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed for both CPU and CUDA\n",
    "T.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ace05_set_reader import ACE05Reader\n",
    "# train_path = \"/work/LDC2006T06/dataset/train/\"\n",
    "# reader = ACE05Reader(lang='en')\n",
    "# doc_dicts = reader.read(train_path)\n",
    "# tokenizer = MDATokenizer('bert-en')\n",
    "# for doc in doc_dicts.values():\n",
    "#     tokenizer.annotate_document(doc)\n",
    "#     for s in doc.sentences: \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_label2idx = {'X': 0, 'O': 1, 'PERSON': 2, 'ORGANIZATION': 3, 'DATE': 4, 'NUMBER': 5, 'TITLE': 6, 'COUNTRY': 7, 'LOCATION': 8, 'CITY': 9, 'MISC': 10, 'STATE_OR_PROVINCE': 11, 'DURATION': 12, 'NATIONALITY': 13, 'CAUSE_OF_DEATH': 14, 'CRIMINAL_CHARGE': 15, 'RELIGION': 16, 'URL': 17, 'IDEOLOGY': 18}\n",
    "\n",
    "r_label2idx = {'no_relation': 0, 'per:title': 1, 'org:top_members/employees': 2, 'per:employee_of': 3, 'org:alternate_names': 4, 'org:country_of_headquarters': 5, 'per:countries_of_residence': 6, 'org:city_of_headquarters': 7, 'per:cities_of_residence': 8, 'per:age': 9, 'per:stateorprovinces_of_residence': 10, 'per:origin': 11, 'org:subsidiaries': 12, 'org:parents': 13, 'per:spouse': 14, 'org:stateorprovince_of_headquarters': 15, 'per:children': 16, 'per:other_family': 17, 'per:alternate_names': 18, 'org:members': 19, 'per:siblings': 20, 'per:schools_attended': 21, 'per:parents': 22, 'per:date_of_death': 23, 'org:member_of': 24, 'org:founded_by': 25, 'org:website': 26, 'per:cause_of_death': 27, 'org:political/religious_affiliation': 28, 'org:founded': 29, 'per:city_of_death': 30, 'org:shareholders': 31, 'org:number_of_employees/members': 32, 'per:date_of_birth': 33, 'per:city_of_birth': 34, 'per:charges': 35, 'per:stateorprovince_of_death': 36, 'per:religion': 37, 'per:stateorprovince_of_birth': 38, 'per:country_of_birth': 39, 'org:dissolved': 40, 'per:country_of_death': 41}\n",
    "\n",
    "# r_label2idx = {'PHYS': 1, 'PART-WHOLE': 2, 'PER-SOC': 3, 'ORG-AFF': 4, 'ART': 5, 'GEN-AFF': 6, 'NONE': 0}\n",
    "\n",
    "r_idx2label = {v: k for k, v in r_label2idx.items()}\n",
    "\n",
    "class RelationDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    Reads Structure object formatted datasets files, and creates AllenNLP instances.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer: Tokenizer=None, token_indexers: Dict[str, TokenIndexer]=None, \n",
    "                 MAX_WORDPIECES: int=config.max_seq_len, \n",
    "                 is_training = False, reader: DataLoader=None):\n",
    "        # make sure results may be reproduced when sampling...\n",
    "        super().__init__(lazy=False)\n",
    "        random.seed(0)\n",
    "        self.is_training = is_training\n",
    "        self.reader = reader\n",
    "        \n",
    "        # NOTE AllenNLP automatically adds [CLS] and [SEP] word peices in the begining and end of the context,\n",
    "        # therefore we need to subtract 2\n",
    "        self.MAX_WORDPIECES = MAX_WORDPIECES - 2\n",
    "        \n",
    "        self.tokenizer = tokenizer or WordTokenizer()\n",
    "        \n",
    "        # BERT specific init\n",
    "        self._token_indexers = token_indexers\n",
    "\n",
    "    def text_to_instance(self, sentence: Sentence) -> Instance:\n",
    "\n",
    "        field = {}\n",
    "        \n",
    "        # tokens\n",
    "        sentence_field = TextField(sentence[\"tokens\"], self._token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "\n",
    "#         arg_vec = T.tensor([[0, 0] for i in range(len(sentence.tokens) + 2)], dtype=T.long)   # long type to feed into embedding layer\n",
    "        arg_vec = T.tensor([0 for i in range(config.max_seq_len)], dtype=T.long)   # long type to feed into embedding layer\n",
    "\n",
    "        # position \n",
    "        for i in range(sentence[\"len\"]):\n",
    "            arg_vec[i] = sentence[\"position\"][i]    # arg_l position, i-1 for [CLS]\n",
    "        fields[\"arg_idx\"] = ArrayField(arg_vec)\n",
    "\n",
    "        # information for each sentence length\n",
    "#         fields[\"sen_len\"] = LabelField(sentence[\"len\"], skip_indexing=True)\n",
    "\n",
    "        # relation\n",
    "        if self.is_training:\n",
    "            fields[\"label\"] = LabelField(sentence[\"relation\"], skip_indexing=True)\n",
    "        \n",
    "        return Instance(fields)\n",
    "    \n",
    "    def _read(self, file_path: str)->Iterator: \n",
    "        sen_list = self.reader.data\n",
    "        for s in sen_list:\n",
    "            if s[\"len\"] <= config.max_seq_len:\n",
    "                yield self.text_to_instance(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ace05_reader = ACE05Reader(lang='en')\n",
    "\n",
    "# token_indexer = PretrainedBertIndexer(\n",
    "#     pretrained_model=\"bert-base-uncased\",\n",
    "# #         max_pieces=config.max_seq_len,\n",
    "# #         do_lowercase=False               # for cased condition\n",
    "# )\n",
    "\n",
    "# # AllenNLP DatasetReader\n",
    "# reader = RelationDatasetReader(\n",
    "#     is_training=True, \n",
    "#     ace05_reader=ace05_reader, \n",
    "#     tokenizer=lambda s: token_indexer.wordpiece_tokenizer(s),\n",
    "#     token_indexers={\"tokens\": token_indexer}\n",
    "# )\n",
    "\n",
    "# train_ds = reader.read(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                out_sz: int=len(r_label2idx)):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self._entity_embeddings = T.nn.Embedding(num_embeddings=len(e_label2idx), embedding_dim=config.arg_sz, padding_idx=0)\n",
    "        self.gru = T.nn.GRU(word_embeddings.get_output_dim()+config.arg_sz, config.hidden_sz, batch_first=True)\n",
    "        self.projection = nn.Linear(config.hidden_sz, out_sz)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, tokens: Dict[str, T.tensor], arg_idx: T.tensor, label: T.tensor = None) -> Dict[str, T.tensor]:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        \n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        pad_len = embeddings.shape[-2]\n",
    "\n",
    "        arg_idx = arg_idx[:,:pad_len]\n",
    "        arg_idx = arg_idx.type(T.long)\n",
    "#         print(arg_idx)\n",
    "        arg_emb = self._entity_embeddings(arg_idx)\n",
    "#         print(arg_emb)\n",
    "        concat = T.cat((embeddings, arg_emb), -1)\n",
    "        ot, ht = self.gru(concat, None) # revise this \"None\"\n",
    "        ot = ot[:,-1,:]    \n",
    "        class_logits = self.projection(ot)\n",
    "#         print(vars(label))\n",
    "        output = {\"class_logits\": class_logits}\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit # the sigmoid function\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> np.ndarray:\n",
    "        out_dict = self.model(**batch)\n",
    "        return expit(tonp(out_dict[\"class_logits\"]))\n",
    "    \n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = tqdm(pred_generator, total=self.iterator.get_num_batches(ds))\n",
    "        preds = []\n",
    "        with T.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                preds.append(self._extract_data(batch))\n",
    "        return np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comfusion_matrix(label_classes, predict_classes, out_folder, file_name):\n",
    "    label_types = list(r_idx2label.values())\n",
    "\n",
    "    cm = confusion_matrix(label_classes, predict_classes, label_types)\n",
    "    print(cm)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    for (i, j), z in np.ndenumerate(cm):\n",
    "        ax.text(j, i, '{:0.0f}'.format(z), ha='center', va='center', color='white')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + label_types)\n",
    "    ax.set_yticklabels([''] + label_types)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    plt.savefig(out_folder + 'confusion_matrix_' + file_name + '.png')\n",
    "    plt.show()\n",
    "\n",
    "    pre, recall, f1, sup = prs(label_classes, predict_classes, average='macro')\n",
    "    \n",
    "    print(\"Accuracy:\", sum(cm[i][i] for i in range(len(cm))) / len(label_classes))\n",
    "    print(\"Precision:\", pre)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_analyze(ds, true, pred, opt):\n",
    "    \n",
    "    # classify different kinds of error\n",
    "    detail = [[[] for j in range(len(r_label2idx))] for i in range(len(r_label2idx))]\n",
    "    for i in range(len(ds)):\n",
    "         if true[i] != pred[i]:\n",
    "            detail[r_label2idx[true[i]]][r_label2idx[pred[i]]].append(i)\n",
    "    \n",
    "    # print into a csv file\n",
    "    with open(output_path + \"error_detail_\" + opt + \".csv\", \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Sentence\", \"Two_Entity\", \"Predict\", \"Label\", \"idx\"])\n",
    "        for j in range(len(detail)):\n",
    "            for k in range(len(detail)):\n",
    "                with_element = 0\n",
    "                if k == j:\n",
    "                    continue\n",
    "                for i in detail[j][k]:\n",
    "                    with_element = 1\n",
    "                    ent1 = []\n",
    "                    ent2 = []\n",
    "                    seq = 0\n",
    "                    prev = 0\n",
    "                    for g in range(len(vars(ds[i].fields['arg_idx'])['array'])):\n",
    "                        if int(vars(ds[i].fields['arg_idx'])['array'][g]) != 1 and int(vars(ds[i].fields['arg_idx'])['array'][g]) != 0:\n",
    "                            if ent1 == [] or (seq == 1 and prev == vars(ds[i].fields['arg_idx'])['array'][g]):\n",
    "                                prev = vars(ds[i].fields['arg_idx'])['array'][g]\n",
    "                                seq = 1\n",
    "                                ent1.append(vars(ds[i].fields['tokens'])['tokens'][g-1])\n",
    "                            else:\n",
    "                                seq = 2\n",
    "                                ent2.append(vars(ds[i].fields['tokens'])['tokens'][g-1])\n",
    "                        else: \n",
    "                            seq = 0\n",
    "    \n",
    "                    tostr = lambda a: [str(a[i]) for i in range(len(a))] \n",
    "                    writer.writerow([\" \".join(tostr(vars(ds[i].fields['tokens'])['tokens'])), [ent1, ent2], pred[i], true[i], i])\n",
    "                if with_element == 1:\n",
    "                    writer.writerow(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68124/68124 [02:28<00:00, 459.65it/s]\n",
      "68000it [00:28, 2349.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    tacred_reader = DataLoader(train_path, list(e_label2idx.keys()), list(r_label2idx.keys()), {\"lower\": True})\n",
    "    \n",
    "    token_indexer = PretrainedBertIndexer(\n",
    "        pretrained_model=\"bert-base-uncased\",\n",
    "#         max_pieces=config.max_seq_len,\n",
    "#         do_lowercase=False               # for cased condition\n",
    "    )\n",
    " \n",
    "\t# AllenNLP DatasetReader\n",
    "    reader = RelationDatasetReader(\n",
    "        is_training=True, \n",
    "        reader=tacred_reader, \n",
    "        tokenizer=lambda s: token_indexer.wordpiece_tokenizer(s),\n",
    "        token_indexers={\"tokens\": token_indexer}\n",
    "    )\n",
    "\n",
    "    train_ds = reader.read(train_path)\n",
    "    print(len(train_ds))\n",
    "#     for e in range(20):\n",
    "#         print(len(vars(train_ds[e].fields['tokens'])['tokens']))\n",
    "#         print(vars(train_ds[e].fields['tokens']))\n",
    "#         print(type(vars(train_ds[0].fields['arg_idx'])['array'][0]))\n",
    "#         print(vars(train_ds[e].fields['arg_idx']))\n",
    "#         print(vars(train_ds[e].fields['label']))\n",
    "    \n",
    "    vocab = Vocabulary()\n",
    "    iterator = BucketIterator(batch_size=config.batch_size, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "    iterator.index_with(vocab)\n",
    "\n",
    "    bert_embedder = PretrainedBertEmbedder(\n",
    "        pretrained_model=\"bert-base-uncased\",\n",
    "        top_layer_only=True, # conserve memory   \n",
    "    )\n",
    "    word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                                # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                               allow_unmatched_keys = True)\n",
    "#     print(token_indexer.tokens_to_indices([Token(text=\"[unused0]\")], vocab, \"test\"))\n",
    "    \n",
    "    model = BERT(word_embeddings)\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # training\n",
    "    from allennlp.training.trainer import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds,\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=config.epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.9713 ||: 100%|██████████| 1063/1063 [02:17<00:00,  7.71it/s]\n",
      "loss: 0.6422 ||: 100%|██████████| 1063/1063 [01:59<00:00,  8.89it/s]\n",
      "loss: 0.4661 ||: 100%|██████████| 1063/1063 [01:59<00:00,  9.62it/s]\n",
      "loss: 0.3827 ||: 100%|██████████| 1063/1063 [02:00<00:00,  8.59it/s]\n",
      "loss: 0.3333 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.83it/s]\n",
      "loss: 0.3042 ||: 100%|██████████| 1063/1063 [02:02<00:00,  6.27it/s]\n",
      "loss: 0.2801 ||: 100%|██████████| 1063/1063 [02:01<00:00,  9.17it/s]\n",
      "loss: 0.2600 ||: 100%|██████████| 1063/1063 [02:03<00:00,  7.33it/s]\n",
      "loss: 0.2419 ||: 100%|██████████| 1063/1063 [02:01<00:00,  7.84it/s]\n",
      "loss: 0.2239 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.80it/s]\n",
      "loss: 0.2106 ||: 100%|██████████| 1063/1063 [02:01<00:00, 10.27it/s]\n",
      "loss: 0.1978 ||: 100%|██████████| 1063/1063 [02:01<00:00,  7.97it/s]\n",
      "loss: 0.1831 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.72it/s]\n",
      "loss: 0.1731 ||: 100%|██████████| 1063/1063 [02:02<00:00,  8.71it/s]\n",
      "loss: 0.1622 ||: 100%|██████████| 1063/1063 [02:02<00:00,  9.17it/s]\n",
      "loss: 0.1537 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.74it/s]\n",
      "loss: 0.1444 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.94it/s]\n",
      "loss: 0.1362 ||: 100%|██████████| 1063/1063 [02:04<00:00,  9.31it/s]\n",
      "loss: 0.1287 ||: 100%|██████████| 1063/1063 [01:58<00:00,  7.54it/s]\n",
      "loss: 0.1234 ||: 100%|██████████| 1063/1063 [02:02<00:00,  8.69it/s]\n",
      "loss: 0.1145 ||: 100%|██████████| 1063/1063 [02:01<00:00,  7.39it/s]\n",
      "loss: 0.1115 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.82it/s]\n",
      "loss: 0.1050 ||: 100%|██████████| 1063/1063 [02:01<00:00,  9.09it/s]\n",
      "loss: 0.1000 ||: 100%|██████████| 1063/1063 [02:01<00:00,  7.58it/s]\n",
      "loss: 0.0968 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.74it/s]\n",
      "loss: 0.0900 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.74it/s]\n",
      "loss: 0.0864 ||: 100%|██████████| 1063/1063 [02:01<00:00,  7.74it/s]\n",
      "loss: 0.0849 ||: 100%|██████████| 1063/1063 [02:03<00:00,  8.58it/s]\n",
      "loss: 0.0819 ||: 100%|██████████| 1063/1063 [02:01<00:00,  9.76it/s]\n",
      "loss: 0.0790 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.70it/s]\n",
      "loss: 0.0742 ||: 100%|██████████| 1063/1063 [02:01<00:00,  9.47it/s]\n",
      "loss: 0.0723 ||: 100%|██████████| 1063/1063 [02:01<00:00,  6.11it/s]\n",
      "loss: 0.0702 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.75it/s]\n",
      "loss: 0.0663 ||: 100%|██████████| 1063/1063 [02:01<00:00, 10.21it/s]\n",
      "loss: 0.0649 ||: 100%|██████████| 1063/1063 [02:01<00:00, 10.59it/s]\n",
      "loss: 0.0639 ||: 100%|██████████| 1063/1063 [02:01<00:00,  9.02it/s]\n",
      "loss: 0.0617 ||: 100%|██████████| 1063/1063 [02:04<00:00,  9.25it/s]\n",
      "loss: 0.0616 ||: 100%|██████████| 1063/1063 [02:01<00:00,  6.54it/s]\n",
      "loss: 0.0573 ||: 100%|██████████| 1063/1063 [02:03<00:00, 11.44it/s]\n",
      "loss: 0.0586 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.74it/s]\n",
      "loss: 0.0552 ||: 100%|██████████| 1063/1063 [02:03<00:00,  8.64it/s]\n",
      "loss: 0.0544 ||: 100%|██████████| 1063/1063 [02:02<00:00,  8.24it/s]\n",
      "loss: 0.0534 ||: 100%|██████████| 1063/1063 [02:00<00:00,  8.83it/s]\n",
      "loss: 0.0531 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.77it/s]\n",
      "loss: 0.0508 ||: 100%|██████████| 1063/1063 [02:04<00:00,  9.92it/s]\n",
      "loss: 0.0515 ||: 100%|██████████| 1063/1063 [02:00<00:00,  8.80it/s]\n",
      "loss: 0.0477 ||: 100%|██████████| 1063/1063 [02:04<00:00,  8.35it/s]\n",
      "loss: 0.0495 ||: 100%|██████████| 1063/1063 [02:01<00:00,  9.63it/s]\n",
      "loss: 0.0467 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.73it/s]\n",
      "loss: 0.0447 ||: 100%|██████████| 1063/1063 [02:02<00:00,  8.71it/s]\n",
      "loss: 0.0467 ||: 100%|██████████| 1063/1063 [02:00<00:00,  9.67it/s]\n",
      "loss: 0.0418 ||: 100%|██████████| 1063/1063 [01:57<00:00, 11.21it/s]\n",
      "loss: 0.0445 ||: 100%|██████████| 1063/1063 [01:58<00:00,  8.89it/s]\n",
      "loss: 0.0436 ||: 100%|██████████| 1063/1063 [01:57<00:00, 10.62it/s]\n",
      "loss: 0.0437 ||: 100%|██████████| 1063/1063 [02:00<00:00,  9.09it/s]\n",
      "loss: 0.0395 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.71it/s]\n",
      "loss: 0.0422 ||: 100%|██████████| 1063/1063 [02:03<00:00, 10.74it/s]\n",
      "loss: 0.0426 ||: 100%|██████████| 1063/1063 [02:02<00:00,  7.44it/s]\n",
      "loss: 0.0386 ||: 100%|██████████| 1063/1063 [02:01<00:00,  9.64it/s]\n",
      "loss: 0.0389 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.73it/s]\n",
      "loss: 0.0400 ||: 100%|██████████| 1063/1063 [02:01<00:00,  6.82it/s]\n",
      "loss: 0.0369 ||: 100%|██████████| 1063/1063 [01:59<00:00,  8.87it/s]\n",
      "loss: 0.0372 ||: 100%|██████████| 1063/1063 [01:59<00:00,  9.04it/s]\n",
      "loss: 0.0365 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.72it/s]\n",
      "loss: 0.0366 ||: 100%|██████████| 1063/1063 [02:02<00:00,  7.68it/s]\n",
      "loss: 0.0365 ||: 100%|██████████| 1063/1063 [02:02<00:00,  8.93it/s]\n",
      "loss: 0.0359 ||: 100%|██████████| 1063/1063 [02:03<00:00, 12.70it/s]\n",
      "loss: 0.0353 ||: 100%|██████████| 1063/1063 [02:01<00:00, 10.49it/s]\n",
      "loss: 0.0340 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.72it/s]\n",
      "loss: 0.0325 ||: 100%|██████████| 1063/1063 [02:02<00:00,  6.96it/s]\n",
      "loss: 0.0349 ||: 100%|██████████| 1063/1063 [02:02<00:00,  8.68it/s]\n",
      "loss: 0.0328 ||: 100%|██████████| 1063/1063 [02:01<00:00,  8.72it/s]\n",
      "loss: 0.0333 ||:  67%|██████▋   | 711/1063 [01:22<00:37,  9.49it/s]"
     ]
    }
   ],
   "source": [
    "    # train the model \n",
    "    metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # load model\n",
    "#     model.load_state_dict(T.load(model_folder + \"/model.th\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # save \n",
    "    with open(model_folder+'model.th', 'wb') as f:\n",
    "        T.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # training data analysis\n",
    "    seq_iterator = BasicIterator(batch_size=config.batch_size)\n",
    "    seq_iterator.index_with(vocab)\n",
    "    \n",
    "    predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "    train_preds = predictor.predict(train_ds) \n",
    "    \n",
    "    label_types = [r_idx2label.get(i.fields['label'].label) for i in train_ds]\n",
    "    predict_types = [r_idx2label.get(i) for i in np.argmax(train_preds, axis=-1)]\n",
    "    err_analyze(train_ds, label_types, predict_types, \"train\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plot_comfusion_matrix(label_types, predict_types, output_path, \"train_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # testing data analysis\n",
    "    tacred_reader = DataLoader(test_path, list(e_label2idx.keys()), list(r_label2idx.keys()), {\"lower\": True})\n",
    "    # AllenNLP DatasetReader\n",
    "    reader = RelationDatasetReader(\n",
    "        is_training=True, \n",
    "        reader=tacred_reader, \n",
    "        tokenizer=lambda s: token_indexer.wordpiece_tokenizer(s),\n",
    "        token_indexers={\"tokens\": token_indexer}\n",
    "    )\n",
    "    \n",
    "    test_ds = reader.read(test_path)\n",
    "    print(len(test_ds))\n",
    "    seq_iterator = BasicIterator(batch_size=config.batch_size)\n",
    "    seq_iterator.index_with(vocab)\n",
    "    \n",
    "    predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "    test_preds = predictor.predict(test_ds) \n",
    "    \n",
    "    label_types = [r_idx2label.get(i.fields['label'].label) for i in test_ds]\n",
    "    predict_types = [r_idx2label.get(i) for i in np.argmax(test_preds, axis=-1)]  \n",
    "    \n",
    "    err_analyze(test_ds, label_types, predict_types, \"test\")\n",
    "#     pre = np.array([r_label2idx[i] for i in predict_types])\n",
    "#     lab = np.array([r_label2idx[i] for i in label_types])\n",
    "#     compute_macro_PRF(pre, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plot_comfusion_matrix(label_types, predict_types, output_path, \"test_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
