{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REQUIREMENTS ##\n",
    "\n",
    "import sys, os, _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## CONSTANTS ##\n",
    "\n",
    "data_dir = '../testing_data'                       # Directory for Data and Other files\n",
    "ckpt_dir = '../checkpoint'                 # Directory for Saving Checkpoints \n",
    "word_embd_dir = '../checkpoint/word_embd'  # Directory for Saving Checkpoints of Word Embedding Layer\n",
    "model_dir = '../checkpoint/model3v2'       # Directory for Saving Checkpoints of Model\n",
    "summary_dir = model_dir + '/summary'       # Directory for Saving Summaries of training\n",
    "\n",
    "word_embd_dim = 100                        # Dimension of embedding layer for words\n",
    "pos_embd_dim = 25                          # Dimension of embedding layer for POS Tags\n",
    "dep_embd_dim = 25                          # Dimension of embedding layer for Dependency Types\n",
    "\n",
    "word_vocab_size = 400001                   # Vocab size for Words\n",
    "pos_vocab_size = 44                       # Vocab size for POS Tags\n",
    "dep_vocab_size = 45                        # Vocab size for Dependency Types\n",
    "\n",
    "relation_classes = 7                      # No. of Relation Classes\n",
    "\n",
    "state_size = 100                           # Dimension of States of LSTM-RNNs\n",
    "batch_size = 10                            # Batch Size for training\n",
    "\n",
    "max_len_seq = 90                           # Maximum length of sentences\n",
    "max_len_path = 20                          # Maximum length of lca paths\n",
    "max_num_child = 20                         # Maximum no. of childrens in Dependency Tree\n",
    "\n",
    "lambda_l2 = 0.0001                         # lambda of l2-regulaizer\n",
    "init_learning_rate = 0.001                 # Initial Learning Rate\n",
    "decay_steps = 2000                         # Decay Steps for Learning Rate\n",
    "decay_rate = 0.96                          # Decay Rate for Learning Rate\n",
    "gradient_clipping = 10                     # Size of Gradient Clipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT ##\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    # Length of the whole sequence\n",
    "    fp_length = tf.placeholder(tf.int32, shape=[batch_size], name=\"fp_ength\")\n",
    "    # Words and POS Tags in sequence\n",
    "    fp = tf.placeholder(tf.int32, [batch_size, 2, max_len_seq], name=\"full_path\")\n",
    "    # Length of both LCA Paths\n",
    "    sp_length = tf.placeholder(tf.int32, shape=[batch_size, 2], name=\"sp_length\")\n",
    "    # Dependency Types in LCA Paths\n",
    "    sp = tf.placeholder(tf.int32, [batch_size, 2, max_len_path], name=\"shortest_path\")\n",
    "    # Position of words in LCA Paths in whole sequence\n",
    "    sp_pos = tf.placeholder(tf.int32, [batch_size, 2, max_len_path], name=\"sp_pos\")\n",
    "    # Position in whole sequence of the children in Dependency Tree of words in LCA Paths \n",
    "    sp_childs = tf.placeholder(tf.int32, [batch_size, 2, max_len_path, max_num_child], name=\"sp_childs\")\n",
    "    # No. of children in Dependency Tree of words in LCA Paths\n",
    "    sp_num_childs = tf.placeholder(tf.int32, [batch_size, 2, max_len_path], name=\"sp_num_childs\")\n",
    "    # True Relation btw the entities\n",
    "    relation = tf.placeholder(tf.int32, [batch_size], name=\"relation\")\n",
    "    # Hot vector of true entities\n",
    "    y_entity = tf.placeholder(tf.int32, [batch_size, max_len_seq], name=\"y_enity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EMBEDDING LAYER ##\n",
    "\n",
    "# Embedding Layer of Words \n",
    "with tf.name_scope(\"word_embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[word_vocab_size, word_embd_dim]), name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32,[word_vocab_size, word_embd_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    embd_fp_word = tf.nn.embedding_lookup(W,fp[:,0])\n",
    "    word_embedding_saver = tf.train.Saver({\"word_embedding/W\": W})\n",
    "    \n",
    "# Embedding Layer of POS Tags \n",
    "with tf.name_scope(\"pos_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([pos_vocab_size, pos_embd_dim]), name=\"W\")\n",
    "    embd_fp_pos = tf.nn.embedding_lookup(W, fp[:,1])\n",
    "    pos_embedding_saver = tf.train.Saver({\"pos_embedding/W\": W})\n",
    "    \n",
    "# Embedding Layer of Dependency Types \n",
    "with tf.name_scope(\"dep_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([dep_vocab_size, dep_embd_dim]), name=\"W\")\n",
    "    embd_sp = tf.nn.embedding_lookup(W, sp)\n",
    "    dep_embedding_saver = tf.train.Saver({\"dep_embedding/W\": W})\n",
    "\n",
    "# Embedding layer as input for Forward sequential LSTM-RNNs\n",
    "embd_fp = tf.concat([embd_fp_word, embd_fp_pos], axis=2)\n",
    "\n",
    "# Embedding layer as input for Backward sequential LSTM-RNNs\n",
    "embd_fp_rev = tf.reverse(embd_fp, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditions for while loops\n",
    "def cond1(i, const, steps, *agrs):\n",
    "    return i< steps\n",
    "\n",
    "def cond2(i, steps, *agrs):\n",
    "    return i< steps\n",
    "\n",
    "# Initial Hidden and Cell States for Sequential LSTMs\n",
    "init_state_seq = tf.zeros([2, 1, state_size])\n",
    "\n",
    "x = tf.constant(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SEQUENCE LAYER ##\n",
    "\n",
    "# Function for initializing Sequential LSTM-RNN\n",
    "def lstm_seq_init(channel, embedding_dim, state_size):\n",
    "    init_const = tf.zeros([1, state_size])\n",
    "    with tf.variable_scope(channel):\n",
    "        \n",
    "        # Input Gate's weigths and bias\n",
    "        W_i = tf.get_variable(\"W_i\",shape=[embedding_dim, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_i = tf.get_variable(\"U_i\",shape=[state_size, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_i = tf.get_variable(\"b_i\", initializer=init_const)\n",
    "\n",
    "        # Forget Gate's weigths and bias\n",
    "        W_f = tf.get_variable(\"W_f\",shape=[embedding_dim, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_f = tf.get_variable(\"U_f\",shape=[state_size, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_f = tf.get_variable(\"b_f\", initializer=init_const)\n",
    "\n",
    "        # Output Gate's weigths and bias\n",
    "        W_o = tf.get_variable(\"W_o\",shape=[embedding_dim, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_o = tf.get_variable(\"U_o\",shape=[state_size, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_o = tf.get_variable(\"b_o\", initializer=init_const)\n",
    "\n",
    "        W_g = tf.get_variable(\"W_g\",shape=[embedding_dim, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_g = tf.get_variable(\"U_g\",shape=[state_size, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_g = tf.get_variable(\"b_g\", initializer=init_const)\n",
    "\n",
    "# Intialized Forward Sequential LSTM-RNN\n",
    "lstm_seq_init(\"lstm_fw\", word_embd_dim + pos_embd_dim, state_size)\n",
    "\n",
    "# Intialized Backward Sequential LSTM-RNN\n",
    "lstm_seq_init(\"lstm_bw\", word_embd_dim + pos_embd_dim, state_size)\n",
    "\n",
    "# Function for running Sequence LSTM \n",
    "def lstm_seq(input_embd, seq_len, scope):\n",
    "    \n",
    "    # While Loop body for running over the sequence\n",
    "    def body(j, const, steps, input_embd, states_seq, states_series):\n",
    "        inputs = tf.expand_dims(input_embd[j], [0])\n",
    "        \n",
    "        # Hidden State of LSTM-RNN\n",
    "        hs = states_seq[0]\n",
    "        \n",
    "        # Cell State of LSTM-RNN\n",
    "        cs = states_seq[1]\n",
    "        \n",
    "        # Hidden State Series\n",
    "        hs_ = states_series[0]\n",
    "        \n",
    "        # Cell State Series\n",
    "        cs_ = states_series[1]\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=True):\n",
    "            # Calling the Variables\n",
    "            W_i = tf.get_variable(\"W_i\")\n",
    "            U_i = tf.get_variable(\"U_i\")\n",
    "            b_i = tf.get_variable(\"b_i\")\n",
    "\n",
    "            W_f = tf.get_variable(\"W_f\")\n",
    "            U_f = tf.get_variable(\"U_f\")\n",
    "            b_f = tf.get_variable(\"b_f\")\n",
    "\n",
    "            W_o = tf.get_variable(\"W_o\")\n",
    "            U_o = tf.get_variable(\"U_o\")\n",
    "            b_o = tf.get_variable(\"b_o\")\n",
    "\n",
    "            W_g = tf.get_variable(\"W_g\")\n",
    "            U_g = tf.get_variable(\"U_g\")\n",
    "            b_g = tf.get_variable(\"b_g\")\n",
    "            \n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, W_i) + tf.matmul(hs, U_i) + b_i)\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, W_f) + tf.matmul(hs, U_f) + b_f)\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, W_o) + tf.matmul(hs, U_o) + b_o)\n",
    "            gt = tf.tanh(tf.matmul(inputs, W_g) + tf.matmul(hs, U_g) + b_g)\n",
    "            cs = input_gate * gt + forget_gate * cs\n",
    "            hs = output_gate * tf.tanh(cs)\n",
    "\n",
    "            # Concating Hidden State Series and Hidden State\n",
    "            hs_ = tf.cond(tf.equal(j, const), lambda: hs, lambda: tf.concat([hs_, hs], 0))\n",
    "\n",
    "            # Concating Cell State Series and Cell State\n",
    "            cs_ = tf.cond(tf.equal(j, const), lambda: cs, lambda: tf.concat([cs_, cs], 0))\n",
    "            \n",
    "            # Stacking Hidden and Cell State\n",
    "            states_seq = tf.stack([hs, cs], axis=0)\n",
    "            \n",
    "            # Stacking Hidden and Cell State Series\n",
    "            states_series = tf.stack([hs_, cs_], axis=0)\n",
    "\n",
    "            return j+1, const, steps, input_embd, states_seq, states_series\n",
    "    \n",
    "    # Running While Loop over the Sequence\n",
    "    _, _, _, _, _, state_series_seq = tf.while_loop(cond1, body, \n",
    "            [0, 0, seq_len, input_embd, init_state_seq, init_state_seq],\n",
    "            shape_invariants=[x.get_shape(), x.get_shape(), x.get_shape(), input_embd.get_shape(),\n",
    "            init_state_seq.get_shape(), \n",
    "            tf.TensorShape([2, None, state_size])])\n",
    "    \n",
    "    # Return State Series of Sequence LSTMs\n",
    "    return state_series_seq\n",
    "\n",
    "# Computing the Sequence Layer\n",
    "\n",
    "states_series_fw = []\n",
    "states_series_bw = []\n",
    "hidden_states_seq = []\n",
    "\n",
    "for b in range(batch_size):\n",
    "    seq_len = fp_length[b]\n",
    "    input_embd = embd_fp[b]  \n",
    "    # Running Forward Sequence LSTM\n",
    "    states_series_fw.append(lstm_seq(input_embd, seq_len, \"lstm_fw\"))\n",
    "    \n",
    "    input_embd = embd_fp_rev[b]  \n",
    "    # Running Backward Sequence LSTM\n",
    "    states_series_bw.append(tf.reverse(lstm_seq(input_embd, seq_len, \"lstm_bw\"), [1]))\n",
    "    \n",
    "    # Concating Hidden States of both Forward and Backward Seq LSTMs\n",
    "    hidden_states_seq.append(tf.concat([states_series_fw[b][0], states_series_bw[b][0]], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DEPENDENCY LAYER ##\n",
    "\n",
    "# Function for initializing Tree Structured LSTM-RNN\n",
    "def lstm_dep_init(channel, dep_input_size, state_size):\n",
    "    init_const = tf.zeros([1, state_size])\n",
    "    with tf.variable_scope(channel):\n",
    "        \n",
    "        # Input Gate's weigths and bias\n",
    "        W_i = tf.get_variable(\"W_i\", shape=[dep_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_i = tf.get_variable(\"U_i\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_i = tf.get_variable(\"b_i\", initializer=init_const)\n",
    "        \n",
    "        # Input Gate's weights for Children in Dependency Tree\n",
    "        U_it = tf.get_variable(\"U_it\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        \n",
    "        # Forget Gate's weigths and bias\n",
    "        W_f = tf.get_variable(\"W_f\", shape=[dep_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_f = tf.get_variable(\"U_f\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_f = tf.get_variable(\"b_f\", initializer=init_const)\n",
    "        \n",
    "        # Forget Gate for Children in LCA Path's weigths\n",
    "        U_fsp = tf.get_variable(\"U_fsp\", shape=[2, state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # Forget Gate for Children in Dependency Tree's weigths\n",
    "        U_ffp = tf.get_variable(\"U_ffp\", shape=[max_num_child, state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        \n",
    "        # Output Gate's weigths and bias\n",
    "        W_o = tf.get_variable(\"W_o\", shape=[dep_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_o = tf.get_variable(\"U_o\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_o = tf.get_variable(\"b_o\", initializer=init_const)\n",
    "        \n",
    "        # Output Gate's weights for Children in Dependency Tree\n",
    "        U_ot = tf.get_variable(\"U_ot\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        \n",
    "        W_u = tf.get_variable(\"W_u\", shape=[dep_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_u = tf.get_variable(\"U_u\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_u = tf.get_variable(\"b_u\", initializer=init_const)\n",
    "        U_ut = tf.get_variable(\"U_ut\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# Dimension of input in Treestructured LSTM-RNN\n",
    "dep_input_size = state_size * 2 + dep_embd_dim\n",
    "\n",
    "# Initializing  Bottom-Up Treestructured LSTM\n",
    "lstm_dep_init(\"lstm_btup\", dep_input_size, state_size)\n",
    "\n",
    "# Initializing Top-Down Treestructured LSTM\n",
    "lstm_dep_init(\"lstm_tpdn\", dep_input_size, state_size)\n",
    "\n",
    "# Initial Hidden and Cell State for Treestructured LSTM\n",
    "init_state = tf.zeros([2, 1, 1, state_size])\n",
    "\n",
    "# Function for running TreeStructured LSTM\n",
    "def lstm_dep(b, p, start, seq_len, input_embd, input_pos, input_childs, input_num_child, states_seq, init_state_dep, scope):  \n",
    "    \n",
    "    # While loop Body for running TreeStrucctured LSTM\n",
    "    def loop_over_seq(index, const, steps, input_pos, input_embd, input_childs, input_num_child, states_seq, states_dep, states_series):\n",
    "        \n",
    "        # Input for the lstm dep\n",
    "        inputs = tf.expand_dims(tf.concat([hidden_states_seq[b][input_pos[p][index]], input_embd[p][index]],0),0)\n",
    "       \n",
    "        # Children in Dependency Tree\n",
    "        childs = input_childs[p][index]\n",
    "        \n",
    "        # No. of Children in Dependency Tree\n",
    "        num_child = input_num_child[p][index]  \n",
    "        \n",
    "        # No. of Children in LCA Path\n",
    "        num_child_sp = tf.shape(states_dep[0])[0]\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=True):\n",
    "            # Calling the Variables\n",
    "            W_i = tf.get_variable(\"W_i\")\n",
    "            U_i = tf.get_variable(\"U_i\")\n",
    "            b_i = tf.get_variable(\"b_i\")\n",
    "            U_it = tf.get_variable(\"U_it\")\n",
    "\n",
    "            W_f = tf.get_variable(\"W_f\")\n",
    "            U_f = tf.get_variable(\"U_f\")\n",
    "            b_f = tf.get_variable(\"b_f\")\n",
    "            U_fsp = tf.get_variable(\"U_fsp\")\n",
    "            U_ffp = tf.get_variable(\"U_ffp\")\n",
    "            \n",
    "            W_o = tf.get_variable(\"W_o\")\n",
    "            U_o= tf.get_variable(\"U_o\")\n",
    "            b_o = tf.get_variable(\"b_o\")\n",
    "            U_ot = tf.get_variable(\"U_ot\")\n",
    "\n",
    "            W_u = tf.get_variable(\"W_u\")\n",
    "            U_u = tf.get_variable(\"U_u\")\n",
    "            b_u = tf.get_variable(\"b_u\")\n",
    "            U_ut = tf.get_variable(\"U_ut\")    \n",
    "\n",
    "            ## Computing Input, Forget, Output Gates \n",
    "            ## e.g. it = x*W + b + h*U\n",
    "            it = tf.matmul(inputs, W_i) + b_i + tf.matmul(states_dep[0][0], U_i)\n",
    "            ft = tf.matmul(inputs, W_f) + b_f + tf.matmul(states_dep[0][0], U_f)\n",
    "            ot = tf.matmul(inputs, W_o) + b_o + tf.matmul(states_dep[0][0], U_o)\n",
    "            ut = tf.matmul(inputs, W_u) + b_u + tf.matmul(states_dep[0][0], U_u)\n",
    "            \n",
    "            ## \n",
    "            def matmul(k, steps, it, ft, ot, ut):\n",
    "                it += tf.matmul(states_dep[0][k], U_i)\n",
    "                ft += tf.matmul(states_dep[0][k], U_f) \n",
    "                ot += tf.matmul(states_dep[0][k], U_o) \n",
    "                ut += tf.matmul(states_dep[0][k], U_u) \n",
    "                return k+1, steps, it, ft, ot, ut\n",
    "            \n",
    "            _, _, it, ft, ot, ut = tf.while_loop(cond2, matmul, [1, num_child_sp, it, ft, ot, ut])\n",
    "\n",
    "            ## Looping over the children in Dependency Tree \n",
    "            ## No. of loops is equal to the number of children in the dependency tree\n",
    "            def child_sum(k, steps, out, U): \n",
    "                ### Calculating out = out + h_child* U\n",
    "                ### Suming for all the children\n",
    "                out += tf.matmul(states_seq[0][childs[k]], U)\n",
    "                return k+1, steps, out, U\n",
    "\n",
    "            ## While loop with body as child_sum\n",
    "            ## Computing input gate, output gate by suming the h*U for all the children in the dependency tree\n",
    "            _, _, ht_i, _ = tf.while_loop(cond2, child_sum, [0, num_child, it, U_it])\n",
    "            _, _, ht_o, _ = tf.while_loop(cond2, child_sum, [0, num_child, ot, U_ot])\n",
    "            _, _, ht_u, _ = tf.while_loop(cond2, child_sum, [0, num_child, ut, U_ut])\n",
    "\n",
    "            ## Sigmoid over the gates\n",
    "            input_gate = tf.sigmoid(ht_i)\n",
    "            output_gate = tf.sigmoid(ht_o)\n",
    "            \n",
    "            u_input = tf.tanh(ht_u)\n",
    "\n",
    "            # Computing Cell State\n",
    "            cell_state = input_gate * u_input \n",
    "\n",
    "            # Computing Forget Gates for Children in LCA Path and Adding it to Compute Cell State\n",
    "            def cell_state_sp(k, steps, cell_state):\n",
    "                _, _, f_sp, _ = tf.while_loop(cond2, child_sum, [0, num_child, ft, U_fsp[k]])\n",
    "                cell_state += tf.sigmoid(f_sp) * states_dep[1][k]\n",
    "                return k+1, steps, cell_state\n",
    "            \n",
    "            _, _, cell_state = tf.while_loop(cond2, cell_state_sp, [0, num_child_sp, cell_state])\n",
    "\n",
    "            # Computing Forget Gates for Children in Dependency Tree and Adding it to Compute Cell State\n",
    "            def cell_states_fp(k, steps, ctl):\n",
    "                _, _, f_fp, _ = tf.while_loop(cond2, child_sum, [0, num_child, ft, U_ffp[k]])\n",
    "                ctl += tf.sigmoid(f_fp) * states_seq[1][childs[k]]\n",
    "                return k+1, steps, ctl\n",
    "\n",
    "            # Cell State \n",
    "            _, _, cds = tf.while_loop(cond2, cell_states_fp, [0, num_child, cell_state])\n",
    "            \n",
    "            # Hidden State\n",
    "            hds = tf.expand_dims(output_gate * tf.tanh(cds), 0)\n",
    "\n",
    "            # Expanding one dimension \n",
    "            cds = tf.expand_dims(cds, 0)\n",
    "            \n",
    "            # Stacking Hidden and Cell State\n",
    "            states_dep = tf.stack([hds, cds], axis=0)\n",
    "\n",
    "            # Concating Hidden State Series\n",
    "            hds_ = tf.cond(tf.equal(index, const), lambda: states_dep[0],\n",
    "                           lambda: tf.concat([states_series[0], states_dep[0]], 0))\n",
    "            # Concating Cell State Series\n",
    "            cds_ = tf.cond(tf.equal(index, const), lambda: states_dep[1],\n",
    "                           lambda: tf.concat([states_series[1], states_dep[1]], 0))\n",
    "\n",
    "            # Stacking Hidden and Cell State Series\n",
    "            states_series = tf.stack([hds_, cds_], axis=0)\n",
    "\n",
    "        return index+1, const, steps, input_pos, input_embd, input_childs, input_num_child, states_seq, states_dep, states_series\n",
    "    \n",
    "    # Running While Loop over sequence(LCA Path)\n",
    "    _, _, _, _, _, _, _, _, _, states_series_dep = tf.while_loop(cond1, loop_over_seq,[start, start, seq_len,\n",
    "                            input_pos, input_embd, input_childs, input_num_child, states_seq, init_state_dep,\n",
    "                            init_state], shape_invariants=[x.get_shape(), x.get_shape(), x.get_shape(),\n",
    "                            input_pos.get_shape(), input_embd.get_shape(), input_childs.get_shape(),\n",
    "                            input_num_child.get_shape(), states_seq.get_shape(),\n",
    "                            tf.TensorShape([2, None, 1, state_size]), tf.TensorShape([2, None, 1, state_size])])\n",
    "    \n",
    "    # Return Hidden and Cell State Series of TreeStructured LSTMs\n",
    "#     len(states_series_dep)\n",
    "    return states_series_dep\n",
    "\n",
    "lca_series_btup = []\n",
    "dp_series_tpdn = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    \n",
    "    # Position of words in the sentence \n",
    "    input_pos = sp_pos[i]\n",
    "    \n",
    "    # Dependency Embedding  \n",
    "    input_embd = embd_sp[i]\n",
    "    \n",
    "    # Children's position in the sentence\n",
    "    input_childs = sp_childs[i]\n",
    "    \n",
    "    # No. of children in Dependency tree\n",
    "    input_num_child = sp_num_childs[i]\n",
    "    \n",
    "    # Reverse Sequenece\n",
    "    input_pos_rev = tf.reverse(sp_pos[i], [1])\n",
    "    input_embd_rev = tf.reverse(embd_sp[i], [1])\n",
    "    input_childs_rev = tf.reverse(sp_childs[i], [1])\n",
    "    input_num_child_rev = tf.reverse(sp_num_childs[i], [1])\n",
    "    \n",
    "    # Expanding Dimension of States of Sequence LSTMs\n",
    "    states_seq_fw = tf.expand_dims(states_series_fw[i], 2)\n",
    "    states_seq_bw = tf.expand_dims(states_series_bw[i], 2)\n",
    "    \n",
    "    # Comuting States from 1st entity upto LCA (Bottom-Up)\n",
    "    s1 = lstm_dep(i, 0, 0, sp_length[i][0]-1, input_embd, input_pos, input_childs, input_num_child,\n",
    "                 states_seq_fw, init_state, \"lstm_btup\")\n",
    "    \n",
    "    # Last State in Bottom-Up which will serve as previous state for LCA\n",
    "    lca_btup = tf.cond(sp_length[i][0]>1, lambda: s1[:,sp_length[i][0]-2], lambda: init_state[:,0])\n",
    "\n",
    "    # Computing States from 2nd entity upto LCA (Bottom-Up)\n",
    "    s2 = lstm_dep(i, 1, 0, sp_length[i][1], input_embd_rev, input_pos_rev, input_childs_rev, input_num_child_rev,\n",
    "                 states_seq_bw, init_state, \"lstm_btup\")\n",
    "    \n",
    "    # Stacking Last State from both Bottom-Up Trees which will serve as previous state for LCA\n",
    "    lca_btup = tf.cond(sp_length[i][1]>0, lambda: tf.stack([lca_btup, s2[:,sp_length[i][1]-1]],axis=1), lambda: tf.expand_dims(lca_btup, 1))\n",
    "\n",
    "    # Computing State for LCA (Bottom Up)\n",
    "    lca_series_btup.append(lstm_dep(i, 0, sp_length[i][0]-1, sp_length[i][0], input_embd, input_pos, input_childs, input_num_child, states_seq_fw, lca_btup, \"lstm_btup\")[0,0])\n",
    "    \n",
    "    # Computing State for LCA (Top Down)\n",
    "    lca_tpdn = lstm_dep(i, 0, 0, 1, input_embd_rev, input_pos_rev, input_childs_rev, input_num_child_rev, \n",
    "                        states_seq_bw, init_state, \"lstm_tpdn\")\n",
    "\n",
    "    # Computing States from LCA to 1st entity (Top-Down)\n",
    "    dp1 = lstm_dep(i, 0, 1, sp_length[i][0], input_embd_rev, input_pos_rev, input_childs_rev, input_num_child_rev,\n",
    "                   states_seq_bw, lca_tpdn, \"lstm_tpdn\")[0,-1]\n",
    "    \n",
    "    # dp1 has State of 1st entity (Top-Down)\n",
    "    dp1 = tf.cond(sp_length[i][0]>1, lambda: dp1, lambda: lca_tpdn[0][0])\n",
    "\n",
    "    # Computing States from LCA to 2nd entity (Top-Down)\n",
    "    dp2 = lstm_dep(i, 1, 0, sp_length[i][1], input_embd, input_pos, input_childs, input_num_child,\n",
    "                   states_seq_fw, lca_tpdn, \"lstm_tpdn\")[0,-1]\n",
    "    \n",
    "    # dp2 has State of 2nd entity (Top-Down)\n",
    "    dp2 = tf.cond(sp_length[i][1]>0, lambda: dp2, lambda: lca_tpdn[0][0])\n",
    "\n",
    "    # Concating the States of 1st and 2nd Entities (Top-Down)\n",
    "    dp_series_tpdn.append(tf.concat([dp1, dp2], 1))\n",
    "        \n",
    "# Concating the LCA (Bottom-Up) State and Entities (Top-Down) States\n",
    "for i in range(batch_size):\n",
    "    \n",
    "    temp = tf.concat([lca_series_btup[i], dp_series_tpdn[i]], 1)\n",
    "    if(i==0):\n",
    "        dp_series = temp\n",
    "    else:\n",
    "        dp_series = tf.concat([dp_series,temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-e676012949ab>:61: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "Tensor(\"loss_seq/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## ENTITY DETECTION ## \n",
    "\n",
    "# Hidden Layer After Sequence LSTM\n",
    "with tf.name_scope(\"hidden_layer_seq\"):\n",
    "    W = tf.Variable(tf.truncated_normal([200, 100], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "    \n",
    "    y_hidden_layer = []\n",
    "    y_hl = tf.zeros([1, 100])\n",
    "\n",
    "    for batch in range(batch_size):\n",
    "        s_seq = tf.expand_dims(hidden_states_seq[batch], 1)\n",
    "\n",
    "        # Looping over the equence for computing Hidden Layer\n",
    "        def matmul_hl(j, const, steps, input_seq, out_seq):\n",
    "            temp = tf.tanh(tf.matmul(input_seq[j], W) + b)\n",
    "            out_seq = tf.cond(tf.equal(j, const), lambda: temp, lambda: tf.concat([out_seq, temp], 0))\n",
    "            return j+1, const, steps, input_seq, out_seq\n",
    "        \n",
    "        _, _, _, _, output_seq = tf.while_loop(cond1, matmul_hl, \n",
    "                                [0, 0, fp_length[batch], s_seq, y_hl],\n",
    "                                shape_invariants=[x.get_shape(), x.get_shape(), x.get_shape(), \n",
    "                                s_seq.get_shape(), tf.TensorShape([None, 100])])\n",
    "        \n",
    "        y_hidden_layer.append(output_seq)\n",
    "\n",
    "# Dropout For Hidden Layer Sequence\n",
    "with tf.name_scope(\"dropout_hidden_seq\"):\n",
    "    y_hidden_layer_drop = []\n",
    "    for i in range(batch_size):\n",
    "        y_drop = tf.nn.dropout(y_hidden_layer[i], 0.3)\n",
    "        y_hidden_layer_drop.append(y_drop)\n",
    "        \n",
    "# SoftMax Layer for Sequence \n",
    "with tf.name_scope(\"softmax_layer_seq\"):\n",
    "    W = tf.Variable(tf.truncated_normal([100, 2], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([2]), name=\"b\")\n",
    "    \n",
    "    logits_entity = []\n",
    "    predictions_entity = []\n",
    "    lg = tf.zeros([1, 2])\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        \n",
    "        # Looping over the sequence for computing Logits\n",
    "        def matmul_softmax(j, const, steps, y_hl, lg):\n",
    "            temp = tf.matmul(y_hl[j], W) + b\n",
    "            lg = tf.cond(tf.equal(j, const), lambda: temp, lambda: tf.concat([lg, temp], 0))\n",
    "            return j+1, const, steps, y_hl, lg\n",
    "\n",
    "        y_hl = tf.expand_dims(y_hidden_layer_drop[batch], 1)\n",
    "        \n",
    "        _, _, _, _, logit = tf.while_loop(cond1, matmul_softmax, \n",
    "                                [0, 0, fp_length[batch], y_hl,lg],\n",
    "                                shape_invariants=[x.get_shape(), x.get_shape(), x.get_shape(), \n",
    "                                y_hl.get_shape(), tf.TensorShape([None, 2])])\n",
    "        \n",
    "        logits_entity.append(logit)\n",
    "        \n",
    "        # Predictions for Entities\n",
    "        predictions_entity.append(tf.arg_max(logit, 1))\n",
    "        \n",
    "# One Hot Vector of True Entities in the sequence of its length\n",
    "Y_en = [y_entity[i][:fp_length[i]] for i in range(batch_size)]\n",
    "\n",
    "# One Hot Vector of Logits in the seqeuence of sentence's length\n",
    "Y_softmax = [tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_entity[i], labels=Y_en[i])) for i in range(batch_size)]\n",
    "\n",
    "# Loss for Sequence \n",
    "with tf.name_scope(\"loss_seq\"):\n",
    "    loss_seq = tf.reduce_mean(Y_softmax)\n",
    "    print(loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/hist is illegal; using word_embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name word_embedding/W:0/grad/sparsity is illegal; using word_embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name pos_embedding/W:0/grad/hist is illegal; using pos_embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name pos_embedding/W:0/grad/sparsity is illegal; using pos_embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name dep_embedding/W:0/grad/hist is illegal; using dep_embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name dep_embedding/W:0/grad/sparsity is illegal; using dep_embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/W_i:0/grad/hist is illegal; using lstm_fw/W_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/W_i:0/grad/sparsity is illegal; using lstm_fw/W_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/U_i:0/grad/hist is illegal; using lstm_fw/U_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/U_i:0/grad/sparsity is illegal; using lstm_fw/U_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/b_i:0/grad/hist is illegal; using lstm_fw/b_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/b_i:0/grad/sparsity is illegal; using lstm_fw/b_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/W_f:0/grad/hist is illegal; using lstm_fw/W_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/W_f:0/grad/sparsity is illegal; using lstm_fw/W_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/U_f:0/grad/hist is illegal; using lstm_fw/U_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/U_f:0/grad/sparsity is illegal; using lstm_fw/U_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/b_f:0/grad/hist is illegal; using lstm_fw/b_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/b_f:0/grad/sparsity is illegal; using lstm_fw/b_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/W_o:0/grad/hist is illegal; using lstm_fw/W_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/W_o:0/grad/sparsity is illegal; using lstm_fw/W_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/U_o:0/grad/hist is illegal; using lstm_fw/U_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/U_o:0/grad/sparsity is illegal; using lstm_fw/U_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/b_o:0/grad/hist is illegal; using lstm_fw/b_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/b_o:0/grad/sparsity is illegal; using lstm_fw/b_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/W_g:0/grad/hist is illegal; using lstm_fw/W_g_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/W_g:0/grad/sparsity is illegal; using lstm_fw/W_g_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/U_g:0/grad/hist is illegal; using lstm_fw/U_g_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/U_g:0/grad/sparsity is illegal; using lstm_fw/U_g_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/b_g:0/grad/hist is illegal; using lstm_fw/b_g_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_fw/b_g:0/grad/sparsity is illegal; using lstm_fw/b_g_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/W_i:0/grad/hist is illegal; using lstm_bw/W_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/W_i:0/grad/sparsity is illegal; using lstm_bw/W_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/U_i:0/grad/hist is illegal; using lstm_bw/U_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/U_i:0/grad/sparsity is illegal; using lstm_bw/U_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/b_i:0/grad/hist is illegal; using lstm_bw/b_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/b_i:0/grad/sparsity is illegal; using lstm_bw/b_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/W_f:0/grad/hist is illegal; using lstm_bw/W_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/W_f:0/grad/sparsity is illegal; using lstm_bw/W_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/U_f:0/grad/hist is illegal; using lstm_bw/U_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/U_f:0/grad/sparsity is illegal; using lstm_bw/U_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/b_f:0/grad/hist is illegal; using lstm_bw/b_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/b_f:0/grad/sparsity is illegal; using lstm_bw/b_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/W_o:0/grad/hist is illegal; using lstm_bw/W_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/W_o:0/grad/sparsity is illegal; using lstm_bw/W_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/U_o:0/grad/hist is illegal; using lstm_bw/U_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/U_o:0/grad/sparsity is illegal; using lstm_bw/U_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/b_o:0/grad/hist is illegal; using lstm_bw/b_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/b_o:0/grad/sparsity is illegal; using lstm_bw/b_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/W_g:0/grad/hist is illegal; using lstm_bw/W_g_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/W_g:0/grad/sparsity is illegal; using lstm_bw/W_g_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/U_g:0/grad/hist is illegal; using lstm_bw/U_g_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/U_g:0/grad/sparsity is illegal; using lstm_bw/U_g_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/b_g:0/grad/hist is illegal; using lstm_bw/b_g_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_bw/b_g:0/grad/sparsity is illegal; using lstm_bw/b_g_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/W_i:0/grad/hist is illegal; using lstm_btup/W_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/W_i:0/grad/sparsity is illegal; using lstm_btup/W_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_i:0/grad/hist is illegal; using lstm_btup/U_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_i:0/grad/sparsity is illegal; using lstm_btup/U_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/b_i:0/grad/hist is illegal; using lstm_btup/b_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/b_i:0/grad/sparsity is illegal; using lstm_btup/b_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_it:0/grad/hist is illegal; using lstm_btup/U_it_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_it:0/grad/sparsity is illegal; using lstm_btup/U_it_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/W_f:0/grad/hist is illegal; using lstm_btup/W_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/W_f:0/grad/sparsity is illegal; using lstm_btup/W_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_f:0/grad/hist is illegal; using lstm_btup/U_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_f:0/grad/sparsity is illegal; using lstm_btup/U_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/b_f:0/grad/hist is illegal; using lstm_btup/b_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/b_f:0/grad/sparsity is illegal; using lstm_btup/b_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_fsp:0/grad/hist is illegal; using lstm_btup/U_fsp_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_fsp:0/grad/sparsity is illegal; using lstm_btup/U_fsp_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_ffp:0/grad/hist is illegal; using lstm_btup/U_ffp_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_ffp:0/grad/sparsity is illegal; using lstm_btup/U_ffp_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/W_o:0/grad/hist is illegal; using lstm_btup/W_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/W_o:0/grad/sparsity is illegal; using lstm_btup/W_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_o:0/grad/hist is illegal; using lstm_btup/U_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_o:0/grad/sparsity is illegal; using lstm_btup/U_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/b_o:0/grad/hist is illegal; using lstm_btup/b_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/b_o:0/grad/sparsity is illegal; using lstm_btup/b_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_ot:0/grad/hist is illegal; using lstm_btup/U_ot_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_ot:0/grad/sparsity is illegal; using lstm_btup/U_ot_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/W_u:0/grad/hist is illegal; using lstm_btup/W_u_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/W_u:0/grad/sparsity is illegal; using lstm_btup/W_u_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_u:0/grad/hist is illegal; using lstm_btup/U_u_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_u:0/grad/sparsity is illegal; using lstm_btup/U_u_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/b_u:0/grad/hist is illegal; using lstm_btup/b_u_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/b_u:0/grad/sparsity is illegal; using lstm_btup/b_u_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_ut:0/grad/hist is illegal; using lstm_btup/U_ut_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_btup/U_ut:0/grad/sparsity is illegal; using lstm_btup/U_ut_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/W_i:0/grad/hist is illegal; using lstm_tpdn/W_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/W_i:0/grad/sparsity is illegal; using lstm_tpdn/W_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_i:0/grad/hist is illegal; using lstm_tpdn/U_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_i:0/grad/sparsity is illegal; using lstm_tpdn/U_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/b_i:0/grad/hist is illegal; using lstm_tpdn/b_i_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/b_i:0/grad/sparsity is illegal; using lstm_tpdn/b_i_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_it:0/grad/hist is illegal; using lstm_tpdn/U_it_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_it:0/grad/sparsity is illegal; using lstm_tpdn/U_it_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/W_f:0/grad/hist is illegal; using lstm_tpdn/W_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/W_f:0/grad/sparsity is illegal; using lstm_tpdn/W_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_f:0/grad/hist is illegal; using lstm_tpdn/U_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_f:0/grad/sparsity is illegal; using lstm_tpdn/U_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/b_f:0/grad/hist is illegal; using lstm_tpdn/b_f_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/b_f:0/grad/sparsity is illegal; using lstm_tpdn/b_f_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_fsp:0/grad/hist is illegal; using lstm_tpdn/U_fsp_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_fsp:0/grad/sparsity is illegal; using lstm_tpdn/U_fsp_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_ffp:0/grad/hist is illegal; using lstm_tpdn/U_ffp_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_ffp:0/grad/sparsity is illegal; using lstm_tpdn/U_ffp_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/W_o:0/grad/hist is illegal; using lstm_tpdn/W_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/W_o:0/grad/sparsity is illegal; using lstm_tpdn/W_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_o:0/grad/hist is illegal; using lstm_tpdn/U_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_o:0/grad/sparsity is illegal; using lstm_tpdn/U_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/b_o:0/grad/hist is illegal; using lstm_tpdn/b_o_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/b_o:0/grad/sparsity is illegal; using lstm_tpdn/b_o_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_ot:0/grad/hist is illegal; using lstm_tpdn/U_ot_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_ot:0/grad/sparsity is illegal; using lstm_tpdn/U_ot_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/W_u:0/grad/hist is illegal; using lstm_tpdn/W_u_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/W_u:0/grad/sparsity is illegal; using lstm_tpdn/W_u_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_u:0/grad/hist is illegal; using lstm_tpdn/U_u_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_u:0/grad/sparsity is illegal; using lstm_tpdn/U_u_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/b_u:0/grad/hist is illegal; using lstm_tpdn/b_u_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/b_u:0/grad/sparsity is illegal; using lstm_tpdn/b_u_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_ut:0/grad/hist is illegal; using lstm_tpdn/U_ut_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name lstm_tpdn/U_ut:0/grad/sparsity is illegal; using lstm_tpdn/U_ut_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden_layer_seq/W:0/grad/hist is illegal; using hidden_layer_seq/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden_layer_seq/W:0/grad/sparsity is illegal; using hidden_layer_seq/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden_layer_seq/b:0/grad/hist is illegal; using hidden_layer_seq/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden_layer_seq/b:0/grad/sparsity is illegal; using hidden_layer_seq/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name softmax_layer_seq/W:0/grad/hist is illegal; using softmax_layer_seq/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name softmax_layer_seq/W:0/grad/sparsity is illegal; using softmax_layer_seq/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name softmax_layer_seq/b:0/grad/hist is illegal; using softmax_layer_seq/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name softmax_layer_seq/b:0/grad/sparsity is illegal; using softmax_layer_seq/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden_layer_dep/W:0/grad/hist is illegal; using hidden_layer_dep/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden_layer_dep/W:0/grad/sparsity is illegal; using hidden_layer_dep/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name hidden_layer_dep/b:0/grad/hist is illegal; using hidden_layer_dep/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name hidden_layer_dep/b:0/grad/sparsity is illegal; using hidden_layer_dep/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name softmax_layer_dep/W:0/grad/hist is illegal; using softmax_layer_dep/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name softmax_layer_dep/W:0/grad/sparsity is illegal; using softmax_layer_dep/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name softmax_layer_dep/b:0/grad/hist is illegal; using softmax_layer_dep/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name softmax_layer_dep/b:0/grad/sparsity is illegal; using softmax_layer_dep/b_0/grad/sparsity instead.\n"
     ]
    }
   ],
   "source": [
    "## RELATION CLASSIFICATION ##\n",
    "\n",
    "# Hidden Layer in Dependency Layer (after Tree Structured LSTMs)\n",
    "with tf.name_scope(\"hidden_layer_dep\"):\n",
    "    W = tf.Variable(tf.truncated_normal([300, 100], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "    y_p = tf.tanh(tf.matmul(dp_series, W) + b)\n",
    "\n",
    "# Dropout for Hidden Layer in Dependency Layer \n",
    "with tf.name_scope(\"dropout_hidden_dep\"):\n",
    "        y_p_drop = tf.nn.dropout(y_p, 0.3)\n",
    "    \n",
    "# SoftMax Layer in Dependency Layer\n",
    "with tf.name_scope(\"softmax_layer_dep\"):\n",
    "    W = tf.Variable(tf.truncated_normal([100, relation_classes], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([relation_classes]), name=\"b\")\n",
    "    logits = tf.matmul(y_p_drop, W) + b    \n",
    "    predictions_dep = tf.argmax(logits, 1)\n",
    "\n",
    "# Loss for Dependency Layer\n",
    "with tf.name_scope(\"loss_dep\"):\n",
    "    loss_dep = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=relation))\n",
    "\n",
    "# All Trainable Variables\n",
    "tv_all = tf.trainable_variables()\n",
    "\n",
    "tv_regu = []\n",
    "\n",
    "# Variables that are not regularised\n",
    "non_reg = [\"word_embedding/W:0\",\"pos_embedding/W:0\",'dep_embedding/W:0']\n",
    "\n",
    "for t in tv_all:\n",
    "    if t.name not in non_reg:\n",
    "        if t.name.find('b_')==-1:\n",
    "            if t.name.find('b:')==-1:\n",
    "                tv_regu.append(t)\n",
    "\n",
    "# Total Loss\n",
    "with tf.name_scope(\"total_loss\"):\n",
    "    l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "    total_loss = l2_loss + loss_seq + loss_dep\n",
    "\n",
    "# Global Steps for Entity Training and Relation Classification\n",
    "global_step_seq = tf.Variable(0, trainable=False, name=\"global_step_seq\")\n",
    "global_step_dep = tf.Variable(0, trainable=False, name=\"global_step_dep\")\n",
    "\n",
    "# Learning Rates for Entity Training and Relation Classification\n",
    "learning_rate_seq = tf.train.exponential_decay(init_learning_rate, global_step_seq, decay_steps, decay_rate, staircase=True)\n",
    "learning_rate_dep = tf.train.exponential_decay(init_learning_rate, global_step_dep, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "# Optimzier for Loss of Sequence Layer\n",
    "optimizer_seq = tf.train.AdamOptimizer(learning_rate_seq).minimize(loss_seq, global_step=global_step_seq)\n",
    "\n",
    "# Optimizer for Total loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate_dep)\n",
    "\n",
    "# Gradients and Variables for Total Loss\n",
    "grads_vars = optimizer.compute_gradients(total_loss)\n",
    "\n",
    "for g, v in grads_vars:\n",
    "    if(g==None):\n",
    "        print(g, v)\n",
    "\n",
    "        \n",
    "# Clipping of Gradients\n",
    "clipped_grads = [(tf.clip_by_norm(grad, gradient_clipping), var) for grad, var in grads_vars]\n",
    "# Training Optimizer for Total Loss\n",
    "train_op = optimizer.apply_gradients(clipped_grads, global_step=global_step_dep)\n",
    "\n",
    "# Summary \n",
    "grad_summaries = []\n",
    "for g, v in grads_vars:\n",
    "    if g is not None:\n",
    "        grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "        sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        grad_summaries.append(grad_hist_summary)\n",
    "        grad_summaries.append(sparsity_summary)\n",
    "grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "loss_seq_summary = tf.summary.scalar(\"loss_seq\", loss_seq)\n",
    "loss_dep_summary = tf.summary.scalar(\"loss_dep\", loss_dep)\n",
    "total_loss_summary = tf.summary.scalar(\"total_loss\", total_loss)\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VOCAB Preprocessing Functions ##\n",
    "\n",
    "# Vocab for Words (Glove)\n",
    "f = open(data_dir + '/vocab_glove', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Word to Vector \n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "# Vocab for POS Tags\n",
    "pos_tags_vocab = []\n",
    "for line in open(data_dir + '/pos_tags.txt'):\n",
    "    pos_tags_vocab.append(line.strip())\n",
    "\n",
    "# Vocab for Dependency Types\n",
    "dep_vocab = []\n",
    "for line in open(data_dir + '/dependency_types.txt'):\n",
    "    dep_vocab.append(line.strip())\n",
    "\n",
    "# Vocab for Relation Classes\n",
    "relation_vocab = []\n",
    "for line in open(data_dir + '/relation_typesv3.txt'):\n",
    "    relation_vocab.append(line.strip())\n",
    "    \n",
    "# Relation to Vector\n",
    "rel2id = dict((w, i) for i,w in enumerate(relation_vocab))\n",
    "id2rel = dict((i, w) for i,w in enumerate(relation_vocab))\n",
    "\n",
    "# POS Tags to Vector\n",
    "pos_tag2id = dict((w, i) for i,w in enumerate(pos_tags_vocab))\n",
    "id2pos_tag = dict((i, w) for i,w in enumerate(pos_tags_vocab))\n",
    "\n",
    "# Dependency Types to Vector\n",
    "dep2id = dict((w, i) for i,w in enumerate(dep_vocab))\n",
    "id2dep = dict((i, w) for i,w in enumerate(dep_vocab))\n",
    "\n",
    "pos_tag2id['OTH'] = 43\n",
    "id2pos_tag[43] = 'OTH'\n",
    "\n",
    "dep2id['OTH'] = 44\n",
    "id2dep[44] = 'OTH'\n",
    "\n",
    "# Grouping POS Tags\n",
    "JJ_pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "NN_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "RB_pos_tags = ['RB', 'RBR', 'RBS']\n",
    "PRP_pos_tags = ['PRP', 'PRP$']\n",
    "VB_pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "_pos_tags = ['CC', 'CD', 'DT', 'IN', '.', 'POS', '$', ',', 'EX', 'WRB', \"''\", 'MD', 'RP', 'WP', ':', 'TO', '``', 'WP$', 'WDT', 'PDT', 'SYM', 'LS', 'UH', 'FW', '#']\n",
    "\n",
    "def pos_tag(x):\n",
    "    if x in JJ_pos_tags:\n",
    "        return pos_tag2id['JJ']\n",
    "    if x in NN_pos_tags:\n",
    "        return pos_tag2id['NN']\n",
    "    if x in RB_pos_tags:\n",
    "        return pos_tag2id['RB']\n",
    "    if x in PRP_pos_tags:\n",
    "        return pos_tag2id['PRP']\n",
    "    if x in VB_pos_tags:\n",
    "        return pos_tag2id['VB']\n",
    "    if x in _pos_tags:\n",
    "        return pos_tag2id[x]\n",
    "    else:\n",
    "        return 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA PREPROCESSING ##\n",
    "\n",
    "# Function for preparing input for training or testing\n",
    "def prepare_input(words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1,pos_path2, childs_path1, childs_path2, relations):\n",
    "\n",
    "    # Size of the dataset\n",
    "    length = len(words_seq)\n",
    "    print(length)\n",
    "    # Position of words of LCA Paths in whole sentence \n",
    "    pos_path1 = [[i-1 for i in w] for w in pos_path1]\n",
    "    pos_path2 = [[i-1 for i in w] for w in pos_path2]\n",
    "\n",
    "    # Removing None\n",
    "    for i in range(length):\n",
    "            words_seq[i] = [w for w in words_seq[i] if w !=None ]\n",
    "            deps_seq[i] = [w for w in deps_seq[i] if w !=None ]\n",
    "            pos_tags_seq[i] = [w for w in pos_tags_seq[i] if w !=None ]\n",
    "\n",
    "    # Hot Vector of Entities\n",
    "    entity = np.zeros([length, max_len_seq])\n",
    "    \n",
    "    for i in range(length):\n",
    "        entity[i][pos_path1[i][0]] = 1\n",
    "        if(pos_path2[i]==[]):\n",
    "            entity[i][pos_path1[i][-1]] = 1\n",
    "        else:\n",
    "            entity[i][pos_path2[i][0]] = 1\n",
    "\n",
    "    len_path1 = []\n",
    "    len_path2 = []\n",
    "\n",
    "    num_child_path1 = np.ones([length, max_len_path],dtype=int)\n",
    "    num_child_path2 = np.ones([length, max_len_path],dtype=int)\n",
    "\n",
    "    # Length of LCA paths\n",
    "    for w in word_path1:\n",
    "        len_path1.append(len(w))\n",
    "\n",
    "    for w in word_path2:\n",
    "        len_path2.append(len(w))\n",
    "\n",
    "    # No. of Children of words of LCA paths in sentence\n",
    "    for i, w in enumerate(childs_path1):\n",
    "        if(w!=[]):\n",
    "            for j,c in enumerate(w):\n",
    "                num_child_path1[i][j] = len(c)\n",
    "        else:\n",
    "            num_child_path1[i][0] = 0\n",
    "\n",
    "    for i, w in enumerate(childs_path2):\n",
    "        if(w!=[]):\n",
    "            for j,c in enumerate(w):\n",
    "                num_child_path2[i][j] = len(c)\n",
    "        else:\n",
    "            num_child_path2[i][0] = 0\n",
    "\n",
    "    # Position of Children in sentence \n",
    "    for i in range(length):\n",
    "        if(childs_path2[i]!=[]):\n",
    "            for j, c in enumerate(childs_path2[i]):\n",
    "                if(c == []):\n",
    "                    childs_path2[i][j]  = [-1]\n",
    "        else:\n",
    "            childs_path2[i] = [[-1]]\n",
    "            \n",
    "        if(childs_path1[i]!=[]):\n",
    "            for j, c in enumerate(childs_path1[i]):\n",
    "                if(c == []):\n",
    "                    childs_path1[i][j]  = [-1]\n",
    "        else:\n",
    "            childs_path1[i] = [[-1]]\n",
    "\n",
    "    # Replacing words not in vcab with unknown_token\n",
    "    for i in range(length):\n",
    "        if(word_path2[i]==[]):\n",
    "            word_path2[i].append(unknown_token)\n",
    "        if(dep_path2[i]==[]):\n",
    "            dep_path2[i].append('OTH')\n",
    "        if(pos_tags_path2==[]):\n",
    "            pos_tags_path2[i].append('OTH')\n",
    "\n",
    "    # Converting Words, POS Tags, Dependency Types, Relation Classes to Vectors\n",
    "    for i in range(length):\n",
    "        for j, word in enumerate(words_seq[i]):\n",
    "            word = word.lower()\n",
    "            words_seq[i][j] = word if word in word2id else unknown_token \n",
    "\n",
    "        for l, d in enumerate(deps_seq[i]):\n",
    "            deps_seq[i][l] = d if d in dep2id else 'OTH'\n",
    "\n",
    "        for j, word in enumerate(word_path1[i]):\n",
    "            word = word.lower()\n",
    "            word_path1[i][j] = word if word in word2id else unknown_token \n",
    "\n",
    "        for l, d in enumerate(dep_path1[i]):\n",
    "            dep_path1[i][l] = d if d in dep2id else 'OTH'\n",
    "\n",
    "        for j, word in enumerate(word_path2[i]):\n",
    "            word = word.lower()\n",
    "            word_path2[i][j] = word if word in word2id else unknown_token \n",
    "\n",
    "        for l, d in enumerate(dep_path2[i]):\n",
    "            dep_path2[i][l] = d if d in dep2id else 'OTH'\n",
    "    \n",
    "    words_seq_id = np.ones([length, max_len_seq],dtype=int)\n",
    "    deps_seq_id = np.ones([length, max_len_seq],dtype=int)\n",
    "    pos_tags_seq_id = np.ones([length, max_len_seq],dtype=int)\n",
    "    \n",
    "    word_path1_id = np.ones([length, max_len_path],dtype=int)\n",
    "    word_path2_id = np.ones([length, max_len_path],dtype=int)\n",
    "    \n",
    "    dep_path1_id = np.ones([length, max_len_path],dtype=int)\n",
    "    dep_path2_id = np.ones([length, max_len_path],dtype=int)\n",
    "    \n",
    "    pos_tags_path1_id = np.ones([length, max_len_path],dtype=int)\n",
    "    pos_tags_path2_id = np.ones([length, max_len_path],dtype=int)\n",
    "    \n",
    "    pos_path1_ = np.ones([length, max_len_path],dtype=int)\n",
    "    pos_path2_ = np.ones([length, max_len_path],dtype=int)\n",
    "    \n",
    "    childs_path1_ = np.ones([length, max_len_path, max_num_child],dtype=int)\n",
    "    childs_path2_ = np.ones([length, max_len_path, max_num_child],dtype=int)\n",
    "    \n",
    "    seq_len = []\n",
    "\n",
    "    for i in range(length):\n",
    "        \n",
    "        temp = []\n",
    "        seq_len.append(len(words_seq[i]))\n",
    "\n",
    "        for j, w in enumerate(pos_path1[i]):\n",
    "            pos_path1_[i][j] = w\n",
    "        \n",
    "        for j, w in enumerate(pos_path2[i]):\n",
    "            pos_path2_[i][j] = w\n",
    "        \n",
    "        for j,child in enumerate(childs_path1[i]):\n",
    "            for k,c in enumerate(child):\n",
    "                childs_path1_[i][j][k] = c -1\n",
    "                \n",
    "        for j,child in enumerate(childs_path2[i]):\n",
    "            for k,c in enumerate(child):\n",
    "                childs_path2_[i][j][k] = c -1   \n",
    "    \n",
    "        for j, w in enumerate(words_seq[i]):\n",
    "            words_seq_id[i][j] = word2id[w]\n",
    "\n",
    "        \n",
    "        for j, w in enumerate(pos_tags_seq[i]):\n",
    "            pos_tags_seq_id[i][j] = pos_tag(w)\n",
    "        \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(deps_seq[i]):\n",
    "            deps_seq_id[i][j] = dep2id[w]\n",
    "         \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(word_path1[i]):\n",
    "            word_path1_id[i][j]   = word2id[w]\n",
    "         \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(pos_tags_path1[i]):\n",
    "            pos_tags_path1_id[i][j] = pos_tag(w)\n",
    "        \n",
    "        \n",
    "        for j, w in enumerate(dep_path1[i]):\n",
    "            dep_path1_id[i][j] = dep2id[w]\n",
    "         \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(word_path2[i]):\n",
    "            word_path2_id[i][j] = word2id[w]\n",
    "       \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(pos_tags_path2[i]):\n",
    "            pos_tags_path2_id[i][j] = pos_tag(w)\n",
    "         \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(dep_path2[i]):\n",
    "            dep_path2_id[i][j]  = dep2id[w]\n",
    "         \n",
    "\n",
    "    rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "    \n",
    "    return seq_len, words_seq_id, pos_tags_seq_id, deps_seq_id, len_path1, len_path2, pos_path1_, pos_path2_, dep_path1_id, dep_path2_id, childs_path1_, childs_path2_, num_child_path1, num_child_path2, rel_ids, entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRAPH ##\n",
    "\n",
    "sess = tf.Session()\n",
    "# For initializing all the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# For Saving the model\n",
    "saver = tf.train.Saver()\n",
    "# For writing Summaries\n",
    "summary_writer = tf.summary.FileWriter(summary_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initializing the embedding layer with Pretrained Glove Embedding Layer\n",
    "# f = open('data/word_embedding', 'rb')\n",
    "# word_embedding = pickle.load(f)\n",
    "# f.close()\n",
    "# sess.run(embedding_init, feed_dict={embedding_placeholder:word_embedding})\n",
    "# word_embedding_saver.save(sess, word_embd_dir + '/word_embd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restoring the model from latest checkpoint\n",
    "# model = tf.train.latest_checkpoint(model_dir)\n",
    "# saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Restoring the word embedding layer\n",
    "# latest_embd = tf.train.latest_checkpoint(word_embd_dir)\n",
    "# word_embedding_saver.restore(sess, latest_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18930\n"
     ]
    }
   ],
   "source": [
    "## TRAIN DATA PREPARATION ##\n",
    "\n",
    "f = open(data_dir + '/train_pathsv3', 'rb')\n",
    "words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1, pos_path2, childs_path1, childs_path2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/train_relationsv3.txt'):\n",
    "    relations.append(line.strip().split()[1])\n",
    "\n",
    "length = len(words_seq)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "seq_len, words_seq_id, pos_tags_seq_id, deps_seq_id, len_path1, len_path2, pos_path1, pos_path2, dep_path1_id, dep_path2_id, childs_path1, childs_path2, num_child_path1, num_child_path2, rel_ids, entity = prepare_input(words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1,pos_path2, childs_path1, childs_path2, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1893 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1893 [03:03<96:12:04, 183.05s/it]\u001b[A\n",
      "  0%|          | 2/1893 [03:06<67:50:54, 129.17s/it]\u001b[A\n",
      "  0%|          | 3/1893 [03:09<47:52:05, 91.18s/it] \u001b[A\n",
      "  0%|          | 4/1893 [03:11<33:56:48, 64.69s/it]\u001b[A\n",
      "  0%|          | 5/1893 [03:14<24:12:55, 46.17s/it]\u001b[A\n",
      "  0%|          | 6/1893 [03:17<17:21:01, 33.10s/it]\u001b[A\n",
      "  0%|          | 7/1893 [03:19<12:31:48, 23.92s/it]\u001b[A\n",
      "  0%|          | 8/1893 [03:22<9:05:08, 17.35s/it] \u001b[A\n",
      "  0%|          | 9/1893 [03:26<7:01:44, 13.43s/it]\u001b[A\n",
      "  1%|          | 10/1893 [03:30<5:36:56, 10.74s/it]\u001b[A\n",
      "  1%|          | 11/1893 [03:35<4:37:33,  8.85s/it]\u001b[A\n",
      "  1%|          | 12/1893 [03:39<3:58:11,  7.60s/it]\u001b[A\n",
      "  1%|          | 13/1893 [03:44<3:26:15,  6.58s/it]\u001b[A\n",
      "  1%|          | 14/1893 [03:47<2:58:39,  5.71s/it]\u001b[A\n",
      "  1%|          | 15/1893 [03:51<2:36:27,  5.00s/it]\u001b[A\n",
      "  1%|          | 16/1893 [03:54<2:20:01,  4.48s/it]\u001b[A\n",
      "  1%|          | 17/1893 [03:57<2:04:12,  3.97s/it]\u001b[A\n",
      "  1%|          | 18/1893 [04:00<1:55:40,  3.70s/it]\u001b[A\n",
      "  1%|          | 19/1893 [04:03<1:53:03,  3.62s/it]\u001b[A\n",
      "  1%|          | 20/1893 [04:06<1:42:27,  3.28s/it]\u001b[A\n",
      "  1%|          | 21/1893 [04:08<1:29:14,  2.86s/it]\u001b[A\n",
      "  1%|          | 22/1893 [04:09<1:18:06,  2.51s/it]\u001b[A\n",
      "  1%|          | 23/1893 [04:11<1:10:33,  2.26s/it]\u001b[A\n",
      "  1%|         | 24/1893 [04:14<1:15:00,  2.41s/it]\u001b[A\n",
      "  1%|         | 25/1893 [04:17<1:24:57,  2.73s/it]\u001b[A\n",
      "  1%|         | 26/1893 [04:20<1:29:13,  2.87s/it]\u001b[A\n",
      "  1%|         | 27/1893 [04:24<1:33:35,  3.01s/it]\u001b[A\n",
      "  1%|         | 28/1893 [04:27<1:34:09,  3.03s/it]\u001b[A\n",
      "  2%|         | 29/1893 [04:30<1:37:38,  3.14s/it]\u001b[A\n",
      "  2%|         | 30/1893 [04:34<1:42:48,  3.31s/it]\u001b[A\n",
      "  2%|         | 31/1893 [04:38<1:47:18,  3.46s/it]\u001b[A\n",
      "  2%|         | 32/1893 [04:41<1:48:28,  3.50s/it]\u001b[A\n",
      "  2%|         | 33/1893 [04:45<1:50:23,  3.56s/it]\u001b[A\n",
      "  2%|         | 34/1893 [04:48<1:46:41,  3.44s/it]\u001b[A\n",
      "  2%|         | 35/1893 [04:51<1:46:09,  3.43s/it]\u001b[A\n",
      "  2%|         | 36/1893 [04:54<1:40:40,  3.25s/it]\u001b[A\n",
      "  2%|         | 37/1893 [04:57<1:38:31,  3.19s/it]\u001b[A\n",
      "  2%|         | 38/1893 [05:00<1:35:16,  3.08s/it]\u001b[A\n",
      "  2%|         | 39/1893 [05:03<1:32:54,  3.01s/it]\u001b[A\n",
      "  2%|         | 40/1893 [05:06<1:34:37,  3.06s/it]\u001b[A\n",
      "  2%|         | 41/1893 [05:10<1:41:07,  3.28s/it]\u001b[A\n",
      "  2%|         | 42/1893 [05:13<1:42:18,  3.32s/it]\u001b[A\n",
      "  2%|         | 43/1893 [05:16<1:39:39,  3.23s/it]\u001b[A\n",
      "  2%|         | 44/1893 [05:19<1:36:43,  3.14s/it]\u001b[A\n",
      "  2%|         | 45/1893 [05:22<1:32:33,  3.01s/it]\u001b[A\n",
      "  2%|         | 46/1893 [05:25<1:28:20,  2.87s/it]\u001b[A\n",
      "  2%|         | 47/1893 [05:27<1:24:38,  2.75s/it]\u001b[A\n",
      "  3%|         | 48/1893 [05:29<1:19:56,  2.60s/it]\u001b[A\n",
      "  3%|         | 49/1893 [05:32<1:19:32,  2.59s/it]\u001b[A\n",
      "  3%|         | 50/1893 [05:34<1:19:19,  2.58s/it]\u001b[A\n",
      "  3%|         | 51/1893 [05:37<1:17:59,  2.54s/it]\u001b[A\n",
      "  3%|         | 52/1893 [05:41<1:28:16,  2.88s/it]\u001b[A\n",
      "  3%|         | 53/1893 [05:44<1:29:49,  2.93s/it]\u001b[A\n",
      "  3%|         | 54/1893 [05:47<1:34:01,  3.07s/it]\u001b[A\n",
      "  3%|         | 55/1893 [05:50<1:33:27,  3.05s/it]\u001b[A\n",
      "  3%|         | 56/1893 [05:53<1:28:15,  2.88s/it]\u001b[A\n",
      "  3%|         | 57/1893 [05:55<1:25:03,  2.78s/it]\u001b[A\n",
      "  3%|         | 58/1893 [05:58<1:22:43,  2.71s/it]\u001b[A\n",
      "  3%|         | 59/1893 [06:00<1:21:25,  2.66s/it]\u001b[A\n",
      "  3%|         | 60/1893 [06:03<1:23:27,  2.73s/it]\u001b[A\n",
      "  3%|         | 61/1893 [06:06<1:26:05,  2.82s/it]\u001b[A\n",
      "  3%|         | 62/1893 [06:08<1:20:33,  2.64s/it]\u001b[A\n",
      "  3%|         | 63/1893 [06:11<1:21:51,  2.68s/it]\u001b[A\n",
      "  3%|         | 64/1893 [06:14<1:19:41,  2.61s/it]\u001b[A\n",
      "  3%|         | 65/1893 [06:16<1:20:08,  2.63s/it]\u001b[A\n",
      "  3%|         | 66/1893 [06:19<1:17:43,  2.55s/it]\u001b[A\n",
      "  4%|         | 67/1893 [06:21<1:18:44,  2.59s/it]\u001b[A\n",
      "  4%|         | 68/1893 [06:24<1:16:23,  2.51s/it]\u001b[A\n",
      "  4%|         | 69/1893 [06:26<1:13:17,  2.41s/it]\u001b[A\n",
      "  4%|         | 70/1893 [06:28<1:12:44,  2.39s/it]\u001b[A\n",
      "  4%|         | 71/1893 [06:31<1:13:29,  2.42s/it]\u001b[A\n",
      "  4%|         | 72/1893 [06:33<1:10:44,  2.33s/it]\u001b[A\n",
      "  4%|         | 73/1893 [06:35<1:07:34,  2.23s/it]\u001b[A\n",
      "  4%|         | 74/1893 [06:37<1:11:37,  2.36s/it]\u001b[A\n",
      "  4%|         | 75/1893 [06:40<1:12:39,  2.40s/it]\u001b[A\n",
      "  4%|         | 76/1893 [06:42<1:14:11,  2.45s/it]\u001b[A\n",
      "  4%|         | 77/1893 [06:45<1:16:33,  2.53s/it]\u001b[A\n",
      "  4%|         | 78/1893 [06:50<1:38:32,  3.26s/it]\u001b[A\n",
      "  4%|         | 79/1893 [06:55<1:57:15,  3.88s/it]\u001b[A\n",
      "  4%|         | 80/1893 [06:59<1:58:01,  3.91s/it]\u001b[A\n",
      "  4%|         | 81/1893 [07:02<1:47:11,  3.55s/it]\u001b[A\n",
      "  4%|         | 82/1893 [07:04<1:35:45,  3.17s/it]\u001b[A\n",
      "  4%|         | 83/1893 [07:08<1:36:02,  3.18s/it]\u001b[A\n",
      "  4%|         | 84/1893 [07:11<1:35:34,  3.17s/it]\u001b[A\n",
      "  4%|         | 85/1893 [07:14<1:35:44,  3.18s/it]\u001b[A\n",
      "  5%|         | 86/1893 [07:17<1:30:09,  2.99s/it]\u001b[A\n",
      "  5%|         | 87/1893 [07:20<1:32:14,  3.06s/it]\u001b[A\n",
      "  5%|         | 88/1893 [07:23<1:31:17,  3.03s/it]\u001b[A\n",
      "  5%|         | 89/1893 [07:25<1:23:54,  2.79s/it]\u001b[A\n",
      "  5%|         | 90/1893 [07:27<1:16:26,  2.54s/it]\u001b[A\n",
      "  5%|         | 91/1893 [07:29<1:09:54,  2.33s/it]\u001b[A\n",
      "  5%|         | 92/1893 [07:30<1:04:12,  2.14s/it]\u001b[A\n",
      "  5%|         | 93/1893 [07:32<1:02:19,  2.08s/it]\u001b[A\n",
      "  5%|         | 94/1893 [07:34<59:41,  1.99s/it]  \u001b[A\n",
      "  5%|         | 95/1893 [07:36<57:55,  1.93s/it]\u001b[A\n",
      "  5%|         | 96/1893 [07:40<1:14:59,  2.50s/it]\u001b[A\n",
      "  5%|         | 97/1893 [07:43<1:25:31,  2.86s/it]\u001b[A\n",
      "  5%|         | 98/1893 [07:47<1:31:56,  3.07s/it]\u001b[A\n",
      "  5%|         | 99/1893 [07:51<1:35:54,  3.21s/it]\u001b[A\n",
      "  5%|         | 100/1893 [07:54<1:39:34,  3.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|         | 101/1893 [07:57<1:37:42,  3.27s/it]\u001b[A\n",
      "  5%|         | 102/1893 [08:00<1:29:09,  2.99s/it]\u001b[A\n",
      "  5%|         | 103/1893 [08:02<1:22:42,  2.77s/it]\u001b[A\n",
      "  5%|         | 104/1893 [08:04<1:16:57,  2.58s/it]\u001b[A\n",
      "  6%|         | 105/1893 [08:07<1:17:43,  2.61s/it]\u001b[A\n",
      "  6%|         | 106/1893 [08:09<1:16:32,  2.57s/it]\u001b[A\n",
      "  6%|         | 107/1893 [08:12<1:15:04,  2.52s/it]\u001b[A\n",
      "  6%|         | 108/1893 [08:14<1:12:38,  2.44s/it]\u001b[A\n",
      "  6%|         | 109/1893 [08:16<1:10:38,  2.38s/it]\u001b[A\n",
      "  6%|         | 110/1893 [08:19<1:13:49,  2.48s/it]\u001b[A\n",
      "  6%|         | 111/1893 [08:21<1:12:58,  2.46s/it]\u001b[A\n",
      "  6%|         | 112/1893 [08:24<1:13:31,  2.48s/it]\u001b[A\n",
      "  6%|         | 113/1893 [08:26<1:12:59,  2.46s/it]\u001b[A\n",
      "  6%|         | 114/1893 [08:28<1:05:49,  2.22s/it]\u001b[A\n",
      "  6%|         | 115/1893 [08:30<1:03:55,  2.16s/it]\u001b[A\n",
      "  6%|         | 116/1893 [08:32<1:05:32,  2.21s/it]\u001b[A\n",
      "  6%|         | 117/1893 [08:34<1:06:16,  2.24s/it]\u001b[A\n",
      "  6%|         | 118/1893 [08:37<1:04:53,  2.19s/it]\u001b[A\n",
      "  6%|         | 119/1893 [08:39<1:06:29,  2.25s/it]\u001b[A\n",
      "  6%|         | 120/1893 [08:41<1:06:29,  2.25s/it]\u001b[A\n",
      "  6%|         | 121/1893 [08:44<1:08:16,  2.31s/it]\u001b[A\n",
      "  6%|         | 122/1893 [08:46<1:10:35,  2.39s/it]\u001b[A\n",
      "  6%|         | 123/1893 [08:49<1:11:34,  2.43s/it]\u001b[A\n",
      "  7%|         | 124/1893 [08:50<1:05:25,  2.22s/it]\u001b[A\n",
      "  7%|         | 125/1893 [08:52<1:03:09,  2.14s/it]\u001b[A\n",
      "  7%|         | 126/1893 [08:54<1:01:20,  2.08s/it]\u001b[A\n",
      "  7%|         | 127/1893 [08:56<1:00:19,  2.05s/it]\u001b[A\n",
      "  7%|         | 128/1893 [08:59<1:01:44,  2.10s/it]\u001b[A\n",
      "  7%|         | 129/1893 [09:02<1:11:37,  2.44s/it]\u001b[A\n",
      "  7%|         | 130/1893 [09:05<1:14:18,  2.53s/it]\u001b[A\n",
      "  7%|         | 131/1893 [09:07<1:16:17,  2.60s/it]\u001b[A\n",
      "  7%|         | 132/1893 [09:10<1:21:31,  2.78s/it]\u001b[A\n",
      "  7%|         | 133/1893 [09:13<1:23:10,  2.84s/it]\u001b[A\n",
      "  7%|         | 134/1893 [09:16<1:16:22,  2.61s/it]\u001b[A\n",
      "  7%|         | 135/1893 [09:17<1:08:37,  2.34s/it]\u001b[A\n",
      "  7%|         | 136/1893 [09:19<1:01:40,  2.11s/it]\u001b[A\n",
      "  7%|         | 137/1893 [09:20<57:29,  1.96s/it]  \u001b[A\n",
      "  7%|         | 138/1893 [09:22<57:36,  1.97s/it]\u001b[A\n",
      "  7%|         | 139/1893 [09:25<59:39,  2.04s/it]\u001b[A\n",
      "  7%|         | 140/1893 [09:27<1:05:20,  2.24s/it]\u001b[A\n",
      "  7%|         | 141/1893 [09:30<1:07:48,  2.32s/it]\u001b[A\n",
      "  8%|         | 142/1893 [09:33<1:12:11,  2.47s/it]\u001b[A\n",
      "  8%|         | 143/1893 [09:35<1:11:43,  2.46s/it]\u001b[A\n",
      "  8%|         | 144/1893 [09:38<1:11:39,  2.46s/it]\u001b[A\n",
      "  8%|         | 145/1893 [09:40<1:08:15,  2.34s/it]\u001b[A\n",
      "  8%|         | 146/1893 [09:42<1:06:34,  2.29s/it]\u001b[A\n",
      "  8%|         | 147/1893 [09:44<1:08:25,  2.35s/it]\u001b[A\n",
      "  8%|         | 148/1893 [09:46<1:03:23,  2.18s/it]\u001b[A\n",
      "  8%|         | 149/1893 [09:48<1:01:50,  2.13s/it]\u001b[A\n",
      "  8%|         | 150/1893 [09:50<1:01:15,  2.11s/it]\u001b[A\n",
      "  8%|         | 151/1893 [09:52<1:03:28,  2.19s/it]\u001b[A\n",
      "  8%|         | 152/1893 [09:56<1:10:37,  2.43s/it]\u001b[A\n",
      "  8%|         | 153/1893 [09:59<1:16:44,  2.65s/it]\u001b[A\n",
      "  8%|         | 154/1893 [10:02<1:20:21,  2.77s/it]\u001b[A\n",
      "  8%|         | 155/1893 [10:05<1:27:53,  3.03s/it]\u001b[A\n",
      "  8%|         | 156/1893 [10:09<1:29:43,  3.10s/it]\u001b[A\n",
      "  8%|         | 157/1893 [10:12<1:29:34,  3.10s/it]\u001b[A\n",
      "  8%|         | 158/1893 [10:14<1:26:55,  3.01s/it]\u001b[A\n",
      "  8%|         | 159/1893 [10:18<1:28:45,  3.07s/it]\u001b[A\n",
      "  8%|         | 160/1893 [10:21<1:32:59,  3.22s/it]\u001b[A\n",
      "  9%|         | 161/1893 [10:24<1:32:12,  3.19s/it]\u001b[A\n",
      "  9%|         | 162/1893 [10:28<1:35:31,  3.31s/it]\u001b[A\n",
      "  9%|         | 163/1893 [10:31<1:31:16,  3.17s/it]\u001b[A\n",
      "  9%|         | 164/1893 [10:33<1:23:51,  2.91s/it]\u001b[A\n",
      "  9%|         | 165/1893 [10:36<1:27:12,  3.03s/it]\u001b[A\n",
      "  9%|         | 166/1893 [10:40<1:28:44,  3.08s/it]\u001b[A\n",
      "  9%|         | 167/1893 [10:42<1:23:13,  2.89s/it]\u001b[A\n",
      "  9%|         | 168/1893 [10:44<1:15:39,  2.63s/it]\u001b[A\n",
      "  9%|         | 169/1893 [10:46<1:11:32,  2.49s/it]\u001b[A\n",
      "  9%|         | 170/1893 [10:49<1:10:35,  2.46s/it]\u001b[A\n",
      "  9%|         | 171/1893 [10:51<1:08:48,  2.40s/it]\u001b[A\n",
      "  9%|         | 172/1893 [10:53<1:08:38,  2.39s/it]\u001b[A\n",
      "  9%|         | 173/1893 [10:56<1:09:05,  2.41s/it]\u001b[A\n",
      "  9%|         | 174/1893 [10:58<1:08:39,  2.40s/it]\u001b[A\n",
      "  9%|         | 175/1893 [11:01<1:11:38,  2.50s/it]\u001b[A\n",
      "  9%|         | 176/1893 [11:04<1:12:53,  2.55s/it]\u001b[A\n",
      "  9%|         | 177/1893 [11:06<1:14:59,  2.62s/it]\u001b[A\n",
      "  9%|         | 178/1893 [11:09<1:19:31,  2.78s/it]\u001b[A\n",
      "  9%|         | 179/1893 [11:13<1:22:05,  2.87s/it]\u001b[A\n",
      " 10%|         | 180/1893 [11:16<1:29:46,  3.14s/it]\u001b[A\n",
      " 10%|         | 181/1893 [11:18<1:19:01,  2.77s/it]\u001b[A\n",
      " 10%|         | 182/1893 [11:21<1:14:55,  2.63s/it]\u001b[A\n",
      " 10%|         | 183/1893 [11:23<1:12:01,  2.53s/it]\u001b[A\n",
      " 10%|         | 184/1893 [11:25<1:11:56,  2.53s/it]\u001b[A\n",
      " 10%|         | 185/1893 [11:28<1:11:27,  2.51s/it]\u001b[A\n",
      " 10%|         | 186/1893 [11:30<1:08:54,  2.42s/it]\u001b[A\n",
      " 10%|         | 187/1893 [11:33<1:10:25,  2.48s/it]\u001b[A\n",
      " 10%|         | 188/1893 [11:35<1:06:33,  2.34s/it]\u001b[A\n",
      " 10%|         | 189/1893 [11:36<57:59,  2.04s/it]  \u001b[A\n",
      " 10%|         | 190/1893 [11:37<51:43,  1.82s/it]\u001b[A\n",
      " 10%|         | 191/1893 [11:39<51:10,  1.80s/it]\u001b[A\n",
      " 10%|         | 192/1893 [11:41<50:22,  1.78s/it]\u001b[A\n",
      " 10%|         | 193/1893 [11:43<55:35,  1.96s/it]\u001b[A\n",
      " 10%|         | 194/1893 [11:46<1:01:33,  2.17s/it]\u001b[A\n",
      " 10%|         | 195/1893 [11:49<1:09:29,  2.46s/it]\u001b[A\n",
      " 10%|         | 196/1893 [11:52<1:13:41,  2.61s/it]\u001b[A\n",
      " 10%|         | 197/1893 [11:55<1:15:33,  2.67s/it]\u001b[A\n",
      " 10%|         | 198/1893 [11:57<1:09:00,  2.44s/it]\u001b[A\n",
      " 11%|         | 199/1893 [11:59<1:08:36,  2.43s/it]\u001b[A\n",
      " 11%|         | 200/1893 [12:01<1:08:00,  2.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 11%|         | 201/1893 [12:04<1:05:48,  2.33s/it]\u001b[A\n",
      " 11%|         | 202/1893 [12:06<1:04:54,  2.30s/it]\u001b[A\n",
      " 11%|         | 203/1893 [12:08<1:06:35,  2.36s/it]\u001b[A\n",
      " 11%|         | 204/1893 [12:11<1:09:00,  2.45s/it]\u001b[A\n",
      " 11%|         | 205/1893 [12:14<1:12:54,  2.59s/it]\u001b[A\n",
      " 11%|         | 206/1893 [12:16<1:12:08,  2.57s/it]\u001b[A\n",
      " 11%|         | 207/1893 [12:19<1:14:45,  2.66s/it]\u001b[A\n",
      " 11%|         | 208/1893 [12:22<1:14:21,  2.65s/it]\u001b[A\n",
      " 11%|         | 209/1893 [12:24<1:13:29,  2.62s/it]\u001b[A\n",
      " 11%|         | 210/1893 [12:27<1:12:21,  2.58s/it]\u001b[A\n",
      " 11%|         | 211/1893 [12:30<1:13:48,  2.63s/it]\u001b[A\n",
      " 11%|         | 212/1893 [12:32<1:10:18,  2.51s/it]\u001b[A\n",
      " 11%|        | 213/1893 [12:34<1:09:30,  2.48s/it]\u001b[A\n",
      " 11%|        | 214/1893 [12:37<1:11:13,  2.55s/it]\u001b[A\n",
      " 11%|        | 215/1893 [12:40<1:10:42,  2.53s/it]\u001b[A\n",
      " 11%|        | 216/1893 [12:42<1:09:28,  2.49s/it]\u001b[A\n",
      " 11%|        | 217/1893 [12:44<1:08:25,  2.45s/it]\u001b[A\n",
      " 12%|        | 218/1893 [12:47<1:09:09,  2.48s/it]\u001b[A\n",
      " 12%|        | 219/1893 [12:49<1:10:40,  2.53s/it]\u001b[A\n",
      " 12%|        | 220/1893 [12:52<1:14:36,  2.68s/it]\u001b[A\n",
      " 12%|        | 221/1893 [12:55<1:09:09,  2.48s/it]\u001b[A\n",
      " 12%|        | 222/1893 [12:57<1:07:17,  2.42s/it]\u001b[A\n",
      " 12%|        | 223/1893 [12:59<1:06:28,  2.39s/it]\u001b[A\n",
      " 12%|        | 224/1893 [13:02<1:07:12,  2.42s/it]\u001b[A\n",
      " 12%|        | 225/1893 [13:04<1:06:06,  2.38s/it]\u001b[A\n",
      " 12%|        | 226/1893 [13:06<1:06:57,  2.41s/it]\u001b[A\n",
      " 12%|        | 227/1893 [13:09<1:09:15,  2.49s/it]\u001b[A\n",
      " 12%|        | 228/1893 [13:12<1:12:03,  2.60s/it]\u001b[A\n",
      " 12%|        | 229/1893 [13:15<1:13:41,  2.66s/it]\u001b[A\n",
      " 12%|        | 230/1893 [13:18<1:18:42,  2.84s/it]\u001b[A\n",
      " 12%|        | 231/1893 [13:21<1:18:20,  2.83s/it]\u001b[A\n",
      " 12%|        | 232/1893 [13:24<1:19:35,  2.88s/it]\u001b[A\n",
      " 12%|        | 233/1893 [13:27<1:19:16,  2.87s/it]\u001b[A\n",
      " 12%|        | 234/1893 [13:30<1:21:59,  2.97s/it]\u001b[A\n",
      " 12%|        | 235/1893 [13:33<1:21:08,  2.94s/it]\u001b[A\n",
      " 12%|        | 236/1893 [13:36<1:26:26,  3.13s/it]\u001b[A\n",
      " 13%|        | 237/1893 [13:39<1:26:39,  3.14s/it]\u001b[A\n",
      " 13%|        | 238/1893 [13:42<1:25:09,  3.09s/it]\u001b[A\n",
      " 13%|        | 239/1893 [13:45<1:21:47,  2.97s/it]\u001b[A\n",
      " 13%|        | 240/1893 [13:47<1:15:53,  2.75s/it]\u001b[A\n",
      " 13%|        | 241/1893 [13:50<1:13:53,  2.68s/it]\u001b[A\n",
      " 13%|        | 242/1893 [13:52<1:11:52,  2.61s/it]\u001b[A\n",
      " 13%|        | 243/1893 [13:55<1:09:52,  2.54s/it]\u001b[A\n",
      " 13%|        | 244/1893 [13:57<1:05:58,  2.40s/it]\u001b[A\n",
      " 13%|        | 245/1893 [14:00<1:12:41,  2.65s/it]\u001b[A\n",
      " 13%|        | 246/1893 [14:03<1:17:08,  2.81s/it]\u001b[A\n",
      " 13%|        | 247/1893 [14:07<1:24:39,  3.09s/it]\u001b[A\n",
      " 13%|        | 248/1893 [14:11<1:32:25,  3.37s/it]\u001b[A\n",
      " 13%|        | 249/1893 [14:15<1:38:24,  3.59s/it]\u001b[A\n",
      " 13%|        | 250/1893 [14:19<1:37:59,  3.58s/it]\u001b[A\n",
      " 13%|        | 251/1893 [14:22<1:34:34,  3.46s/it]\u001b[A\n",
      " 13%|        | 252/1893 [14:26<1:39:00,  3.62s/it]\u001b[A\n",
      " 13%|        | 253/1893 [14:29<1:39:43,  3.65s/it]\u001b[A\n",
      " 13%|        | 254/1893 [14:33<1:35:56,  3.51s/it]\u001b[A\n",
      " 13%|        | 255/1893 [14:35<1:30:20,  3.31s/it]\u001b[A\n",
      " 14%|        | 256/1893 [14:38<1:24:18,  3.09s/it]\u001b[A\n",
      " 14%|        | 257/1893 [14:40<1:18:49,  2.89s/it]\u001b[A\n",
      " 14%|        | 258/1893 [14:42<1:07:30,  2.48s/it]\u001b[A\n",
      " 14%|        | 259/1893 [14:44<1:02:12,  2.28s/it]\u001b[A\n",
      " 14%|        | 260/1893 [14:47<1:07:57,  2.50s/it]\u001b[A\n",
      " 14%|        | 261/1893 [14:50<1:11:31,  2.63s/it]\u001b[A\n",
      " 14%|        | 262/1893 [14:53<1:17:21,  2.85s/it]\u001b[A\n",
      " 14%|        | 263/1893 [14:57<1:25:15,  3.14s/it]\u001b[A\n",
      " 14%|        | 264/1893 [15:01<1:31:02,  3.35s/it]\u001b[A\n",
      " 14%|        | 265/1893 [15:03<1:25:39,  3.16s/it]\u001b[A\n",
      " 14%|        | 266/1893 [15:06<1:23:11,  3.07s/it]\u001b[A\n",
      " 14%|        | 267/1893 [15:09<1:23:42,  3.09s/it]\u001b[A\n",
      " 14%|        | 268/1893 [15:12<1:22:50,  3.06s/it]\u001b[A\n",
      " 14%|        | 269/1893 [15:15<1:19:21,  2.93s/it]\u001b[A\n",
      " 14%|        | 270/1893 [15:18<1:19:00,  2.92s/it]\u001b[A\n",
      " 14%|        | 271/1893 [15:21<1:22:16,  3.04s/it]\u001b[A\n",
      " 14%|        | 272/1893 [15:24<1:18:18,  2.90s/it]\u001b[A\n",
      " 14%|        | 273/1893 [15:26<1:13:47,  2.73s/it]\u001b[A\n",
      " 14%|        | 274/1893 [15:29<1:12:11,  2.68s/it]\u001b[A\n",
      " 15%|        | 275/1893 [15:31<1:05:56,  2.45s/it]\u001b[A\n",
      " 15%|        | 276/1893 [15:33<1:06:06,  2.45s/it]\u001b[A\n",
      " 15%|        | 277/1893 [15:35<1:03:00,  2.34s/it]\u001b[A\n",
      " 15%|        | 278/1893 [15:38<1:04:33,  2.40s/it]\u001b[A\n",
      " 15%|        | 279/1893 [15:40<1:04:27,  2.40s/it]\u001b[A\n",
      " 15%|        | 280/1893 [15:42<1:03:16,  2.35s/it]\u001b[A\n",
      " 15%|        | 281/1893 [15:45<1:03:33,  2.37s/it]\u001b[A\n",
      " 15%|        | 282/1893 [15:47<58:29,  2.18s/it]  \u001b[A\n",
      " 15%|        | 283/1893 [15:48<54:05,  2.02s/it]\u001b[A\n",
      " 15%|        | 284/1893 [15:50<52:16,  1.95s/it]\u001b[A\n",
      " 15%|        | 285/1893 [15:52<54:00,  2.02s/it]\u001b[A\n",
      " 15%|        | 286/1893 [15:55<59:27,  2.22s/it]\u001b[A\n",
      " 15%|        | 287/1893 [15:57<1:00:41,  2.27s/it]\u001b[A\n",
      " 15%|        | 288/1893 [16:00<1:01:46,  2.31s/it]\u001b[A\n",
      " 15%|        | 289/1893 [16:02<58:37,  2.19s/it]  \u001b[A\n",
      " 15%|        | 290/1893 [16:05<1:06:18,  2.48s/it]\u001b[A\n",
      " 15%|        | 291/1893 [16:08<1:11:05,  2.66s/it]\u001b[A\n",
      " 15%|        | 292/1893 [16:11<1:17:03,  2.89s/it]\u001b[A\n",
      " 15%|        | 293/1893 [16:14<1:18:51,  2.96s/it]\u001b[A\n",
      " 16%|        | 294/1893 [16:17<1:13:24,  2.75s/it]\u001b[A\n",
      " 16%|        | 295/1893 [16:18<1:06:17,  2.49s/it]\u001b[A\n",
      " 16%|        | 296/1893 [16:20<1:02:11,  2.34s/it]\u001b[A\n",
      " 16%|        | 297/1893 [16:25<1:16:11,  2.86s/it]\u001b[A\n",
      " 16%|        | 298/1893 [16:28<1:23:36,  3.14s/it]\u001b[A\n",
      " 16%|        | 299/1893 [16:32<1:30:31,  3.41s/it]\u001b[A\n",
      " 16%|        | 300/1893 [16:37<1:38:50,  3.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|        | 301/1893 [16:41<1:44:33,  3.94s/it]\u001b[A\n",
      " 16%|        | 302/1893 [16:45<1:40:11,  3.78s/it]\u001b[A\n",
      " 16%|        | 303/1893 [16:47<1:27:41,  3.31s/it]\u001b[A\n",
      " 16%|        | 304/1893 [16:49<1:19:07,  2.99s/it]\u001b[A\n",
      " 16%|        | 305/1893 [16:52<1:15:09,  2.84s/it]\u001b[A\n",
      " 16%|        | 306/1893 [16:54<1:13:11,  2.77s/it]\u001b[A\n",
      " 16%|        | 307/1893 [16:58<1:18:02,  2.95s/it]\u001b[A\n",
      " 16%|        | 308/1893 [17:01<1:19:09,  3.00s/it]\u001b[A\n",
      " 16%|        | 309/1893 [17:04<1:21:42,  3.10s/it]\u001b[A\n",
      " 16%|        | 310/1893 [17:07<1:20:02,  3.03s/it]\u001b[A\n",
      " 16%|        | 311/1893 [17:09<1:15:58,  2.88s/it]\u001b[A\n",
      " 16%|        | 312/1893 [17:12<1:10:23,  2.67s/it]\u001b[A\n",
      " 17%|        | 313/1893 [17:15<1:12:42,  2.76s/it]\u001b[A\n",
      " 17%|        | 314/1893 [17:18<1:16:27,  2.91s/it]\u001b[A\n",
      " 17%|        | 315/1893 [17:21<1:21:13,  3.09s/it]\u001b[A\n",
      " 17%|        | 316/1893 [17:25<1:24:34,  3.22s/it]\u001b[A\n",
      " 17%|        | 317/1893 [17:29<1:28:15,  3.36s/it]\u001b[A\n",
      " 17%|        | 318/1893 [17:32<1:30:33,  3.45s/it]\u001b[A\n",
      " 17%|        | 319/1893 [17:36<1:32:12,  3.52s/it]\u001b[A\n",
      " 17%|        | 320/1893 [17:40<1:36:17,  3.67s/it]\u001b[A\n",
      " 17%|        | 321/1893 [17:42<1:25:27,  3.26s/it]\u001b[A\n",
      " 17%|        | 322/1893 [17:45<1:18:44,  3.01s/it]\u001b[A\n",
      " 17%|        | 323/1893 [17:47<1:15:52,  2.90s/it]\u001b[A\n",
      " 17%|        | 324/1893 [17:50<1:10:53,  2.71s/it]\u001b[A\n",
      " 17%|        | 325/1893 [17:53<1:16:52,  2.94s/it]\u001b[A\n",
      " 17%|        | 326/1893 [17:56<1:18:56,  3.02s/it]\u001b[A\n",
      " 17%|        | 327/1893 [17:59<1:16:33,  2.93s/it]\u001b[A\n",
      " 17%|        | 328/1893 [18:01<1:10:50,  2.72s/it]\u001b[A\n",
      " 17%|        | 329/1893 [18:04<1:09:21,  2.66s/it]\u001b[A\n",
      " 17%|        | 330/1893 [18:07<1:11:42,  2.75s/it]\u001b[A\n",
      " 17%|        | 331/1893 [18:10<1:12:53,  2.80s/it]\u001b[A\n",
      " 18%|        | 332/1893 [18:13<1:17:33,  2.98s/it]\u001b[A\n",
      " 18%|        | 333/1893 [18:16<1:20:12,  3.08s/it]\u001b[A\n",
      " 18%|        | 334/1893 [18:19<1:18:41,  3.03s/it]\u001b[A\n",
      " 18%|        | 335/1893 [18:22<1:20:03,  3.08s/it]\u001b[A\n",
      " 18%|        | 336/1893 [18:25<1:19:51,  3.08s/it]\u001b[A\n",
      " 18%|        | 337/1893 [18:29<1:22:42,  3.19s/it]\u001b[A\n",
      " 18%|        | 338/1893 [18:33<1:26:53,  3.35s/it]\u001b[A\n",
      " 18%|        | 339/1893 [18:36<1:26:32,  3.34s/it]\u001b[A\n",
      " 18%|        | 340/1893 [18:40<1:29:04,  3.44s/it]\u001b[A\n",
      " 18%|        | 341/1893 [18:43<1:25:32,  3.31s/it]\u001b[A\n",
      " 18%|        | 342/1893 [18:45<1:16:17,  2.95s/it]\u001b[A\n",
      " 18%|        | 343/1893 [18:47<1:09:56,  2.71s/it]\u001b[A\n",
      " 18%|        | 344/1893 [18:49<1:07:53,  2.63s/it]\u001b[A\n",
      " 18%|        | 345/1893 [18:52<1:06:11,  2.57s/it]\u001b[A\n",
      " 18%|        | 346/1893 [18:54<1:02:00,  2.40s/it]\u001b[A\n",
      " 18%|        | 347/1893 [18:56<59:01,  2.29s/it]  \u001b[A\n",
      " 18%|        | 348/1893 [18:58<55:07,  2.14s/it]\u001b[A\n",
      " 18%|        | 349/1893 [19:00<56:39,  2.20s/it]\u001b[A\n",
      " 18%|        | 350/1893 [19:03<59:42,  2.32s/it]\u001b[A\n",
      " 19%|        | 351/1893 [19:05<1:03:42,  2.48s/it]\u001b[A\n",
      " 19%|        | 352/1893 [19:08<1:00:55,  2.37s/it]\u001b[A\n",
      " 19%|        | 353/1893 [19:10<1:00:45,  2.37s/it]\u001b[A\n",
      " 19%|        | 354/1893 [19:12<1:01:33,  2.40s/it]\u001b[A\n",
      " 19%|        | 355/1893 [19:14<59:01,  2.30s/it]  \u001b[A\n",
      " 19%|        | 356/1893 [19:16<54:53,  2.14s/it]\u001b[A\n",
      " 19%|        | 357/1893 [19:19<56:14,  2.20s/it]\u001b[A\n",
      " 19%|        | 358/1893 [19:21<56:50,  2.22s/it]\u001b[A\n",
      " 19%|        | 359/1893 [19:23<53:35,  2.10s/it]\u001b[A\n",
      " 19%|        | 360/1893 [19:25<52:02,  2.04s/it]\u001b[A\n",
      " 19%|        | 361/1893 [19:26<50:27,  1.98s/it]\u001b[A\n",
      " 19%|        | 362/1893 [19:28<49:00,  1.92s/it]\u001b[A\n",
      " 19%|        | 363/1893 [19:30<51:30,  2.02s/it]\u001b[A\n",
      " 19%|        | 364/1893 [19:32<51:46,  2.03s/it]\u001b[A\n",
      " 19%|        | 365/1893 [19:34<51:19,  2.02s/it]\u001b[A\n",
      " 19%|        | 366/1893 [19:36<50:40,  1.99s/it]\u001b[A\n",
      " 19%|        | 367/1893 [19:39<56:04,  2.20s/it]\u001b[A\n",
      " 19%|        | 368/1893 [19:42<1:00:26,  2.38s/it]\u001b[A\n",
      " 19%|        | 369/1893 [19:45<1:02:49,  2.47s/it]\u001b[A\n",
      " 20%|        | 370/1893 [19:47<1:02:02,  2.44s/it]\u001b[A\n",
      " 20%|        | 371/1893 [19:49<1:00:05,  2.37s/it]\u001b[A\n",
      " 20%|        | 372/1893 [19:51<58:02,  2.29s/it]  \u001b[A\n",
      " 20%|        | 373/1893 [19:53<56:20,  2.22s/it]\u001b[A\n",
      " 20%|        | 374/1893 [19:55<55:51,  2.21s/it]\u001b[A\n",
      " 20%|        | 375/1893 [19:57<53:17,  2.11s/it]\u001b[A\n",
      " 20%|        | 376/1893 [19:59<51:10,  2.02s/it]\u001b[A\n",
      " 20%|        | 377/1893 [20:01<48:41,  1.93s/it]\u001b[A\n",
      " 20%|        | 378/1893 [20:03<48:52,  1.94s/it]\u001b[A\n",
      " 20%|        | 379/1893 [20:05<50:13,  1.99s/it]\u001b[A\n",
      " 20%|        | 380/1893 [20:07<50:13,  1.99s/it]\u001b[A\n",
      " 20%|        | 381/1893 [20:09<52:29,  2.08s/it]\u001b[A\n",
      " 20%|        | 382/1893 [20:12<55:19,  2.20s/it]\u001b[A\n",
      " 20%|        | 383/1893 [20:14<59:06,  2.35s/it]\u001b[A\n",
      " 20%|        | 384/1893 [20:17<59:31,  2.37s/it]\u001b[A\n",
      " 20%|        | 385/1893 [20:19<1:00:34,  2.41s/it]\u001b[A\n",
      " 20%|        | 386/1893 [20:22<1:01:57,  2.47s/it]\u001b[A\n",
      " 20%|        | 387/1893 [20:24<1:00:00,  2.39s/it]\u001b[A\n",
      " 20%|        | 388/1893 [20:26<59:34,  2.38s/it]  \u001b[A\n",
      " 21%|        | 389/1893 [20:29<59:55,  2.39s/it]\u001b[A\n",
      " 21%|        | 390/1893 [20:32<1:01:37,  2.46s/it]\u001b[A\n",
      " 21%|        | 391/1893 [20:34<1:02:04,  2.48s/it]\u001b[A\n",
      " 21%|        | 392/1893 [20:37<1:03:08,  2.52s/it]\u001b[A\n",
      " 21%|        | 393/1893 [20:39<1:05:05,  2.60s/it]\u001b[A\n",
      " 21%|        | 394/1893 [20:42<1:05:14,  2.61s/it]\u001b[A\n",
      " 21%|        | 395/1893 [20:44<1:03:08,  2.53s/it]\u001b[A\n",
      " 21%|        | 396/1893 [20:47<1:04:53,  2.60s/it]\u001b[A\n",
      " 21%|        | 397/1893 [20:50<1:07:24,  2.70s/it]\u001b[A\n",
      " 21%|        | 398/1893 [20:53<1:05:28,  2.63s/it]\u001b[A\n",
      " 21%|        | 399/1893 [20:56<1:08:14,  2.74s/it]\u001b[A\n",
      " 21%|        | 400/1893 [20:58<1:07:59,  2.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|        | 401/1893 [21:01<1:10:18,  2.83s/it]\u001b[A\n",
      " 21%|        | 402/1893 [21:04<1:11:39,  2.88s/it]\u001b[A\n",
      " 21%|       | 403/1893 [21:07<1:13:07,  2.94s/it]\u001b[A\n",
      " 21%|       | 404/1893 [21:10<1:10:15,  2.83s/it]\u001b[A\n",
      " 21%|       | 405/1893 [21:13<1:08:12,  2.75s/it]\u001b[A\n",
      " 21%|       | 406/1893 [21:16<1:11:23,  2.88s/it]\u001b[A\n",
      " 22%|       | 407/1893 [21:18<1:08:54,  2.78s/it]\u001b[A\n",
      " 22%|       | 408/1893 [21:21<1:06:41,  2.69s/it]\u001b[A\n",
      " 22%|       | 409/1893 [21:23<1:06:25,  2.69s/it]\u001b[A\n",
      " 22%|       | 410/1893 [21:26<1:05:24,  2.65s/it]\u001b[A\n",
      " 22%|       | 411/1893 [21:29<1:04:54,  2.63s/it]\u001b[A\n",
      " 22%|       | 412/1893 [21:32<1:10:13,  2.85s/it]\u001b[A\n",
      " 22%|       | 413/1893 [21:35<1:09:33,  2.82s/it]\u001b[A\n",
      " 22%|       | 414/1893 [21:38<1:09:40,  2.83s/it]\u001b[A\n",
      " 22%|       | 415/1893 [21:41<1:13:09,  2.97s/it]\u001b[A\n",
      " 22%|       | 416/1893 [21:44<1:12:05,  2.93s/it]\u001b[A\n",
      " 22%|       | 417/1893 [21:47<1:11:10,  2.89s/it]\u001b[A\n",
      " 22%|       | 418/1893 [21:49<1:09:39,  2.83s/it]\u001b[A\n",
      " 22%|       | 419/1893 [21:52<1:08:32,  2.79s/it]\u001b[A\n",
      " 22%|       | 420/1893 [21:55<1:11:39,  2.92s/it]\u001b[A\n",
      " 22%|       | 421/1893 [21:58<1:12:12,  2.94s/it]\u001b[A\n",
      " 22%|       | 422/1893 [22:01<1:13:00,  2.98s/it]\u001b[A\n",
      " 22%|       | 423/1893 [22:04<1:13:44,  3.01s/it]\u001b[A\n",
      " 22%|       | 424/1893 [22:07<1:13:42,  3.01s/it]\u001b[A\n",
      " 22%|       | 425/1893 [22:10<1:11:46,  2.93s/it]\u001b[A\n",
      " 23%|       | 426/1893 [22:13<1:12:20,  2.96s/it]\u001b[A\n",
      " 23%|       | 427/1893 [22:16<1:13:46,  3.02s/it]\u001b[A\n",
      " 23%|       | 428/1893 [22:19<1:13:47,  3.02s/it]\u001b[A\n",
      " 23%|       | 429/1893 [22:23<1:17:14,  3.17s/it]\u001b[A\n",
      " 23%|       | 430/1893 [22:26<1:17:15,  3.17s/it]\u001b[A\n",
      " 23%|       | 431/1893 [22:29<1:14:08,  3.04s/it]\u001b[A\n",
      " 23%|       | 432/1893 [22:32<1:13:04,  3.00s/it]\u001b[A\n",
      " 23%|       | 433/1893 [22:35<1:14:14,  3.05s/it]\u001b[A\n",
      " 23%|       | 434/1893 [22:38<1:16:42,  3.15s/it]\u001b[A\n",
      " 23%|       | 435/1893 [22:41<1:17:56,  3.21s/it]\u001b[A\n",
      " 23%|       | 436/1893 [22:45<1:18:15,  3.22s/it]\u001b[A\n",
      " 23%|       | 437/1893 [22:48<1:19:20,  3.27s/it]\u001b[A\n",
      " 23%|       | 438/1893 [22:52<1:22:10,  3.39s/it]\u001b[A\n",
      " 23%|       | 439/1893 [22:55<1:22:06,  3.39s/it]\u001b[A\n",
      " 23%|       | 440/1893 [22:58<1:19:55,  3.30s/it]\u001b[A\n",
      " 23%|       | 441/1893 [23:02<1:20:51,  3.34s/it]\u001b[A\n",
      " 23%|       | 442/1893 [23:05<1:19:44,  3.30s/it]\u001b[A\n",
      " 23%|       | 443/1893 [23:08<1:18:48,  3.26s/it]\u001b[A\n",
      " 23%|       | 444/1893 [23:11<1:16:53,  3.18s/it]\u001b[A\n",
      " 24%|       | 445/1893 [23:14<1:13:57,  3.06s/it]\u001b[A\n",
      " 24%|       | 446/1893 [23:16<1:10:22,  2.92s/it]\u001b[A\n",
      " 24%|       | 447/1893 [23:19<1:07:46,  2.81s/it]\u001b[A\n",
      " 24%|       | 448/1893 [23:21<1:05:23,  2.72s/it]\u001b[A\n",
      " 24%|       | 449/1893 [23:24<1:05:02,  2.70s/it]\u001b[A\n",
      " 24%|       | 450/1893 [23:27<1:03:29,  2.64s/it]\u001b[A\n",
      " 24%|       | 451/1893 [23:30<1:09:41,  2.90s/it]\u001b[A\n",
      " 24%|       | 452/1893 [23:33<1:10:19,  2.93s/it]\u001b[A\n",
      " 24%|       | 453/1893 [23:36<1:11:01,  2.96s/it]\u001b[A\n",
      " 24%|       | 454/1893 [23:39<1:08:44,  2.87s/it]\u001b[A\n",
      " 24%|       | 455/1893 [23:42<1:07:36,  2.82s/it]\u001b[A\n",
      " 24%|       | 456/1893 [23:44<1:06:28,  2.78s/it]\u001b[A\n",
      " 24%|       | 457/1893 [23:47<1:06:33,  2.78s/it]\u001b[A\n",
      " 24%|       | 458/1893 [23:50<1:04:51,  2.71s/it]\u001b[A\n",
      " 24%|       | 459/1893 [23:52<1:05:09,  2.73s/it]\u001b[A\n",
      " 24%|       | 460/1893 [23:55<1:02:11,  2.60s/it]\u001b[A\n",
      " 24%|       | 461/1893 [23:57<1:01:58,  2.60s/it]\u001b[A\n",
      " 24%|       | 462/1893 [24:00<1:03:22,  2.66s/it]\u001b[A\n",
      " 24%|       | 463/1893 [24:03<1:06:13,  2.78s/it]\u001b[A\n",
      " 25%|       | 464/1893 [24:07<1:11:48,  3.02s/it]\u001b[A\n",
      " 25%|       | 465/1893 [24:10<1:11:26,  3.00s/it]\u001b[A\n",
      " 25%|       | 466/1893 [24:13<1:11:35,  3.01s/it]\u001b[A\n",
      " 25%|       | 467/1893 [24:16<1:15:57,  3.20s/it]\u001b[A\n",
      " 25%|       | 468/1893 [24:20<1:16:56,  3.24s/it]\u001b[A\n",
      " 25%|       | 469/1893 [24:23<1:19:20,  3.34s/it]\u001b[A\n",
      " 25%|       | 470/1893 [24:27<1:20:39,  3.40s/it]\u001b[A\n",
      " 25%|       | 471/1893 [24:29<1:12:24,  3.06s/it]\u001b[A\n",
      " 25%|       | 472/1893 [24:32<1:12:10,  3.05s/it]\u001b[A\n",
      " 25%|       | 473/1893 [24:35<1:11:22,  3.02s/it]\u001b[A\n",
      " 25%|       | 474/1893 [24:38<1:12:53,  3.08s/it]\u001b[A\n",
      " 25%|       | 475/1893 [24:42<1:15:06,  3.18s/it]\u001b[A\n",
      " 25%|       | 476/1893 [24:45<1:18:29,  3.32s/it]\u001b[A\n",
      " 25%|       | 477/1893 [24:48<1:17:29,  3.28s/it]\u001b[A\n",
      " 25%|       | 478/1893 [24:52<1:19:44,  3.38s/it]\u001b[A\n",
      " 25%|       | 479/1893 [24:55<1:16:36,  3.25s/it]\u001b[A\n",
      " 25%|       | 480/1893 [24:59<1:19:00,  3.36s/it]\u001b[A\n",
      " 25%|       | 481/1893 [25:02<1:20:26,  3.42s/it]\u001b[A\n",
      " 25%|       | 482/1893 [25:06<1:22:22,  3.50s/it]\u001b[A\n",
      " 26%|       | 483/1893 [25:09<1:21:26,  3.47s/it]\u001b[A\n",
      " 26%|       | 484/1893 [25:13<1:22:34,  3.52s/it]\u001b[A\n",
      " 26%|       | 485/1893 [25:15<1:16:01,  3.24s/it]\u001b[A\n",
      " 26%|       | 486/1893 [25:18<1:10:29,  3.01s/it]\u001b[A\n",
      " 26%|       | 487/1893 [25:20<1:06:19,  2.83s/it]\u001b[A\n",
      " 26%|       | 488/1893 [25:23<1:02:51,  2.68s/it]\u001b[A\n",
      " 26%|       | 489/1893 [25:26<1:04:34,  2.76s/it]\u001b[A\n",
      " 26%|       | 490/1893 [25:30<1:15:25,  3.23s/it]\u001b[A\n",
      " 26%|       | 491/1893 [25:34<1:18:38,  3.37s/it]\u001b[A\n",
      " 26%|       | 492/1893 [25:37<1:15:22,  3.23s/it]\u001b[A\n",
      " 26%|       | 493/1893 [25:39<1:11:40,  3.07s/it]\u001b[A\n",
      " 26%|       | 494/1893 [25:42<1:07:10,  2.88s/it]\u001b[A\n",
      " 26%|       | 495/1893 [25:45<1:07:00,  2.88s/it]\u001b[A\n",
      " 26%|       | 496/1893 [25:47<1:06:16,  2.85s/it]\u001b[A\n",
      " 26%|       | 497/1893 [25:50<1:04:26,  2.77s/it]\u001b[A\n",
      " 26%|       | 498/1893 [25:53<1:03:48,  2.74s/it]\u001b[A\n",
      " 26%|       | 499/1893 [25:56<1:06:30,  2.86s/it]\u001b[A\n",
      " 26%|       | 500/1893 [25:58<1:05:36,  2.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|       | 501/1893 [26:00<59:03,  2.55s/it]  \u001b[A\n",
      " 27%|       | 502/1893 [26:03<56:59,  2.46s/it]\u001b[A\n",
      " 27%|       | 503/1893 [26:05<57:51,  2.50s/it]\u001b[A\n",
      " 27%|       | 504/1893 [26:08<58:41,  2.54s/it]\u001b[A\n",
      " 27%|       | 505/1893 [26:10<59:04,  2.55s/it]\u001b[A\n",
      " 27%|       | 506/1893 [26:13<58:49,  2.54s/it]\u001b[A\n",
      " 27%|       | 507/1893 [26:15<57:50,  2.50s/it]\u001b[A\n",
      " 27%|       | 508/1893 [26:18<55:21,  2.40s/it]\u001b[A\n",
      " 27%|       | 509/1893 [26:20<56:49,  2.46s/it]\u001b[A\n",
      " 27%|       | 510/1893 [26:22<54:14,  2.35s/it]\u001b[A\n",
      " 27%|       | 511/1893 [26:25<55:25,  2.41s/it]\u001b[A\n",
      " 27%|       | 512/1893 [26:27<54:15,  2.36s/it]\u001b[A\n",
      " 27%|       | 513/1893 [26:29<55:08,  2.40s/it]\u001b[A\n",
      " 27%|       | 514/1893 [26:32<54:46,  2.38s/it]\u001b[A\n",
      " 27%|       | 515/1893 [26:34<53:23,  2.32s/it]\u001b[A\n",
      " 27%|       | 516/1893 [26:36<52:35,  2.29s/it]\u001b[A\n",
      " 27%|       | 517/1893 [26:39<58:45,  2.56s/it]\u001b[A\n",
      " 27%|       | 518/1893 [26:42<58:29,  2.55s/it]\u001b[A\n",
      " 27%|       | 519/1893 [26:45<59:23,  2.59s/it]\u001b[A\n",
      " 27%|       | 520/1893 [26:48<1:01:26,  2.69s/it]\u001b[A\n",
      " 28%|       | 521/1893 [26:50<58:38,  2.56s/it]  \u001b[A\n",
      " 28%|       | 522/1893 [26:52<58:08,  2.54s/it]\u001b[A\n",
      " 28%|       | 523/1893 [26:55<56:18,  2.47s/it]\u001b[A\n",
      " 28%|       | 524/1893 [26:58<59:55,  2.63s/it]\u001b[A\n",
      " 28%|       | 525/1893 [27:00<59:30,  2.61s/it]\u001b[A\n",
      " 28%|       | 526/1893 [27:03<59:25,  2.61s/it]\u001b[A\n",
      " 28%|       | 527/1893 [27:04<52:50,  2.32s/it]\u001b[A\n",
      " 28%|       | 528/1893 [27:07<50:58,  2.24s/it]\u001b[A\n",
      " 28%|       | 529/1893 [27:09<52:40,  2.32s/it]\u001b[A\n",
      " 28%|       | 530/1893 [27:12<59:39,  2.63s/it]\u001b[A\n",
      " 28%|       | 531/1893 [27:15<59:30,  2.62s/it]\u001b[A\n",
      " 28%|       | 532/1893 [27:19<1:06:26,  2.93s/it]\u001b[A\n",
      " 28%|       | 533/1893 [27:22<1:10:47,  3.12s/it]\u001b[A\n",
      " 28%|       | 534/1893 [27:26<1:13:44,  3.26s/it]\u001b[A\n",
      " 28%|       | 535/1893 [27:30<1:17:42,  3.43s/it]\u001b[A\n",
      " 28%|       | 536/1893 [27:33<1:18:06,  3.45s/it]\u001b[A\n",
      " 28%|       | 537/1893 [27:35<1:09:58,  3.10s/it]\u001b[A\n",
      " 28%|       | 538/1893 [27:37<1:02:20,  2.76s/it]\u001b[A\n",
      " 28%|       | 539/1893 [27:39<57:58,  2.57s/it]  \u001b[A\n",
      " 29%|       | 540/1893 [27:42<1:00:15,  2.67s/it]\u001b[A\n",
      " 29%|       | 541/1893 [27:45<59:03,  2.62s/it]  \u001b[A\n",
      " 29%|       | 542/1893 [27:47<58:04,  2.58s/it]\u001b[A\n",
      " 29%|       | 543/1893 [27:50<59:50,  2.66s/it]\u001b[A\n",
      " 29%|       | 544/1893 [27:53<57:25,  2.55s/it]\u001b[A\n",
      " 29%|       | 545/1893 [27:55<56:50,  2.53s/it]\u001b[A\n",
      " 29%|       | 546/1893 [27:58<57:18,  2.55s/it]\u001b[A\n",
      " 29%|       | 547/1893 [28:00<56:38,  2.53s/it]\u001b[A\n",
      " 29%|       | 548/1893 [28:03<58:06,  2.59s/it]\u001b[A\n",
      " 29%|       | 549/1893 [28:06<59:33,  2.66s/it]\u001b[A\n",
      " 29%|       | 550/1893 [28:08<56:19,  2.52s/it]\u001b[A\n",
      " 29%|       | 551/1893 [28:10<54:49,  2.45s/it]\u001b[A\n",
      " 29%|       | 552/1893 [28:13<56:51,  2.54s/it]\u001b[A\n",
      " 29%|       | 553/1893 [28:15<57:02,  2.55s/it]\u001b[A\n",
      " 29%|       | 554/1893 [28:18<59:51,  2.68s/it]\u001b[A\n",
      " 29%|       | 555/1893 [28:20<55:46,  2.50s/it]\u001b[A\n",
      " 29%|       | 556/1893 [28:23<54:26,  2.44s/it]\u001b[A\n",
      " 29%|       | 557/1893 [28:25<54:29,  2.45s/it]\u001b[A\n",
      " 29%|       | 558/1893 [28:28<54:37,  2.46s/it]\u001b[A\n",
      " 30%|       | 559/1893 [28:30<55:24,  2.49s/it]\u001b[A\n",
      " 30%|       | 560/1893 [28:33<59:31,  2.68s/it]\u001b[A\n",
      " 30%|       | 561/1893 [28:36<1:01:08,  2.75s/it]\u001b[A\n",
      " 30%|       | 562/1893 [28:39<1:00:07,  2.71s/it]\u001b[A\n",
      " 30%|       | 563/1893 [28:41<57:03,  2.57s/it]  \u001b[A\n",
      " 30%|       | 564/1893 [28:44<55:09,  2.49s/it]\u001b[A\n",
      " 30%|       | 565/1893 [28:47<1:00:50,  2.75s/it]\u001b[A\n",
      " 30%|       | 566/1893 [28:49<59:34,  2.69s/it]  \u001b[A\n",
      " 30%|       | 567/1893 [28:52<58:26,  2.64s/it]\u001b[A\n",
      " 30%|       | 568/1893 [28:55<1:00:31,  2.74s/it]\u001b[A\n",
      " 30%|       | 569/1893 [28:58<1:02:23,  2.83s/it]\u001b[A\n",
      " 30%|       | 570/1893 [29:00<1:00:19,  2.74s/it]\u001b[A\n",
      " 30%|       | 571/1893 [29:03<59:37,  2.71s/it]  \u001b[A\n",
      " 30%|       | 572/1893 [29:06<57:48,  2.63s/it]\u001b[A\n",
      " 30%|       | 573/1893 [29:08<54:46,  2.49s/it]\u001b[A\n",
      " 30%|       | 574/1893 [29:10<52:36,  2.39s/it]\u001b[A\n",
      " 30%|       | 575/1893 [29:12<53:18,  2.43s/it]\u001b[A\n",
      " 30%|       | 576/1893 [29:15<52:04,  2.37s/it]\u001b[A\n",
      " 30%|       | 577/1893 [29:17<55:12,  2.52s/it]\u001b[A\n",
      " 31%|       | 578/1893 [29:20<53:08,  2.42s/it]\u001b[A\n",
      " 31%|       | 579/1893 [29:22<55:11,  2.52s/it]\u001b[A\n",
      " 31%|       | 580/1893 [29:24<50:42,  2.32s/it]\u001b[A\n",
      " 31%|       | 581/1893 [29:27<51:54,  2.37s/it]\u001b[A\n",
      " 31%|       | 582/1893 [29:29<53:26,  2.45s/it]\u001b[A\n",
      " 31%|       | 583/1893 [29:32<52:41,  2.41s/it]\u001b[A\n",
      " 31%|       | 584/1893 [29:34<50:47,  2.33s/it]\u001b[A\n",
      " 31%|       | 585/1893 [29:36<51:42,  2.37s/it]\u001b[A\n",
      " 31%|       | 586/1893 [29:39<52:33,  2.41s/it]\u001b[A\n",
      " 31%|       | 587/1893 [29:41<51:49,  2.38s/it]\u001b[A\n",
      " 31%|       | 588/1893 [29:44<52:06,  2.40s/it]\u001b[A\n",
      " 31%|       | 589/1893 [29:46<52:01,  2.39s/it]\u001b[A\n",
      " 31%|       | 590/1893 [29:49<53:15,  2.45s/it]\u001b[A\n",
      " 31%|       | 591/1893 [29:51<55:18,  2.55s/it]\u001b[A\n",
      " 31%|      | 592/1893 [29:55<59:54,  2.76s/it]\u001b[A\n",
      " 31%|      | 593/1893 [29:58<1:01:14,  2.83s/it]\u001b[A\n",
      " 31%|      | 594/1893 [30:01<1:04:10,  2.96s/it]\u001b[A\n",
      " 31%|      | 595/1893 [30:04<1:06:33,  3.08s/it]\u001b[A\n",
      " 31%|      | 596/1893 [30:07<1:06:32,  3.08s/it]\u001b[A\n",
      " 32%|      | 597/1893 [30:12<1:13:48,  3.42s/it]\u001b[A\n",
      " 32%|      | 598/1893 [30:15<1:14:11,  3.44s/it]\u001b[A\n",
      " 32%|      | 599/1893 [30:17<1:07:13,  3.12s/it]\u001b[A\n",
      " 32%|      | 600/1893 [30:20<1:02:08,  2.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 32%|      | 601/1893 [30:22<59:22,  2.76s/it]  \u001b[A\n",
      " 32%|      | 602/1893 [30:26<1:03:07,  2.93s/it]\u001b[A\n",
      " 32%|      | 603/1893 [30:29<1:08:30,  3.19s/it]\u001b[A\n",
      " 32%|      | 604/1893 [30:33<1:09:28,  3.23s/it]\u001b[A\n",
      " 32%|      | 605/1893 [30:36<1:09:29,  3.24s/it]\u001b[A\n",
      " 32%|      | 606/1893 [30:39<1:09:47,  3.25s/it]\u001b[A\n",
      " 32%|      | 607/1893 [30:42<1:09:40,  3.25s/it]\u001b[A\n",
      " 32%|      | 608/1893 [30:45<1:06:00,  3.08s/it]\u001b[A\n",
      " 32%|      | 609/1893 [30:48<1:03:17,  2.96s/it]\u001b[A\n",
      " 32%|      | 610/1893 [30:50<59:20,  2.78s/it]  \u001b[A\n",
      " 32%|      | 611/1893 [30:52<55:21,  2.59s/it]\u001b[A\n",
      " 32%|      | 612/1893 [30:54<50:50,  2.38s/it]\u001b[A\n",
      " 32%|      | 613/1893 [30:56<48:09,  2.26s/it]\u001b[A\n",
      " 32%|      | 614/1893 [30:59<51:21,  2.41s/it]\u001b[A\n",
      " 32%|      | 615/1893 [31:02<53:48,  2.53s/it]\u001b[A\n",
      " 33%|      | 616/1893 [31:05<56:52,  2.67s/it]\u001b[A\n",
      " 33%|      | 617/1893 [31:07<55:20,  2.60s/it]\u001b[A\n",
      " 33%|      | 618/1893 [31:09<50:44,  2.39s/it]\u001b[A\n",
      " 33%|      | 619/1893 [31:11<48:42,  2.29s/it]\u001b[A\n",
      " 33%|      | 620/1893 [31:13<48:21,  2.28s/it]\u001b[A\n",
      " 33%|      | 621/1893 [31:16<51:26,  2.43s/it]\u001b[A\n",
      " 33%|      | 622/1893 [31:19<52:20,  2.47s/it]\u001b[A\n",
      " 33%|      | 623/1893 [31:22<54:23,  2.57s/it]\u001b[A\n",
      " 33%|      | 624/1893 [31:24<56:23,  2.67s/it]\u001b[A\n",
      " 33%|      | 625/1893 [31:27<55:59,  2.65s/it]\u001b[A\n",
      " 33%|      | 626/1893 [31:30<58:49,  2.79s/it]\u001b[A\n",
      " 33%|      | 627/1893 [31:33<57:17,  2.72s/it]\u001b[A\n",
      " 33%|      | 628/1893 [31:35<55:04,  2.61s/it]\u001b[A\n",
      " 33%|      | 629/1893 [31:37<51:19,  2.44s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "## TRAINING ##\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for i in tqdm(range(num_epochs)):\n",
    "    \n",
    "    loss_per_epoch = 0\n",
    "    \n",
    "    for j in tqdm(range(num_batches)):\n",
    "        s = j * batch_size\n",
    "        end = (j+1) * batch_size\n",
    "        \n",
    "        feed_dict = {\n",
    "            fp_length: seq_len[s:end],\n",
    "            fp: [[words_seq_id[k], pos_tags_seq_id[k]] for k in range(s, end)],\n",
    "            sp_length: [[len_path1[k], len_path2[k]] for k in range(s, end)],\n",
    "            sp: [[dep_path1_id[k],dep_path2_id[k]] for k in range(s, end)],\n",
    "            sp_pos : [[pos_path1[k], pos_path2[k]] for k in range(s, end)],\n",
    "            sp_childs: [[childs_path1[k], childs_path2[k]] for k in range(s, end)],\n",
    "            sp_num_childs: [[num_child_path1[k], num_child_path2[k]] for k in range(s, end)],\n",
    "            relation: rel_ids[s:end],\n",
    "            y_entity: entity[s:end]}\n",
    "    \n",
    "    # For entity pretraining \n",
    "#         _, _loss, step, _summary = sess.run([optimizer_seq, loss_seq, global_step_seq, summary], feed_dict)\n",
    "\n",
    "    # For complete model training\n",
    "        _, _loss, step, _summary = sess.run([train_op, total_loss, global_step_dep, summary], feed_dict)\n",
    "        \n",
    "        # Suming the loss for each epoch\n",
    "        loss_per_epoch +=_loss\n",
    "        \n",
    "        # Writing the summary\n",
    "        summary_writer.add_summary(_summary, step)\n",
    "        \n",
    "        if(step%100==0):\n",
    "            print(\"Steps:\", step)\n",
    "            \n",
    "        if (j+1)%num_batches==0:\n",
    "            print(\"Epoch:\", i+1,\"Step:\", step, \"loss:\",loss_per_epoch/num_batches)\n",
    "    \n",
    "    # Saving the model      \n",
    "    saver.save(sess, model_dir + '/epoch' + str(i))\n",
    "    print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING ACCURACY ##\n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    s = j * batch_size\n",
    "    end = (j+1) * batch_size\n",
    "\n",
    "    feed_dict = {\n",
    "        fp_length: seq_len[s:end],\n",
    "        fp: [[words_seq_id[k], pos_tags_seq_id[k]] for k in range(s, end)],\n",
    "        sp_length: [[len_path1[k], len_path2[k]] for k in range(s, end)],\n",
    "        sp: [[dep_path1_id[k],dep_path2_id[k]] for k in range(s, end)],\n",
    "        sp_pos : [[pos_path1[k], pos_path2[k]] for k in range(s, end)],\n",
    "        sp_childs: [[childs_path1[k], childs_path2[k]] for k in range(s, end)],\n",
    "        sp_num_childs: [[num_child_path1[k], num_child_path2[k]] for k in range(s, end)],\n",
    "        relation: rel_ids[s:end],\n",
    "        y_entity: entity[s:end]}\n",
    "        \n",
    "    batch_predictions = sess.run(predictions_dep, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "f1 = f1_score(rel_ids[:batch_size*num_batches], y_pred, average='macro')*100\n",
    "print(\"train accuracy\", accuracy,\" F1 Score\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST DATA PREPARATION ##\n",
    "f = open(data_dir + '/test_pathsv3', 'rb')\n",
    "words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1, pos_path2, childs_path1, childs_path2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/test_relationsv3.txt'):\n",
    "    relations.append(line.strip().split()[0])\n",
    "    \n",
    "length = len(words_seq)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "seq_len, words_seq_id, pos_tags_seq_id, deps_seq_id, len_path1, len_path2, pos_path1, pos_path2, dep_path1_id, dep_path2_id, childs_path1, childs_path2, num_child_path1, num_child_path2, rel_ids, entity = prepare_input(words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1,pos_path2, childs_path1, childs_path2, relations)\n",
    "\n",
    "## TEST ACCURACY ##\n",
    "\n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    s = j * batch_size\n",
    "    end = (j+1) * batch_size\n",
    "\n",
    "    feed_dict = {\n",
    "        fp_length: seq_len[s:end],\n",
    "        fp: [[words_seq_id[k], pos_tags_seq_id[k]] for k in range(s, end)],\n",
    "        sp_length: [[len_path1[k], len_path2[k]] for k in range(s, end)],\n",
    "        sp: [[dep_path1_id[k],dep_path2_id[k]] for k in range(s, end)],\n",
    "        sp_pos : [[pos_path1[k], pos_path2[k]] for k in range(s, end)],\n",
    "        sp_childs: [[childs_path1[k], childs_path2[k]] for k in range(s, end)],\n",
    "        sp_num_childs: [[num_child_path1[k], num_child_path2[k]] for k in range(s, end)],\n",
    "        relation: rel_ids[s:end],\n",
    "        y_entity: entity[s:end]}\n",
    "        \n",
    "    batch_predictions = sess.run(predictions_dep, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "f1 = f1_score(rel_ids[:batch_size*num_batches], y_pred, average='macro')*100\n",
    "print(\"test accuracy\", accuracy,\" F1 Score\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
