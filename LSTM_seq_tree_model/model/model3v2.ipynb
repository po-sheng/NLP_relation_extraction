{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REQUIREMENTS ##\n",
    "\n",
    "import sys, os, _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## CONSTANTS ##\n",
    "\n",
    "data_dir = '../testing_data'               # Directory for Data and Other files\n",
    "ckpt_dir = '../checkpoint'                 # Directory for Saving Checkpoints \n",
    "word_embd_dir = '../checkpoint/word_embd'  # Directory for Saving Checkpoints of Word Embedding Layer\n",
    "model_dir = '../checkpoint/model3v2'       # Directory for Saving Checkpoints of Model\n",
    "summary_dir = model_dir + '/summary'       # Directory for Saving Summaries of training\n",
    "err_analyze_dir = '/work/relation_extraction/'\n",
    "\n",
    "word_embd_dim = 100                        # Dimension of embedding layer for words\n",
    "pos_embd_dim = 25                          # Dimension of embedding layer for POS Tags\n",
    "dep_embd_dim = 25                          # Dimension of embedding layer for Dependency Types\n",
    "\n",
    "word_vocab_size = 400001                   # Vocab size for Words\n",
    "pos_vocab_size = 44                       # Vocab size for POS Tags\n",
    "dep_vocab_size = 45                        # Vocab size for Dependency Types\n",
    "\n",
    "relation_classes = 7                      # No. of Relation Classes\n",
    "\n",
    "state_size = 100                           # Dimension of States of LSTM-RNNs\n",
    "batch_size = 10                            # Batch Size for training\n",
    "\n",
    "max_len_seq = 90                           # Maximum length of sentences\n",
    "max_len_path = 20                          # Maximum length of lca paths\n",
    "max_num_child = 20                         # Maximum no. of childrens in Dependency Tree\n",
    "\n",
    "lambda_l2 = 0.0001                         # lambda of l2-regulaizer\n",
    "init_learning_rate = 0.001                 # Initial Learning Rate\n",
    "decay_steps = 2000                         # Decay Steps for Learning Rate\n",
    "decay_rate = 0.96                          # Decay Rate for Learning Rate\n",
    "gradient_clipping = 10                     # Size of Gradient Clipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT ##\n",
    "\n",
    "with tf.name_scope(\"input\"):\n",
    "    # Length of the whole sequence\n",
    "    fp_length = tf.placeholder(tf.int32, shape=[batch_size], name=\"fp_length\")\n",
    "    # Words and POS Tags in sequence\n",
    "    fp = tf.placeholder(tf.int32, [batch_size, 2, max_len_seq], name=\"full_path\")\n",
    "    # Length of both LCA Paths\n",
    "    sp_length = tf.placeholder(tf.int32, shape=[batch_size, 2], name=\"sp_length\")\n",
    "    # Dependency Types in LCA Paths\n",
    "    sp = tf.placeholder(tf.int32, [batch_size, 2, max_len_path], name=\"shortest_path\")\n",
    "    # Position of words in LCA Paths in whole sequence\n",
    "    sp_pos = tf.placeholder(tf.int32, [batch_size, 2, max_len_path], name=\"sp_pos\")\n",
    "    # Position in whole sequence of the children in Dependency Tree of words in LCA Paths \n",
    "    sp_childs = tf.placeholder(tf.int32, [batch_size, 2, max_len_path, max_num_child], name=\"sp_childs\")\n",
    "    # No. of children in Dependency Tree of words in LCA Paths\n",
    "    sp_num_childs = tf.placeholder(tf.int32, [batch_size, 2, max_len_path], name=\"sp_num_childs\")\n",
    "    # True Relation btw the entities\n",
    "    relation = tf.placeholder(tf.int32, [batch_size], name=\"relation\")\n",
    "    # Hot vector of true entities\n",
    "    y_entity = tf.placeholder(tf.int32, [batch_size, max_len_seq], name=\"y_enity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EMBEDDING LAYER ##\n",
    "\n",
    "# Embedding Layer of Words \n",
    "with tf.name_scope(\"word_embedding\"):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[word_vocab_size, word_embd_dim]), name=\"W\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32,[word_vocab_size, word_embd_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    embd_fp_word = tf.nn.embedding_lookup(W,fp[:,0])\n",
    "    word_embedding_saver = tf.train.Saver({\"word_embedding/W\": W})\n",
    "    \n",
    "# Embedding Layer of POS Tags \n",
    "with tf.name_scope(\"pos_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([pos_vocab_size, pos_embd_dim]), name=\"W\")\n",
    "    embd_fp_pos = tf.nn.embedding_lookup(W, fp[:,1])\n",
    "    pos_embedding_saver = tf.train.Saver({\"pos_embedding/W\": W})\n",
    "    \n",
    "# Embedding Layer of Dependency Types \n",
    "with tf.name_scope(\"dep_embedding\"):\n",
    "    W = tf.Variable(tf.random_uniform([dep_vocab_size, dep_embd_dim]), name=\"W\")\n",
    "    embd_sp = tf.nn.embedding_lookup(W, sp)\n",
    "    dep_embedding_saver = tf.train.Saver({\"dep_embedding/W\": W})\n",
    "\n",
    "# Embedding layer as input for Forward sequential LSTM-RNNs\n",
    "embd_fp = tf.concat([embd_fp_word, embd_fp_pos], axis=2)\n",
    "\n",
    "# Embedding layer as input for Backward sequential LSTM-RNNs\n",
    "embd_fp_rev = tf.reverse(embd_fp, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditions for while loops\n",
    "def cond1(i, const, steps, *agrs):\n",
    "    return i< steps\n",
    "\n",
    "def cond2(i, steps, *agrs):\n",
    "    return i< steps\n",
    "\n",
    "# Initial Hidden and Cell States for Sequential LSTMs\n",
    "init_state_seq = tf.zeros([2, 1, state_size])\n",
    "\n",
    "x = tf.constant(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 23:28:09.492148 140099534661440 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## SEQUENCE LAYER ##\n",
    "\n",
    "# Function for initializing Sequential LSTM-RNN\n",
    "def lstm_seq_init(channel, embedding_dim, state_size):\n",
    "    init_const = tf.zeros([1, state_size])\n",
    "    with tf.variable_scope(channel):\n",
    "        \n",
    "        # Input Gate's weigths and bias\n",
    "        W_i = tf.get_variable(\"W_i\",shape=[embedding_dim, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_i = tf.get_variable(\"U_i\",shape=[state_size, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_i = tf.get_variable(\"b_i\", initializer=init_const)\n",
    "\n",
    "        # Forget Gate's weigths and bias\n",
    "        W_f = tf.get_variable(\"W_f\",shape=[embedding_dim, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_f = tf.get_variable(\"U_f\",shape=[state_size, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_f = tf.get_variable(\"b_f\", initializer=init_const)\n",
    "\n",
    "        # Output Gate's weigths and bias\n",
    "        W_o = tf.get_variable(\"W_o\",shape=[embedding_dim, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_o = tf.get_variable(\"U_o\",shape=[state_size, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_o = tf.get_variable(\"b_o\", initializer=init_const)\n",
    "\n",
    "        W_g = tf.get_variable(\"W_g\",shape=[embedding_dim, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_g = tf.get_variable(\"U_g\",shape=[state_size, state_size] ,initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_g = tf.get_variable(\"b_g\", initializer=init_const)\n",
    "\n",
    "# Intialized Forward Sequential LSTM-RNN\n",
    "lstm_seq_init(\"lstm_fw\", word_embd_dim + pos_embd_dim, state_size)\n",
    "\n",
    "# Intialized Backward Sequential LSTM-RNN\n",
    "lstm_seq_init(\"lstm_bw\", word_embd_dim + pos_embd_dim, state_size)\n",
    "\n",
    "# Function for running Sequence LSTM \n",
    "def lstm_seq(input_embd, seq_len, scope):\n",
    "    \n",
    "    # While Loop body for running over the sequence\n",
    "    def body(j, const, steps, input_embd, states_seq, states_series):\n",
    "        inputs = tf.expand_dims(input_embd[j], [0])\n",
    "        \n",
    "        # Hidden State of LSTM-RNN\n",
    "        hs = states_seq[0]\n",
    "        \n",
    "        # Cell State of LSTM-RNN\n",
    "        cs = states_seq[1]\n",
    "        \n",
    "        # Hidden State Series\n",
    "        hs_ = states_series[0]\n",
    "        \n",
    "        # Cell State Series\n",
    "        cs_ = states_series[1]\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=True):\n",
    "            # Calling the Variables\n",
    "            W_i = tf.get_variable(\"W_i\")\n",
    "            U_i = tf.get_variable(\"U_i\")\n",
    "            b_i = tf.get_variable(\"b_i\")\n",
    "\n",
    "            W_f = tf.get_variable(\"W_f\")\n",
    "            U_f = tf.get_variable(\"U_f\")\n",
    "            b_f = tf.get_variable(\"b_f\")\n",
    "\n",
    "            W_o = tf.get_variable(\"W_o\")\n",
    "            U_o = tf.get_variable(\"U_o\")\n",
    "            b_o = tf.get_variable(\"b_o\")\n",
    "\n",
    "            W_g = tf.get_variable(\"W_g\")\n",
    "            U_g = tf.get_variable(\"U_g\")\n",
    "            b_g = tf.get_variable(\"b_g\")\n",
    "            \n",
    "            input_gate = tf.sigmoid(tf.matmul(inputs, W_i) + tf.matmul(hs, U_i) + b_i)\n",
    "            forget_gate = tf.sigmoid(tf.matmul(inputs, W_f) + tf.matmul(hs, U_f) + b_f)\n",
    "            output_gate = tf.sigmoid(tf.matmul(inputs, W_o) + tf.matmul(hs, U_o) + b_o)\n",
    "            gt = tf.tanh(tf.matmul(inputs, W_g) + tf.matmul(hs, U_g) + b_g)\n",
    "            cs = input_gate * gt + forget_gate * cs\n",
    "            hs = output_gate * tf.tanh(cs)\n",
    "\n",
    "            # Concating Hidden State Series and Hidden State\n",
    "            hs_ = tf.cond(tf.equal(j, const), lambda: hs, lambda: tf.concat([hs_, hs], 0))\n",
    "\n",
    "            # Concating Cell State Series and Cell State\n",
    "            cs_ = tf.cond(tf.equal(j, const), lambda: cs, lambda: tf.concat([cs_, cs], 0))\n",
    "            \n",
    "            # Stacking Hidden and Cell State\n",
    "            states_seq = tf.stack([hs, cs], axis=0)\n",
    "            \n",
    "            # Stacking Hidden and Cell State Series\n",
    "            states_series = tf.stack([hs_, cs_], axis=0)\n",
    "\n",
    "            return j+1, const, steps, input_embd, states_seq, states_series\n",
    "    \n",
    "    # Running While Loop over the Sequence\n",
    "    _, _, _, _, _, state_series_seq = tf.while_loop(cond1, body, \n",
    "            [0, 0, seq_len, input_embd, init_state_seq, init_state_seq],\n",
    "            shape_invariants=[x.get_shape(), x.get_shape(), x.get_shape(), input_embd.get_shape(),\n",
    "            init_state_seq.get_shape(), \n",
    "            tf.TensorShape([2, None, state_size])])\n",
    "    \n",
    "    # Return State Series of Sequence LSTMs\n",
    "    return state_series_seq\n",
    "\n",
    "# Computing the Sequence Layer\n",
    "\n",
    "states_series_fw = []\n",
    "states_series_bw = []\n",
    "hidden_states_seq = []\n",
    "\n",
    "for b in range(batch_size):\n",
    "    seq_len = fp_length[b]\n",
    "    input_embd = embd_fp[b]  \n",
    "    # Running Forward Sequence LSTM\n",
    "    states_series_fw.append(lstm_seq(input_embd, seq_len, \"lstm_fw\"))\n",
    "    \n",
    "    input_embd = embd_fp_rev[b]  \n",
    "    # Running Backward Sequence LSTM\n",
    "    states_series_bw.append(tf.reverse(lstm_seq(input_embd, seq_len, \"lstm_bw\"), [1]))\n",
    "    \n",
    "    # Concating Hidden States of both Forward and Backward Seq LSTMs\n",
    "    hidden_states_seq.append(tf.concat([states_series_fw[b][0], states_series_bw[b][0]], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DEPENDENCY LAYER ##\n",
    "\n",
    "# Function for initializing Tree Structured LSTM-RNN\n",
    "def lstm_dep_init(channel, dep_input_size, state_size):\n",
    "    init_const = tf.zeros([1, state_size])\n",
    "    with tf.variable_scope(channel):\n",
    "        \n",
    "        # Input Gate's weigths and bias\n",
    "        W_i = tf.get_variable(\"W_i\", shape=[dep_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_i = tf.get_variable(\"U_i\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_i = tf.get_variable(\"b_i\", initializer=init_const)\n",
    "        \n",
    "        # Input Gate's weights for Children in Dependency Tree\n",
    "        U_it = tf.get_variable(\"U_it\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        \n",
    "        # Forget Gate's weigths and bias\n",
    "        W_f = tf.get_variable(\"W_f\", shape=[dep_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_f = tf.get_variable(\"U_f\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_f = tf.get_variable(\"b_f\", initializer=init_const)\n",
    "        \n",
    "        # Forget Gate for Children in LCA Path's weigths\n",
    "        U_fsp = tf.get_variable(\"U_fsp\", shape=[2, state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        # Forget Gate for Children in Dependency Tree's weigths\n",
    "        U_ffp = tf.get_variable(\"U_ffp\", shape=[max_num_child, state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        \n",
    "        # Output Gate's weigths and bias\n",
    "        W_o = tf.get_variable(\"W_o\", shape=[dep_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_o = tf.get_variable(\"U_o\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_o = tf.get_variable(\"b_o\", initializer=init_const)\n",
    "        \n",
    "        # Output Gate's weights for Children in Dependency Tree\n",
    "        U_ot = tf.get_variable(\"U_ot\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        \n",
    "        W_u = tf.get_variable(\"W_u\", shape=[dep_input_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        U_u = tf.get_variable(\"U_u\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_u = tf.get_variable(\"b_u\", initializer=init_const)\n",
    "        U_ut = tf.get_variable(\"U_ut\", shape=[state_size, state_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "# Dimension of input in Treestructured LSTM-RNN\n",
    "dep_input_size = state_size * 2 + dep_embd_dim\n",
    "\n",
    "# Initializing  Bottom-Up Treestructured LSTM\n",
    "lstm_dep_init(\"lstm_btup\", dep_input_size, state_size)\n",
    "\n",
    "# Initializing Top-Down Treestructured LSTM\n",
    "lstm_dep_init(\"lstm_tpdn\", dep_input_size, state_size)\n",
    "\n",
    "# Initial Hidden and Cell State for Treestructured LSTM\n",
    "init_state = tf.zeros([2, 1, 1, state_size])\n",
    "\n",
    "# Function for running TreeStructured LSTM\n",
    "def lstm_dep(b, p, start, seq_len, input_embd, input_pos, input_childs, input_num_child, states_seq, init_state_dep, scope):  \n",
    "    \n",
    "    # While loop Body for running TreeStrucctured LSTM\n",
    "    def loop_over_seq(index, const, steps, input_pos, input_embd, input_childs, input_num_child, states_seq, states_dep, states_series):\n",
    "        \n",
    "        # Input for the lstm dep\n",
    "        inputs = tf.expand_dims(tf.concat([hidden_states_seq[b][input_pos[p][index]], input_embd[p][index]],0),0)\n",
    "       \n",
    "        # Children in Dependency Tree\n",
    "        childs = input_childs[p][index]\n",
    "        \n",
    "        # No. of Children in Dependency Tree\n",
    "        num_child = input_num_child[p][index]  \n",
    "        \n",
    "        # No. of Children in LCA Path\n",
    "        num_child_sp = tf.shape(states_dep[0])[0]\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=True):\n",
    "            # Calling the Variables\n",
    "            W_i = tf.get_variable(\"W_i\")\n",
    "            U_i = tf.get_variable(\"U_i\")\n",
    "            b_i = tf.get_variable(\"b_i\")\n",
    "            U_it = tf.get_variable(\"U_it\")\n",
    "\n",
    "            W_f = tf.get_variable(\"W_f\")\n",
    "            U_f = tf.get_variable(\"U_f\")\n",
    "            b_f = tf.get_variable(\"b_f\")\n",
    "            U_fsp = tf.get_variable(\"U_fsp\")\n",
    "            U_ffp = tf.get_variable(\"U_ffp\")\n",
    "            \n",
    "            W_o = tf.get_variable(\"W_o\")\n",
    "            U_o= tf.get_variable(\"U_o\")\n",
    "            b_o = tf.get_variable(\"b_o\")\n",
    "            U_ot = tf.get_variable(\"U_ot\")\n",
    "\n",
    "            W_u = tf.get_variable(\"W_u\")\n",
    "            U_u = tf.get_variable(\"U_u\")\n",
    "            b_u = tf.get_variable(\"b_u\")\n",
    "            U_ut = tf.get_variable(\"U_ut\")    \n",
    "\n",
    "            ## Computing Input, Forget, Output Gates \n",
    "            ## e.g. it = x*W + b + h*U\n",
    "            it = tf.matmul(inputs, W_i) + b_i + tf.matmul(states_dep[0][0], U_i)\n",
    "            ft = tf.matmul(inputs, W_f) + b_f + tf.matmul(states_dep[0][0], U_f)\n",
    "            ot = tf.matmul(inputs, W_o) + b_o + tf.matmul(states_dep[0][0], U_o)\n",
    "            ut = tf.matmul(inputs, W_u) + b_u + tf.matmul(states_dep[0][0], U_u)\n",
    "            \n",
    "            ## \n",
    "            def matmul(k, steps, it, ft, ot, ut):\n",
    "                it += tf.matmul(states_dep[0][k], U_i)\n",
    "                ft += tf.matmul(states_dep[0][k], U_f) \n",
    "                ot += tf.matmul(states_dep[0][k], U_o) \n",
    "                ut += tf.matmul(states_dep[0][k], U_u) \n",
    "                return k+1, steps, it, ft, ot, ut\n",
    "            \n",
    "            _, _, it, ft, ot, ut = tf.while_loop(cond2, matmul, [1, num_child_sp, it, ft, ot, ut])\n",
    "\n",
    "            ## Looping over the children in Dependency Tree \n",
    "            ## No. of loops is equal to the number of children in the dependency tree\n",
    "            def child_sum(k, steps, out, U): \n",
    "                ### Calculating out = out + h_child* U\n",
    "                ### Suming for all the children\n",
    "                out += tf.matmul(states_seq[0][childs[k]], U)\n",
    "                return k+1, steps, out, U\n",
    "\n",
    "            ## While loop with body as child_sum\n",
    "            ## Computing input gate, output gate by suming the h*U for all the children in the dependency tree\n",
    "            _, _, ht_i, _ = tf.while_loop(cond2, child_sum, [0, num_child, it, U_it])\n",
    "            _, _, ht_o, _ = tf.while_loop(cond2, child_sum, [0, num_child, ot, U_ot])\n",
    "            _, _, ht_u, _ = tf.while_loop(cond2, child_sum, [0, num_child, ut, U_ut])\n",
    "\n",
    "            ## Sigmoid over the gates\n",
    "            input_gate = tf.sigmoid(ht_i)\n",
    "            output_gate = tf.sigmoid(ht_o)\n",
    "            \n",
    "            u_input = tf.tanh(ht_u)\n",
    "\n",
    "            # Computing Cell State\n",
    "            cell_state = input_gate * u_input \n",
    "\n",
    "            # Computing Forget Gates for Children in LCA Path and Adding it to Compute Cell State\n",
    "            def cell_state_sp(k, steps, cell_state):\n",
    "                _, _, f_sp, _ = tf.while_loop(cond2, child_sum, [0, num_child, ft, U_fsp[k]])\n",
    "                cell_state += tf.sigmoid(f_sp) * states_dep[1][k]\n",
    "                return k+1, steps, cell_state\n",
    "            \n",
    "            _, _, cell_state = tf.while_loop(cond2, cell_state_sp, [0, num_child_sp, cell_state])\n",
    "\n",
    "            # Computing Forget Gates for Children in Dependency Tree and Adding it to Compute Cell State\n",
    "            def cell_states_fp(k, steps, ctl):\n",
    "                _, _, f_fp, _ = tf.while_loop(cond2, child_sum, [0, num_child, ft, U_ffp[k]])\n",
    "                ctl += tf.sigmoid(f_fp) * states_seq[1][childs[k]]\n",
    "                return k+1, steps, ctl\n",
    "\n",
    "            # Cell State \n",
    "            _, _, cds = tf.while_loop(cond2, cell_states_fp, [0, num_child, cell_state])\n",
    "            \n",
    "            # Hidden State\n",
    "            hds = tf.expand_dims(output_gate * tf.tanh(cds), 0)\n",
    "\n",
    "            # Expanding one dimension \n",
    "            cds = tf.expand_dims(cds, 0)\n",
    "            \n",
    "            # Stacking Hidden and Cell State\n",
    "            states_dep = tf.stack([hds, cds], axis=0)\n",
    "\n",
    "            # Concating Hidden State Series\n",
    "            hds_ = tf.cond(tf.equal(index, const), lambda: states_dep[0],\n",
    "                           lambda: tf.concat([states_series[0], states_dep[0]], 0))\n",
    "            # Concating Cell State Series\n",
    "            cds_ = tf.cond(tf.equal(index, const), lambda: states_dep[1],\n",
    "                           lambda: tf.concat([states_series[1], states_dep[1]], 0))\n",
    "\n",
    "            # Stacking Hidden and Cell State Series\n",
    "            states_series = tf.stack([hds_, cds_], axis=0)\n",
    "\n",
    "        return index+1, const, steps, input_pos, input_embd, input_childs, input_num_child, states_seq, states_dep, states_series\n",
    "    \n",
    "    # Running While Loop over sequence(LCA Path)\n",
    "    _, _, _, _, _, _, _, _, _, states_series_dep = tf.while_loop(cond1, loop_over_seq,[start, start, seq_len,\n",
    "                            input_pos, input_embd, input_childs, input_num_child, states_seq, init_state_dep,\n",
    "                            init_state], shape_invariants=[x.get_shape(), x.get_shape(), x.get_shape(),\n",
    "                            input_pos.get_shape(), input_embd.get_shape(), input_childs.get_shape(),\n",
    "                            input_num_child.get_shape(), states_seq.get_shape(),\n",
    "                            tf.TensorShape([2, None, 1, state_size]), tf.TensorShape([2, None, 1, state_size])])\n",
    "    \n",
    "    # Return Hidden and Cell State Series of TreeStructured LSTMs\n",
    "#     len(states_series_dep)\n",
    "    return states_series_dep\n",
    "\n",
    "lca_series_btup = []\n",
    "dp_series_tpdn = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    \n",
    "    # Position of words in the sentence \n",
    "    input_pos = sp_pos[i]\n",
    "    \n",
    "    # Dependency Embedding  \n",
    "    input_embd = embd_sp[i]\n",
    "    \n",
    "    # Children's position in the sentence\n",
    "    input_childs = sp_childs[i]\n",
    "    \n",
    "    # No. of children in Dependency tree\n",
    "    input_num_child = sp_num_childs[i]\n",
    "    \n",
    "    # Reverse Sequenece\n",
    "    input_pos_rev = tf.reverse(sp_pos[i], [1])\n",
    "    input_embd_rev = tf.reverse(embd_sp[i], [1])\n",
    "    input_childs_rev = tf.reverse(sp_childs[i], [1])\n",
    "    input_num_child_rev = tf.reverse(sp_num_childs[i], [1])\n",
    "    \n",
    "    # Expanding Dimension of States of Sequence LSTMs\n",
    "    states_seq_fw = tf.expand_dims(states_series_fw[i], 2)\n",
    "    states_seq_bw = tf.expand_dims(states_series_bw[i], 2)\n",
    "    \n",
    "    # Comuting States from 1st entity upto LCA (Bottom-Up)\n",
    "    s1 = lstm_dep(i, 0, 0, sp_length[i][0]-1, input_embd, input_pos, input_childs, input_num_child,\n",
    "                 states_seq_fw, init_state, \"lstm_btup\")\n",
    "    \n",
    "    # Last State in Bottom-Up which will serve as previous state for LCA\n",
    "    lca_btup = tf.cond(sp_length[i][0]>1, lambda: s1[:,sp_length[i][0]-2], lambda: init_state[:,0])\n",
    "\n",
    "    # Computing States from 2nd entity upto LCA (Bottom-Up)\n",
    "    s2 = lstm_dep(i, 1, 0, sp_length[i][1], input_embd_rev, input_pos_rev, input_childs_rev, input_num_child_rev,\n",
    "                 states_seq_bw, init_state, \"lstm_btup\")\n",
    "    \n",
    "    # Stacking Last State from both Bottom-Up Trees which will serve as previous state for LCA\n",
    "    lca_btup = tf.cond(sp_length[i][1]>0, lambda: tf.stack([lca_btup, s2[:,sp_length[i][1]-1]],axis=1), lambda: tf.expand_dims(lca_btup, 1))\n",
    "\n",
    "    # Computing State for LCA (Bottom Up)\n",
    "    lca_series_btup.append(lstm_dep(i, 0, sp_length[i][0]-1, sp_length[i][0], input_embd, input_pos, input_childs, input_num_child, states_seq_fw, lca_btup, \"lstm_btup\")[0,0])\n",
    "    \n",
    "    # Computing State for LCA (Top Down)\n",
    "    lca_tpdn = lstm_dep(i, 0, 0, 1, input_embd_rev, input_pos_rev, input_childs_rev, input_num_child_rev, \n",
    "                        states_seq_bw, init_state, \"lstm_tpdn\")\n",
    "\n",
    "    # Computing States from LCA to 1st entity (Top-Down)\n",
    "    dp1 = lstm_dep(i, 0, 1, sp_length[i][0], input_embd_rev, input_pos_rev, input_childs_rev, input_num_child_rev,\n",
    "                   states_seq_bw, lca_tpdn, \"lstm_tpdn\")[0,-1]\n",
    "    \n",
    "    # dp1 has State of 1st entity (Top-Down)\n",
    "    dp1 = tf.cond(sp_length[i][0]>1, lambda: dp1, lambda: lca_tpdn[0][0])\n",
    "\n",
    "    # Computing States from LCA to 2nd entity (Top-Down)\n",
    "    dp2 = lstm_dep(i, 1, 0, sp_length[i][1], input_embd, input_pos, input_childs, input_num_child,\n",
    "                   states_seq_fw, lca_tpdn, \"lstm_tpdn\")[0,-1]\n",
    "    \n",
    "    # dp2 has State of 2nd entity (Top-Down)\n",
    "    dp2 = tf.cond(sp_length[i][1]>0, lambda: dp2, lambda: lca_tpdn[0][0])\n",
    "\n",
    "    # Concating the States of 1st and 2nd Entities (Top-Down)\n",
    "    dp_series_tpdn.append(tf.concat([dp1, dp2], 1))\n",
    "        \n",
    "# Concating the LCA (Bottom-Up) State and Entities (Top-Down) States\n",
    "for i in range(batch_size):\n",
    "    \n",
    "    temp = tf.concat([lca_series_btup[i], dp_series_tpdn[i]], 1)\n",
    "    if(i==0):\n",
    "        dp_series = temp\n",
    "    else:\n",
    "        dp_series = tf.concat([dp_series,temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 23:28:44.497551 140099534661440 deprecation.py:506] From <ipython-input-8-30b09348580b>:31: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0726 23:28:44.500753 140099534661440 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0726 23:28:44.522663 140099534661440 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0726 23:28:44.534651 140099534661440 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0726 23:28:44.544896 140099534661440 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0726 23:28:44.561969 140099534661440 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0726 23:28:44.683469 140099534661440 deprecation.py:323] From <ipython-input-8-30b09348580b>:61: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loss_seq/Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## ENTITY DETECTION ## \n",
    "\n",
    "# Hidden Layer After Sequence LSTM\n",
    "with tf.name_scope(\"hidden_layer_seq\"):\n",
    "    W = tf.Variable(tf.truncated_normal([200, 100] , -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "    \n",
    "    y_hidden_layer = []\n",
    "    y_hl = tf.zeros([1, 100])\n",
    "\n",
    "    for batch in range(batch_size):\n",
    "        s_seq = tf.expand_dims(hidden_states_seq[batch], 1)\n",
    "\n",
    "        # Looping over the equence for computing Hidden Layer\n",
    "        def matmul_hl(j, const, steps, input_seq, out_seq):\n",
    "            temp = tf.tanh(tf.matmul(input_seq[j], W) + b)\n",
    "            out_seq = tf.cond(tf.equal(j, const), lambda: temp, lambda: tf.concat([out_seq, temp], 0))\n",
    "            return j+1, const, steps, input_seq, out_seq\n",
    "        \n",
    "        _, _, _, _, output_seq = tf.while_loop(cond1, matmul_hl, \n",
    "                                [0, 0, fp_length[batch], s_seq, y_hl],\n",
    "                                shape_invariants=[x.get_shape(), x.get_shape(), x.get_shape(), \n",
    "                                s_seq.get_shape(), tf.TensorShape([None, 100])])\n",
    "        \n",
    "        y_hidden_layer.append(output_seq)\n",
    "\n",
    "# Dropout For Hidden Layer Sequence\n",
    "with tf.name_scope(\"dropout_hidden_seq\"):\n",
    "    y_hidden_layer_drop = []\n",
    "    for i in range(batch_size):\n",
    "        y_drop = tf.nn.dropout(y_hidden_layer[i], 0.3)\n",
    "        y_hidden_layer_drop.append(y_drop)\n",
    "        \n",
    "# SoftMax Layer for Sequence \n",
    "with tf.name_scope(\"softmax_layer_seq\"):\n",
    "    W = tf.Variable(tf.truncated_normal([100, 2], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([2]), name=\"b\")\n",
    "    \n",
    "    logits_entity = []\n",
    "    predictions_entity = []\n",
    "    lg = tf.zeros([1, 2])\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        \n",
    "        # Looping over the sequence for computing Logits\n",
    "        def matmul_softmax(j, const, steps, y_hl, lg):\n",
    "            temp = tf.matmul(y_hl[j], W) + b\n",
    "            lg = tf.cond(tf.equal(j, const), lambda: temp, lambda: tf.concat([lg, temp], 0))\n",
    "            return j+1, const, steps, y_hl, lg\n",
    "\n",
    "        y_hl = tf.expand_dims(y_hidden_layer_drop[batch], 1)\n",
    "        \n",
    "        _, _, _, _, logit = tf.while_loop(cond1, matmul_softmax, \n",
    "                                [0, 0, fp_length[batch], y_hl,lg],\n",
    "                                shape_invariants=[x.get_shape(), x.get_shape(), x.get_shape(), \n",
    "                                y_hl.get_shape(), tf.TensorShape([None, 2])])\n",
    "        \n",
    "        logits_entity.append(logit)\n",
    "        \n",
    "        # Predictions for Entities\n",
    "        predictions_entity.append(tf.arg_max(logit, 1))\n",
    "        \n",
    "# One Hot Vector of True Entities in the sequence of its length\n",
    "Y_en = [y_entity[i][:fp_length[i]] for i in range(batch_size)]\n",
    "\n",
    "# One Hot Vector of Logits in the seqeuence of sentence's length\n",
    "Y_softmax = [tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_entity[i], labels=Y_en[i])) for i in range(batch_size)]\n",
    "\n",
    "# Loss for Sequence \n",
    "with tf.name_scope(\"loss_seq\"):\n",
    "    loss_seq = tf.reduce_mean(Y_softmax)\n",
    "    print(loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 23:30:30.413526 140099534661440 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "## RELATION CLASSIFICATION ##\n",
    "\n",
    "# Hidden Layer in Dependency Layer (after Tree Structured LSTMs)\n",
    "with tf.name_scope(\"hidden_layer_dep\"):\n",
    "    W = tf.Variable(tf.truncated_normal([300, 100], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "    y_p = tf.tanh(tf.matmul(dp_series, W) + b)\n",
    "\n",
    "# Dropout for Hidden Layer in Dependency Layer \n",
    "with tf.name_scope(\"dropout_hidden_dep\"):\n",
    "        y_p_drop = tf.nn.dropout(y_p, 0.3)\n",
    "    \n",
    "# SoftMax Layer in Dependency Layer\n",
    "with tf.name_scope(\"softmax_layer_dep\"):\n",
    "    W = tf.Variable(tf.truncated_normal([100, relation_classes], -0.1, 0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([relation_classes]), name=\"b\")\n",
    "    logits = tf.matmul(y_p_drop, W) + b    \n",
    "    predictions_dep = tf.argmax(logits, 1)\n",
    "\n",
    "# Loss for Dependency Layer\n",
    "with tf.name_scope(\"loss_dep\"):\n",
    "    loss_dep = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=relation))\n",
    "\n",
    "# All Trainable Variables\n",
    "tv_all = tf.trainable_variables()\n",
    "\n",
    "tv_regu = []\n",
    "\n",
    "# Variables that are not regularised\n",
    "non_reg = [\"word_embedding/W:0\",\"pos_embedding/W:0\",'dep_embedding/W:0']\n",
    "\n",
    "for t in tv_all:\n",
    "    if t.name not in non_reg:\n",
    "        if t.name.find('b_')==-1:\n",
    "            if t.name.find('b:')==-1:\n",
    "                tv_regu.append(t)\n",
    "\n",
    "# Total Loss\n",
    "with tf.name_scope(\"total_loss\"):\n",
    "    l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "    total_loss = l2_loss + loss_seq + loss_dep\n",
    "\n",
    "# Global Steps for Entity Training and Relation Classification\n",
    "global_step_seq = tf.Variable(0, trainable=False, name=\"global_step_seq\")\n",
    "global_step_dep = tf.Variable(0, trainable=False, name=\"global_step_dep\")\n",
    "\n",
    "# Learning Rates for Entity Training and Relation Classification\n",
    "learning_rate_seq = tf.train.exponential_decay(init_learning_rate, global_step_seq, decay_steps, decay_rate, staircase=True)\n",
    "learning_rate_dep = tf.train.exponential_decay(init_learning_rate, global_step_dep, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "# Optimzier for Loss of Sequence Layer\n",
    "optimizer_seq = tf.train.AdamOptimizer(learning_rate_seq).minimize(loss_seq, global_step=global_step_seq)\n",
    "\n",
    "# Optimizer for Total loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate_dep)\n",
    "\n",
    "# Gradients and Variables for Total Loss\n",
    "grads_vars = optimizer.compute_gradients(total_loss)\n",
    "\n",
    "for g, v in grads_vars:\n",
    "    if(g==None):\n",
    "        print(g, v)\n",
    "\n",
    "        \n",
    "# Clipping of Gradients\n",
    "clipped_grads = [(tf.clip_by_norm(grad, gradient_clipping), var) for grad, var in grads_vars]\n",
    "# Training Optimizer for Total Loss\n",
    "train_op = optimizer.apply_gradients(clipped_grads, global_step=global_step_dep)\n",
    "\n",
    "# Summary \n",
    "grad_summaries = []\n",
    "for g, v in grads_vars:\n",
    "    if g is not None:\n",
    "        grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "        sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        grad_summaries.append(grad_hist_summary)\n",
    "        grad_summaries.append(sparsity_summary)\n",
    "grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "loss_seq_summary = tf.summary.scalar(\"loss_seq\", loss_seq)\n",
    "loss_dep_summary = tf.summary.scalar(\"loss_dep\", loss_dep)\n",
    "total_loss_summary = tf.summary.scalar(\"total_loss\", total_loss)\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VOCAB Preprocessing Functions ##\n",
    "\n",
    "# Vocab for Words (Glove)\n",
    "f = open(data_dir + '/vocab_glove', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# Word to Vector \n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "# Vocab for POS Tags\n",
    "pos_tags_vocab = []\n",
    "for line in open(data_dir + '/pos_tags.txt'):\n",
    "    pos_tags_vocab.append(line.strip())\n",
    "\n",
    "# Vocab for Dependency Types\n",
    "dep_vocab = []\n",
    "for line in open(data_dir + '/dependency_types.txt'):\n",
    "    dep_vocab.append(line.strip())\n",
    "\n",
    "# Vocab for Relation Classes\n",
    "relation_vocab = []\n",
    "for line in open(data_dir + '/relation_typesv3.txt'):\n",
    "    relation_vocab.append(line.strip())\n",
    "    \n",
    "# Relation to Vector\n",
    "rel2id = dict((w, i) for i,w in enumerate(relation_vocab))\n",
    "id2rel = dict((i, w) for i,w in enumerate(relation_vocab))\n",
    "\n",
    "# POS Tags to Vector\n",
    "pos_tag2id = dict((w, i) for i,w in enumerate(pos_tags_vocab))\n",
    "id2pos_tag = dict((i, w) for i,w in enumerate(pos_tags_vocab))\n",
    "\n",
    "# Dependency Types to Vector\n",
    "dep2id = dict((w, i) for i,w in enumerate(dep_vocab))\n",
    "id2dep = dict((i, w) for i,w in enumerate(dep_vocab))\n",
    "\n",
    "pos_tag2id['OTH'] = 43\n",
    "id2pos_tag[43] = 'OTH'\n",
    "\n",
    "dep2id['OTH'] = 44\n",
    "id2dep[44] = 'OTH'\n",
    "\n",
    "# Grouping POS Tags\n",
    "JJ_pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "NN_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "RB_pos_tags = ['RB', 'RBR', 'RBS']\n",
    "PRP_pos_tags = ['PRP', 'PRP$']\n",
    "VB_pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "_pos_tags = ['CC', 'CD', 'DT', 'IN', '.', 'POS', '$', ',', 'EX', 'WRB', \"''\", 'MD', 'RP', 'WP', ':', 'TO', '``', 'WP$', 'WDT', 'PDT', 'SYM', 'LS', 'UH', 'FW', '#']\n",
    "\n",
    "def pos_tag(x):\n",
    "    if x in JJ_pos_tags:\n",
    "        return pos_tag2id['JJ']\n",
    "    if x in NN_pos_tags:\n",
    "        return pos_tag2id['NN']\n",
    "    if x in RB_pos_tags:\n",
    "        return pos_tag2id['RB']\n",
    "    if x in PRP_pos_tags:\n",
    "        return pos_tag2id['PRP']\n",
    "    if x in VB_pos_tags:\n",
    "        return pos_tag2id['VB']\n",
    "    if x in _pos_tags:\n",
    "        return pos_tag2id[x]\n",
    "    else:\n",
    "        return 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA PREPROCESSING ##\n",
    "\n",
    "# Function for preparing input for training or testing\n",
    "def prepare_input(words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1,pos_path2, childs_path1, childs_path2, relations):\n",
    "\n",
    "    # Size of the dataset\n",
    "    length = len(words_seq)\n",
    "    print(length)\n",
    "    # Position of words of LCA Paths in whole sentence \n",
    "    pos_path1 = [[i-1 for i in w] for w in pos_path1]\n",
    "    pos_path2 = [[i-1 for i in w] for w in pos_path2]\n",
    "\n",
    "    # Removing None\n",
    "    for i in range(length):\n",
    "            words_seq[i] = [w for w in words_seq[i] if w !=None ]\n",
    "            deps_seq[i] = [w for w in deps_seq[i] if w !=None ]\n",
    "            pos_tags_seq[i] = [w for w in pos_tags_seq[i] if w !=None ]\n",
    "\n",
    "    # Hot Vector of Entities\n",
    "    entity = np.zeros([length, max_len_seq])\n",
    "    \n",
    "    for i in range(length):\n",
    "        entity[i][pos_path1[i][0]] = 1\n",
    "        if(pos_path2[i]==[]):\n",
    "            entity[i][pos_path1[i][-1]] = 1\n",
    "        else:\n",
    "            entity[i][pos_path2[i][0]] = 1\n",
    "\n",
    "    len_path1 = []\n",
    "    len_path2 = []\n",
    "\n",
    "    num_child_path1 = np.ones([length, max_len_path],dtype=int)\n",
    "    num_child_path2 = np.ones([length, max_len_path],dtype=int)\n",
    "\n",
    "    # Length of LCA paths\n",
    "    for w in word_path1:\n",
    "        len_path1.append(len(w))\n",
    "\n",
    "    for w in word_path2:\n",
    "        len_path2.append(len(w))\n",
    "\n",
    "    # No. of Children of words of LCA paths in sentence\n",
    "    for i, w in enumerate(childs_path1):\n",
    "        if(w!=[]):\n",
    "            for j,c in enumerate(w):\n",
    "                num_child_path1[i][j] = len(c)\n",
    "        else:\n",
    "            num_child_path1[i][0] = 0\n",
    "\n",
    "    for i, w in enumerate(childs_path2):\n",
    "        if(w!=[]):\n",
    "            for j,c in enumerate(w):\n",
    "                num_child_path2[i][j] = len(c)\n",
    "        else:\n",
    "            num_child_path2[i][0] = 0\n",
    "\n",
    "    # Position of Children in sentence \n",
    "    for i in range(length):\n",
    "        if(childs_path2[i]!=[]):\n",
    "            for j, c in enumerate(childs_path2[i]):\n",
    "                if(c == []):\n",
    "                    childs_path2[i][j]  = [-1]\n",
    "        else:\n",
    "            childs_path2[i] = [[-1]]\n",
    "            \n",
    "        if(childs_path1[i]!=[]):\n",
    "            for j, c in enumerate(childs_path1[i]):\n",
    "                if(c == []):\n",
    "                    childs_path1[i][j]  = [-1]\n",
    "        else:\n",
    "            childs_path1[i] = [[-1]]\n",
    "\n",
    "    # Replacing words not in vcab with unknown_token\n",
    "    for i in range(length):\n",
    "        if(word_path2[i]==[]):\n",
    "            word_path2[i].append(unknown_token)\n",
    "        if(dep_path2[i]==[]):\n",
    "            dep_path2[i].append('OTH')\n",
    "        if(pos_tags_path2==[]):\n",
    "            pos_tags_path2[i].append('OTH')\n",
    "\n",
    "    # Converting Words, POS Tags, Dependency Types, Relation Classes to Vectors\n",
    "    for i in range(length):\n",
    "        for j, word in enumerate(words_seq[i]):\n",
    "            word = word.lower()\n",
    "            words_seq[i][j] = word if word in word2id else unknown_token \n",
    "\n",
    "        for l, d in enumerate(deps_seq[i]):\n",
    "            deps_seq[i][l] = d if d in dep2id else 'OTH'\n",
    "\n",
    "        for j, word in enumerate(word_path1[i]):\n",
    "            word = word.lower()\n",
    "            word_path1[i][j] = word if word in word2id else unknown_token \n",
    "\n",
    "        for l, d in enumerate(dep_path1[i]):\n",
    "            dep_path1[i][l] = d if d in dep2id else 'OTH'\n",
    "\n",
    "        for j, word in enumerate(word_path2[i]):\n",
    "            word = word.lower()\n",
    "            word_path2[i][j] = word if word in word2id else unknown_token \n",
    "\n",
    "        for l, d in enumerate(dep_path2[i]):\n",
    "            dep_path2[i][l] = d if d in dep2id else 'OTH'\n",
    "    \n",
    "    words_seq_id = np.ones([length, max_len_seq],dtype=int)\n",
    "    deps_seq_id = np.ones([length, max_len_seq],dtype=int)\n",
    "    pos_tags_seq_id = np.ones([length, max_len_seq],dtype=int)\n",
    "    \n",
    "    word_path1_id = np.ones([length, max_len_path],dtype=int)\n",
    "    word_path2_id = np.ones([length, max_len_path],dtype=int)\n",
    "    \n",
    "    dep_path1_id = np.ones([length, max_len_path],dtype=int)\n",
    "    dep_path2_id = np.ones([length, max_len_path],dtype=int)\n",
    "    \n",
    "    pos_tags_path1_id = np.ones([length, max_len_path],dtype=int)\n",
    "    pos_tags_path2_id = np.ones([length, max_len_path],dtype=int)\n",
    "    \n",
    "    pos_path1_ = np.ones([length, max_len_path],dtype=int)\n",
    "    pos_path2_ = np.ones([length, max_len_path],dtype=int)\n",
    "    \n",
    "    childs_path1_ = np.ones([length, max_len_path, max_num_child],dtype=int)\n",
    "    childs_path2_ = np.ones([length, max_len_path, max_num_child],dtype=int)\n",
    "    \n",
    "    seq_len = []\n",
    "\n",
    "    for i in range(length):\n",
    "        \n",
    "        temp = []\n",
    "        seq_len.append(len(words_seq[i]))\n",
    "\n",
    "        for j, w in enumerate(pos_path1[i]):\n",
    "            pos_path1_[i][j] = w\n",
    "        \n",
    "        for j, w in enumerate(pos_path2[i]):\n",
    "            pos_path2_[i][j] = w\n",
    "        \n",
    "        for j,child in enumerate(childs_path1[i]):\n",
    "            for k,c in enumerate(child):\n",
    "                childs_path1_[i][j][k] = c -1\n",
    "                \n",
    "        for j,child in enumerate(childs_path2[i]):\n",
    "            for k,c in enumerate(child):\n",
    "                childs_path2_[i][j][k] = c -1   \n",
    "    \n",
    "        for j, w in enumerate(words_seq[i]):\n",
    "            words_seq_id[i][j] = word2id[w]\n",
    "\n",
    "        \n",
    "        for j, w in enumerate(pos_tags_seq[i]):\n",
    "            pos_tags_seq_id[i][j] = pos_tag(w)\n",
    "        \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(deps_seq[i]):\n",
    "            deps_seq_id[i][j] = dep2id[w]\n",
    "         \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(word_path1[i]):\n",
    "            word_path1_id[i][j]   = word2id[w]\n",
    "         \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(pos_tags_path1[i]):\n",
    "            pos_tags_path1_id[i][j] = pos_tag(w)\n",
    "        \n",
    "        \n",
    "        for j, w in enumerate(dep_path1[i]):\n",
    "            dep_path1_id[i][j] = dep2id[w]\n",
    "         \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(word_path2[i]):\n",
    "            word_path2_id[i][j] = word2id[w]\n",
    "       \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(pos_tags_path2[i]):\n",
    "            pos_tags_path2_id[i][j] = pos_tag(w)\n",
    "         \n",
    "\n",
    "        \n",
    "        for j, w in enumerate(dep_path2[i]):\n",
    "            dep_path2_id[i][j]  = dep2id[w]\n",
    "         \n",
    "    rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "    \n",
    "    return seq_len, words_seq_id, pos_tags_seq_id, deps_seq_id, len_path1, len_path2, pos_path1_, pos_path2_, dep_path1_id, dep_path2_id, childs_path1_, childs_path2_, num_child_path1, num_child_path2, rel_ids, entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRAPH ##\n",
    "\n",
    "# set GPU limit\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "# For initializing all the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# For Saving the model\n",
    "saver = tf.train.Saver()\n",
    "# For writing Summaries\n",
    "summary_writer = tf.summary.FileWriter(summary_dir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initializing the embedding layer with Pretrained Glove Embedding Layer\n",
    "# f = open('data/word_embedding', 'rb')\n",
    "# word_embedding = pickle.load(f)\n",
    "# f.close()\n",
    "# sess.run(embedding_init, feed_dict={embedding_placeholder:word_embedding})\n",
    "# word_embedding_saver.save(sess, word_embd_dir + '/word_embd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 23:31:04.485177 140099534661440 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "# Restoring the model from latest checkpoint\n",
    "model = tf.train.latest_checkpoint(model_dir)\n",
    "saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Restoring the word embedding layer\n",
    "# latest_embd = tf.train.latest_checkpoint(word_embd_dir)\n",
    "# word_embedding_saver.restore(sess, latest_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76390\n"
     ]
    }
   ],
   "source": [
    "## TRAIN DATA PREPARATION ##\n",
    "\n",
    "f = open(data_dir + '/train_pathsv3', 'rb')\n",
    "words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1, pos_path2, childs_path1, childs_path2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/train_relationsv3.txt'):\n",
    "    relations.append(line.strip().split()[1])\n",
    "\n",
    "length = len(words_seq)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "seq_len, words_seq_id, pos_tags_seq_id, deps_seq_id, len_path1, len_path2, pos_path1, pos_path2, dep_path1_id, dep_path2_id, childs_path1, childs_path2, num_child_path1, num_child_path2, rel_ids, entity = prepare_input(words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1,pos_path2, childs_path1, childs_path2, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## TRAINING ##\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# for i in tqdm(range(num_epochs)):\n",
    "    \n",
    "#     loss_per_epoch = 0\n",
    "    \n",
    "#     for j in tqdm(range(num_batches)):\n",
    "#         s = j * batch_size\n",
    "#         end = (j+1) * batch_size\n",
    "        \n",
    "#         feed_dict = {\n",
    "#             fp_length: seq_len[s:end],\n",
    "#             fp: [[words_seq_id[k], pos_tags_seq_id[k]] for k in range(s, end)],\n",
    "#             sp_length: [[len_path1[k], len_path2[k]] for k in range(s, end)],\n",
    "#             sp: [[dep_path1_id[k],dep_path2_id[k]] for k in range(s, end)],\n",
    "#             sp_pos : [[pos_path1[k], pos_path2[k]] for k in range(s, end)],\n",
    "#             sp_childs: [[childs_path1[k], childs_path2[k]] for k in range(s, end)],\n",
    "#             sp_num_childs: [[num_child_path1[k], num_child_path2[k]] for k in range(s, end)],\n",
    "#             relation: rel_ids[s:end],\n",
    "#             y_entity: entity[s:end]}\n",
    "    \n",
    "#     # For entity pretraining \n",
    "# #         _, _loss, step, _summary = sess.run([optimizer_seq, loss_seq, global_step_seq, summary], feed_dict)\n",
    "\n",
    "#     # For complete model training\n",
    "#         _, _loss, step, _summary = sess.run([train_op, total_loss, global_step_dep, summary], feed_dict)\n",
    "        \n",
    "#         # Suming the loss for each epoch\n",
    "#         loss_per_epoch +=_loss\n",
    "        \n",
    "#         # Writing the summary\n",
    "#         summary_writer.add_summary(_summary, step)\n",
    "        \n",
    "#         if(step%100==0):\n",
    "#             print(\"Steps:\", step)\n",
    "            \n",
    "#         if (j+1)%num_batches==0:\n",
    "#             print(\"Epoch:\", i+1,\"Step:\", step, \"loss:\",loss_per_epoch/num_batches)\n",
    "    \n",
    "#     # Saving the model      \n",
    "#     saver.save(sess, model_dir + '/epoch' + str(i))\n",
    "#     print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TRAINING ACCURACY ##\n",
    "# all_predictions = []\n",
    "# for j in range(num_batches):\n",
    "#     s = j * batch_size\n",
    "#     end = (j+1) * batch_size\n",
    "\n",
    "#     feed_dict = {\n",
    "#         fp_length: seq_len[s:end],\n",
    "#         fp: [[words_seq_id[k], pos_tags_seq_id[k]] for k in range(s, end)],\n",
    "#         sp_length: [[len_path1[k], len_path2[k]] for k in range(s, end)],\n",
    "#         sp: [[dep_path1_id[k],dep_path2_id[k]] for k in range(s, end)],\n",
    "#         sp_pos : [[pos_path1[k], pos_path2[k]] for k in range(s, end)],\n",
    "#         sp_childs: [[childs_path1[k], childs_path2[k]] for k in range(s, end)],\n",
    "#         sp_num_childs: [[num_child_path1[k], num_child_path2[k]] for k in range(s, end)],\n",
    "#         relation: rel_ids[s:end],\n",
    "#         y_entity: entity[s:end]}\n",
    "        \n",
    "#     batch_predictions = sess.run(predictions_dep, feed_dict)\n",
    "#     all_predictions.append(batch_predictions)\n",
    "\n",
    "# y_pred = []\n",
    "# for i in range(num_batches):\n",
    "#     for pred in all_predictions[i]:\n",
    "#         y_pred.append(pred)\n",
    "\n",
    "# count = 0\n",
    "# for i in range(batch_size*num_batches):\n",
    "#     count += y_pred[i]==rel_ids[i]\n",
    "# accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "# f1 = f1_score(rel_ids[:batch_size*num_batches], y_pred, average='macro')*100\n",
    "# print(\"train accuracy\", accuracy,\" F1 Score\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n",
      "test accuracy 88.7878787878788  F1 Score 39.7328244727527\n"
     ]
    }
   ],
   "source": [
    "## TEST DATA PREPARATION ##\n",
    "f = open(data_dir + '/test_pathsv3', 'rb')\n",
    "words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1, pos_path2, childs_path1, childs_path2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/test_relationsv3.txt'):\n",
    "    relations.append(line.strip().split()[1])\n",
    "    \n",
    "length = len(words_seq)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "seq_len, words_seq_id, pos_tags_seq_id, deps_seq_id, len_path1, len_path2, pos_path1, pos_path2, dep_path1_id, dep_path2_id, childs_path1, childs_path2, num_child_path1, num_child_path2, rel_ids, entity = prepare_input(words_seq, deps_seq, pos_tags_seq, word_path1, word_path2, dep_path1, dep_path2, pos_tags_path1, pos_tags_path2, pos_path1,pos_path2, childs_path1, childs_path2, relations)\n",
    "\n",
    "## TEST ACCURACY ##\n",
    "\n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    s = j * batch_size\n",
    "    end = (j+1) * batch_size\n",
    "\n",
    "    feed_dict = {\n",
    "        fp_length: seq_len[s:end],\n",
    "        fp: [[words_seq_id[k], pos_tags_seq_id[k]] for k in range(s, end)],\n",
    "        sp_length: [[len_path1[k], len_path2[k]] for k in range(s, end)],\n",
    "        sp: [[dep_path1_id[k],dep_path2_id[k]] for k in range(s, end)],\n",
    "        sp_pos : [[pos_path1[k], pos_path2[k]] for k in range(s, end)],\n",
    "        sp_childs: [[childs_path1[k], childs_path2[k]] for k in range(s, end)],\n",
    "        sp_num_childs: [[num_child_path1[k], num_child_path2[k]] for k in range(s, end)],\n",
    "        relation: rel_ids[s:end],\n",
    "        y_entity: entity[s:end]}\n",
    "        \n",
    "    batch_predictions = sess.run(predictions_dep, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "    \n",
    "count = 0\n",
    "error = [[0 for j in range(relation_classes)] for i in range(relation_classes)]\n",
    "detail = [[[] for j in range(relation_classes)] for i in range(relation_classes)]\n",
    "\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "    error[rel_ids[i]][y_pred[i]] += 1\n",
    "    detail[rel_ids[i]][y_pred[i]].append(i)\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "f1 = f1_score(rel_ids[:batch_size*num_batches], y_pred, average='macro')*100\n",
    "print(\"test accuracy\", accuracy,\" F1 Score\", f1)\n",
    "\n",
    "# write error analysis file\n",
    "import csv\n",
    "\n",
    "# find FP and FN\n",
    "FN = 0\n",
    "FP = 0\n",
    "acc = 0\n",
    "for i in range(len(error) - 1):\n",
    "    FN += error[i][-1]\n",
    "    FP += error[-1][i]\n",
    "    acc += error[i][i]\n",
    "acc += error[-1][-1]\n",
    "\n",
    "# write file\n",
    "with open(err_analyze_dir + \"LSTM_error_analysis.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Total Data:\", len(words_seq), \"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "    writer.writerow(\"\")\n",
    "    re_type = [\"ORG-AFF\", \"GEN-AFF\", \"PHYS\", \"PART-WHOLE\", \"ART\", \"PER-SOC\", \"NONE\"]\n",
    "    writer.writerow([\"\", \"ORG-AFF (Pred)\", \"GEN-AFF (Pred)\", \"PHYS (Pred)\", \"PART-WHOLE (Pred)\", \"ART (Pred)\", \"PER-SOC (Pred)\", \"NONE (Pred)\"])\n",
    "    for i in range(len(error)):\n",
    "        writer.writerow([re_type[i] + \" (Label)\", error[i][0], error[i][1], error[i][2], error[i][3], error[i][4], error[i][5], error[i][6]])\n",
    "    writer.writerow(\"\")\n",
    "    writer.writerow([\"False Positive:\", FP])\n",
    "    writer.writerow([\"False Negative:\", FN])\n",
    "    writer.writerow([\"test accuracy(True Positive + True Negatice):\", acc])\n",
    "    writer.writerow([\"Wrong Type:\", len(words_seq) - FP - FN - acc])\n",
    "    writer.writerow([\"F1 Score:\", f1])\n",
    "    \n",
    "# sentence, two_entity, pred, label \n",
    "with open(err_analyze_dir + \"error_detail.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Sentence\", \"Two_Entity\", \"Predict\", \"Label\", \"idx\"])\n",
    "    for i in range(relation_classes):\n",
    "        for j in range(relation_classes):\n",
    "            if i == j:\n",
    "                continue\n",
    "            for k in range(len(detail[i][j])):\n",
    "                ent1 = word_path1[detail[i][j][k]][0]\n",
    "                if word_path2[detail[i][j][k]] == []:\n",
    "                    ent2 = word_path1[detail[i][j][k]][-1]\n",
    "                else:\n",
    "                    ent2 = word_path2[detail[i][j][k]][0]\n",
    "                sep = \" \"\n",
    "#                 sep.join(words_seq[detail[i][j][k]])\n",
    "                writer.writerow([sep.join(words_seq[detail[i][j][k]]), [ent1, ent2], id2rel[j], id2rel[i], detail[i][j][k]])\n",
    "            writer.writerow(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
